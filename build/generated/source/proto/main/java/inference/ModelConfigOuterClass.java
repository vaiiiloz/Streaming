// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: model_config.proto

package inference;

public final class ModelConfigOuterClass {
  private ModelConfigOuterClass() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistryLite registry) {
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:enum:: DataType
   *&#64;&#64;
   *&#64;&#64;   Data types supported for input and output tensors.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf enum {@code inference.DataType}
   */
  public enum DataType
      implements com.google.protobuf.Internal.EnumLite {
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INVALID = 0
     * </pre>
     *
     * <code>TYPE_INVALID = 0;</code>
     */
    TYPE_INVALID(0),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::BOOL = 1
     * </pre>
     *
     * <code>TYPE_BOOL = 1;</code>
     */
    TYPE_BOOL(1),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT8 = 2
     * </pre>
     *
     * <code>TYPE_UINT8 = 2;</code>
     */
    TYPE_UINT8(2),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT16 = 3
     * </pre>
     *
     * <code>TYPE_UINT16 = 3;</code>
     */
    TYPE_UINT16(3),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT32 = 4
     * </pre>
     *
     * <code>TYPE_UINT32 = 4;</code>
     */
    TYPE_UINT32(4),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT64 = 5
     * </pre>
     *
     * <code>TYPE_UINT64 = 5;</code>
     */
    TYPE_UINT64(5),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT8 = 6
     * </pre>
     *
     * <code>TYPE_INT8 = 6;</code>
     */
    TYPE_INT8(6),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT16 = 7
     * </pre>
     *
     * <code>TYPE_INT16 = 7;</code>
     */
    TYPE_INT16(7),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT32 = 8
     * </pre>
     *
     * <code>TYPE_INT32 = 8;</code>
     */
    TYPE_INT32(8),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT64 = 9
     * </pre>
     *
     * <code>TYPE_INT64 = 9;</code>
     */
    TYPE_INT64(9),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::FP16 = 10
     * </pre>
     *
     * <code>TYPE_FP16 = 10;</code>
     */
    TYPE_FP16(10),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::FP32 = 11
     * </pre>
     *
     * <code>TYPE_FP32 = 11;</code>
     */
    TYPE_FP32(11),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::FP64 = 12
     * </pre>
     *
     * <code>TYPE_FP64 = 12;</code>
     */
    TYPE_FP64(12),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::STRING = 13
     * </pre>
     *
     * <code>TYPE_STRING = 13;</code>
     */
    TYPE_STRING(13),
    UNRECOGNIZED(-1),
    ;

    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INVALID = 0
     * </pre>
     *
     * <code>TYPE_INVALID = 0;</code>
     */
    public static final int TYPE_INVALID_VALUE = 0;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::BOOL = 1
     * </pre>
     *
     * <code>TYPE_BOOL = 1;</code>
     */
    public static final int TYPE_BOOL_VALUE = 1;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT8 = 2
     * </pre>
     *
     * <code>TYPE_UINT8 = 2;</code>
     */
    public static final int TYPE_UINT8_VALUE = 2;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT16 = 3
     * </pre>
     *
     * <code>TYPE_UINT16 = 3;</code>
     */
    public static final int TYPE_UINT16_VALUE = 3;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT32 = 4
     * </pre>
     *
     * <code>TYPE_UINT32 = 4;</code>
     */
    public static final int TYPE_UINT32_VALUE = 4;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT64 = 5
     * </pre>
     *
     * <code>TYPE_UINT64 = 5;</code>
     */
    public static final int TYPE_UINT64_VALUE = 5;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT8 = 6
     * </pre>
     *
     * <code>TYPE_INT8 = 6;</code>
     */
    public static final int TYPE_INT8_VALUE = 6;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT16 = 7
     * </pre>
     *
     * <code>TYPE_INT16 = 7;</code>
     */
    public static final int TYPE_INT16_VALUE = 7;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT32 = 8
     * </pre>
     *
     * <code>TYPE_INT32 = 8;</code>
     */
    public static final int TYPE_INT32_VALUE = 8;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT64 = 9
     * </pre>
     *
     * <code>TYPE_INT64 = 9;</code>
     */
    public static final int TYPE_INT64_VALUE = 9;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::FP16 = 10
     * </pre>
     *
     * <code>TYPE_FP16 = 10;</code>
     */
    public static final int TYPE_FP16_VALUE = 10;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::FP32 = 11
     * </pre>
     *
     * <code>TYPE_FP32 = 11;</code>
     */
    public static final int TYPE_FP32_VALUE = 11;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::FP64 = 12
     * </pre>
     *
     * <code>TYPE_FP64 = 12;</code>
     */
    public static final int TYPE_FP64_VALUE = 12;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::STRING = 13
     * </pre>
     *
     * <code>TYPE_STRING = 13;</code>
     */
    public static final int TYPE_STRING_VALUE = 13;


    @java.lang.Override
    public final int getNumber() {
      if (this == UNRECOGNIZED) {
        throw new java.lang.IllegalArgumentException(
            "Can't get the number of an unknown enum value.");
      }
      return value;
    }

    /**
     * @param value The number of the enum to look for.
     * @return The enum associated with the given number.
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @java.lang.Deprecated
    public static DataType valueOf(int value) {
      return forNumber(value);
    }

    public static DataType forNumber(int value) {
      switch (value) {
        case 0: return TYPE_INVALID;
        case 1: return TYPE_BOOL;
        case 2: return TYPE_UINT8;
        case 3: return TYPE_UINT16;
        case 4: return TYPE_UINT32;
        case 5: return TYPE_UINT64;
        case 6: return TYPE_INT8;
        case 7: return TYPE_INT16;
        case 8: return TYPE_INT32;
        case 9: return TYPE_INT64;
        case 10: return TYPE_FP16;
        case 11: return TYPE_FP32;
        case 12: return TYPE_FP64;
        case 13: return TYPE_STRING;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<DataType>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static final com.google.protobuf.Internal.EnumLiteMap<
        DataType> internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<DataType>() {
            @java.lang.Override
            public DataType findValueByNumber(int number) {
              return DataType.forNumber(number);
            }
          };

    public static com.google.protobuf.Internal.EnumVerifier 
        internalGetVerifier() {
      return DataTypeVerifier.INSTANCE;
    }

    private static final class DataTypeVerifier implements 
         com.google.protobuf.Internal.EnumVerifier { 
            static final com.google.protobuf.Internal.EnumVerifier           INSTANCE = new DataTypeVerifier();
            @java.lang.Override
            public boolean isInRange(int number) {
              return DataType.forNumber(number) != null;
            }
          };

    private final int value;

    private DataType(int value) {
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:inference.DataType)
  }

  public interface ModelRateLimiterOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelRateLimiter)
      com.google.protobuf.MessageLiteOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
     *&#64;&#64;
     *&#64;&#64;     The resources required to execute the request on a model instance.
     *&#64;&#64;     Resources are just names with a corresponding count. The execution
     *&#64;&#64;     of the instance will be blocked until the specificied resources are
     *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
     */
    java.util.List<inference.ModelConfigOuterClass.ModelRateLimiter.Resource> 
        getResourcesList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
     *&#64;&#64;
     *&#64;&#64;     The resources required to execute the request on a model instance.
     *&#64;&#64;     Resources are just names with a corresponding count. The execution
     *&#64;&#64;     of the instance will be blocked until the specificied resources are
     *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
     */
    inference.ModelConfigOuterClass.ModelRateLimiter.Resource getResources(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
     *&#64;&#64;
     *&#64;&#64;     The resources required to execute the request on a model instance.
     *&#64;&#64;     Resources are just names with a corresponding count. The execution
     *&#64;&#64;     of the instance will be blocked until the specificied resources are
     *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
     */
    int getResourcesCount();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 priority
     *&#64;&#64;
     *&#64;&#64;     The weighting value to be used for prioritizing across instances.
     *&#64;&#64;     An instance with priority 2 will be given 1/2 the number of
     *&#64;&#64;     scheduling chances as an instance_group with priority 1. The
     *&#64;&#64;     default priority is 1.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 priority = 2;</code>
     * @return The priority.
     */
    int getPriority();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;  .. cpp:var:: message ModelRateLimiter
   *&#64;&#64;
   *&#64;&#64;     The specifications required by the rate limiter to properly
   *&#64;&#64;     schedule the inference requests across the different models
   *&#64;&#64;     and their instances.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelRateLimiter}
   */
  public  static final class ModelRateLimiter extends
      com.google.protobuf.GeneratedMessageLite<
          ModelRateLimiter, ModelRateLimiter.Builder> implements
      // @@protoc_insertion_point(message_implements:inference.ModelRateLimiter)
      ModelRateLimiterOrBuilder {
    private ModelRateLimiter() {
      resources_ = emptyProtobufList();
    }
    public interface ResourceOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelRateLimiter.Resource)
        com.google.protobuf.MessageLiteOrBuilder {

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name associated with the resource.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The name.
       */
      java.lang.String getName();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name associated with the resource.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      com.google.protobuf.ByteString
          getNameBytes();

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool global
       *&#64;&#64;
       *&#64;&#64;     Whether or not the resource is global. If true then the resource
       *&#64;&#64;     is assumed to be shared among the devices otherwise specified
       *&#64;&#64;     count of the resource is assumed for each device associated
       *&#64;&#64;     with the instance.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool global = 2;</code>
       * @return The global.
       */
      boolean getGlobal();

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 count
       *&#64;&#64;
       *&#64;&#64;     The number of resources required for the execution of the model
       *&#64;&#64;     instance.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 count = 3;</code>
       * @return The count.
       */
      int getCount();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message Resource
     *&#64;&#64;
     *&#64;&#64;     The resource property.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelRateLimiter.Resource}
     */
    public  static final class Resource extends
        com.google.protobuf.GeneratedMessageLite<
            Resource, Resource.Builder> implements
        // @@protoc_insertion_point(message_implements:inference.ModelRateLimiter.Resource)
        ResourceOrBuilder {
      private Resource() {
        name_ = "";
      }
      public static final int NAME_FIELD_NUMBER = 1;
      private java.lang.String name_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name associated with the resource.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The name.
       */
      @java.lang.Override
      public java.lang.String getName() {
        return name_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name associated with the resource.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString
          getNameBytes() {
        return com.google.protobuf.ByteString.copyFromUtf8(name_);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name associated with the resource.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The name to set.
       */
      private void setName(
          java.lang.String value) {
        java.lang.Class<?> valueClass = value.getClass();
  
        name_ = value;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name associated with the resource.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      private void clearName() {
        
        name_ = getDefaultInstance().getName();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name associated with the resource.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The bytes for name to set.
       */
      private void setNameBytes(
          com.google.protobuf.ByteString value) {
        checkByteStringIsUtf8(value);
        name_ = value.toStringUtf8();
        
      }

      public static final int GLOBAL_FIELD_NUMBER = 2;
      private boolean global_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool global
       *&#64;&#64;
       *&#64;&#64;     Whether or not the resource is global. If true then the resource
       *&#64;&#64;     is assumed to be shared among the devices otherwise specified
       *&#64;&#64;     count of the resource is assumed for each device associated
       *&#64;&#64;     with the instance.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool global = 2;</code>
       * @return The global.
       */
      @java.lang.Override
      public boolean getGlobal() {
        return global_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool global
       *&#64;&#64;
       *&#64;&#64;     Whether or not the resource is global. If true then the resource
       *&#64;&#64;     is assumed to be shared among the devices otherwise specified
       *&#64;&#64;     count of the resource is assumed for each device associated
       *&#64;&#64;     with the instance.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool global = 2;</code>
       * @param value The global to set.
       */
      private void setGlobal(boolean value) {
        
        global_ = value;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool global
       *&#64;&#64;
       *&#64;&#64;     Whether or not the resource is global. If true then the resource
       *&#64;&#64;     is assumed to be shared among the devices otherwise specified
       *&#64;&#64;     count of the resource is assumed for each device associated
       *&#64;&#64;     with the instance.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool global = 2;</code>
       */
      private void clearGlobal() {
        
        global_ = false;
      }

      public static final int COUNT_FIELD_NUMBER = 3;
      private int count_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 count
       *&#64;&#64;
       *&#64;&#64;     The number of resources required for the execution of the model
       *&#64;&#64;     instance.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 count = 3;</code>
       * @return The count.
       */
      @java.lang.Override
      public int getCount() {
        return count_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 count
       *&#64;&#64;
       *&#64;&#64;     The number of resources required for the execution of the model
       *&#64;&#64;     instance.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 count = 3;</code>
       * @param value The count to set.
       */
      private void setCount(int value) {
        
        count_ = value;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 count
       *&#64;&#64;
       *&#64;&#64;     The number of resources required for the execution of the model
       *&#64;&#64;     instance.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 count = 3;</code>
       */
      private void clearCount() {
        
        count_ = 0;
      }

      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }

      public static Builder newBuilder() {
        return (Builder) DEFAULT_INSTANCE.createBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelRateLimiter.Resource prototype) {
        return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message Resource
       *&#64;&#64;
       *&#64;&#64;     The resource property.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelRateLimiter.Resource}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageLite.Builder<
            inference.ModelConfigOuterClass.ModelRateLimiter.Resource, Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelRateLimiter.Resource)
          inference.ModelConfigOuterClass.ModelRateLimiter.ResourceOrBuilder {
        // Construct using inference.ModelConfigOuterClass.ModelRateLimiter.Resource.newBuilder()
        private Builder() {
          super(DEFAULT_INSTANCE);
        }


        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;     The name associated with the resource.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @return The name.
         */
        @java.lang.Override
        public java.lang.String getName() {
          return instance.getName();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;     The name associated with the resource.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @return The bytes for name.
         */
        @java.lang.Override
        public com.google.protobuf.ByteString
            getNameBytes() {
          return instance.getNameBytes();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;     The name associated with the resource.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @param value The name to set.
         * @return This builder for chaining.
         */
        public Builder setName(
            java.lang.String value) {
          copyOnWrite();
          instance.setName(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;     The name associated with the resource.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearName() {
          copyOnWrite();
          instance.clearName();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;     The name associated with the resource.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @param value The bytes for name to set.
         * @return This builder for chaining.
         */
        public Builder setNameBytes(
            com.google.protobuf.ByteString value) {
          copyOnWrite();
          instance.setNameBytes(value);
          return this;
        }

        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: bool global
         *&#64;&#64;
         *&#64;&#64;     Whether or not the resource is global. If true then the resource
         *&#64;&#64;     is assumed to be shared among the devices otherwise specified
         *&#64;&#64;     count of the resource is assumed for each device associated
         *&#64;&#64;     with the instance.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool global = 2;</code>
         * @return The global.
         */
        @java.lang.Override
        public boolean getGlobal() {
          return instance.getGlobal();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: bool global
         *&#64;&#64;
         *&#64;&#64;     Whether or not the resource is global. If true then the resource
         *&#64;&#64;     is assumed to be shared among the devices otherwise specified
         *&#64;&#64;     count of the resource is assumed for each device associated
         *&#64;&#64;     with the instance.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool global = 2;</code>
         * @param value The global to set.
         * @return This builder for chaining.
         */
        public Builder setGlobal(boolean value) {
          copyOnWrite();
          instance.setGlobal(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: bool global
         *&#64;&#64;
         *&#64;&#64;     Whether or not the resource is global. If true then the resource
         *&#64;&#64;     is assumed to be shared among the devices otherwise specified
         *&#64;&#64;     count of the resource is assumed for each device associated
         *&#64;&#64;     with the instance.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool global = 2;</code>
         * @return This builder for chaining.
         */
        public Builder clearGlobal() {
          copyOnWrite();
          instance.clearGlobal();
          return this;
        }

        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: uint32 count
         *&#64;&#64;
         *&#64;&#64;     The number of resources required for the execution of the model
         *&#64;&#64;     instance.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint32 count = 3;</code>
         * @return The count.
         */
        @java.lang.Override
        public int getCount() {
          return instance.getCount();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: uint32 count
         *&#64;&#64;
         *&#64;&#64;     The number of resources required for the execution of the model
         *&#64;&#64;     instance.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint32 count = 3;</code>
         * @param value The count to set.
         * @return This builder for chaining.
         */
        public Builder setCount(int value) {
          copyOnWrite();
          instance.setCount(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: uint32 count
         *&#64;&#64;
         *&#64;&#64;     The number of resources required for the execution of the model
         *&#64;&#64;     instance.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint32 count = 3;</code>
         * @return This builder for chaining.
         */
        public Builder clearCount() {
          copyOnWrite();
          instance.clearCount();
          return this;
        }

        // @@protoc_insertion_point(builder_scope:inference.ModelRateLimiter.Resource)
      }
      @java.lang.Override
      @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
      protected final java.lang.Object dynamicMethod(
          com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
          java.lang.Object arg0, java.lang.Object arg1) {
        switch (method) {
          case NEW_MUTABLE_INSTANCE: {
            return new inference.ModelConfigOuterClass.ModelRateLimiter.Resource();
          }
          case NEW_BUILDER: {
            return new Builder();
          }
          case BUILD_MESSAGE_INFO: {
              java.lang.Object[] objects = new java.lang.Object[] {
                "name_",
                "global_",
                "count_",
              };
              java.lang.String info =
                  "\u0000\u0003\u0000\u0000\u0001\u0003\u0003\u0000\u0000\u0000\u0001\u0208\u0002\u0007" +
                  "\u0003\u000b";
              return newMessageInfo(DEFAULT_INSTANCE, info, objects);
          }
          // fall through
          case GET_DEFAULT_INSTANCE: {
            return DEFAULT_INSTANCE;
          }
          case GET_PARSER: {
            com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelRateLimiter.Resource> parser = PARSER;
            if (parser == null) {
              synchronized (inference.ModelConfigOuterClass.ModelRateLimiter.Resource.class) {
                parser = PARSER;
                if (parser == null) {
                  parser =
                      new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelRateLimiter.Resource>(
                          DEFAULT_INSTANCE);
                  PARSER = parser;
                }
              }
            }
            return parser;
        }
        case GET_MEMOIZED_IS_INITIALIZED: {
          return (byte) 1;
        }
        case SET_MEMOIZED_IS_INITIALIZED: {
          return null;
        }
        }
        throw new UnsupportedOperationException();
      }


      // @@protoc_insertion_point(class_scope:inference.ModelRateLimiter.Resource)
      private static final inference.ModelConfigOuterClass.ModelRateLimiter.Resource DEFAULT_INSTANCE;
      static {
        Resource defaultInstance = new Resource();
        // New instances are implicitly immutable so no need to make
        // immutable.
        DEFAULT_INSTANCE = defaultInstance;
        com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
          Resource.class, defaultInstance);
      }

      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static volatile com.google.protobuf.Parser<Resource> PARSER;

      public static com.google.protobuf.Parser<Resource> parser() {
        return DEFAULT_INSTANCE.getParserForType();
      }
    }

    public static final int RESOURCES_FIELD_NUMBER = 1;
    private com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.ModelRateLimiter.Resource> resources_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
     *&#64;&#64;
     *&#64;&#64;     The resources required to execute the request on a model instance.
     *&#64;&#64;     Resources are just names with a corresponding count. The execution
     *&#64;&#64;     of the instance will be blocked until the specificied resources are
     *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
     */
    @java.lang.Override
    public java.util.List<inference.ModelConfigOuterClass.ModelRateLimiter.Resource> getResourcesList() {
      return resources_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
     *&#64;&#64;
     *&#64;&#64;     The resources required to execute the request on a model instance.
     *&#64;&#64;     Resources are just names with a corresponding count. The execution
     *&#64;&#64;     of the instance will be blocked until the specificied resources are
     *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
     */
    public java.util.List<? extends inference.ModelConfigOuterClass.ModelRateLimiter.ResourceOrBuilder> 
        getResourcesOrBuilderList() {
      return resources_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
     *&#64;&#64;
     *&#64;&#64;     The resources required to execute the request on a model instance.
     *&#64;&#64;     Resources are just names with a corresponding count. The execution
     *&#64;&#64;     of the instance will be blocked until the specificied resources are
     *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
     */
    @java.lang.Override
    public int getResourcesCount() {
      return resources_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
     *&#64;&#64;
     *&#64;&#64;     The resources required to execute the request on a model instance.
     *&#64;&#64;     Resources are just names with a corresponding count. The execution
     *&#64;&#64;     of the instance will be blocked until the specificied resources are
     *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelRateLimiter.Resource getResources(int index) {
      return resources_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
     *&#64;&#64;
     *&#64;&#64;     The resources required to execute the request on a model instance.
     *&#64;&#64;     Resources are just names with a corresponding count. The execution
     *&#64;&#64;     of the instance will be blocked until the specificied resources are
     *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
     */
    public inference.ModelConfigOuterClass.ModelRateLimiter.ResourceOrBuilder getResourcesOrBuilder(
        int index) {
      return resources_.get(index);
    }
    private void ensureResourcesIsMutable() {
      com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.ModelRateLimiter.Resource> tmp = resources_;
      if (!tmp.isModifiable()) {
        resources_ =
            com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
       }
    }

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
     *&#64;&#64;
     *&#64;&#64;     The resources required to execute the request on a model instance.
     *&#64;&#64;     Resources are just names with a corresponding count. The execution
     *&#64;&#64;     of the instance will be blocked until the specificied resources are
     *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
     */
    private void setResources(
        int index, inference.ModelConfigOuterClass.ModelRateLimiter.Resource value) {
      value.getClass();
  ensureResourcesIsMutable();
      resources_.set(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
     *&#64;&#64;
     *&#64;&#64;     The resources required to execute the request on a model instance.
     *&#64;&#64;     Resources are just names with a corresponding count. The execution
     *&#64;&#64;     of the instance will be blocked until the specificied resources are
     *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
     */
    private void addResources(inference.ModelConfigOuterClass.ModelRateLimiter.Resource value) {
      value.getClass();
  ensureResourcesIsMutable();
      resources_.add(value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
     *&#64;&#64;
     *&#64;&#64;     The resources required to execute the request on a model instance.
     *&#64;&#64;     Resources are just names with a corresponding count. The execution
     *&#64;&#64;     of the instance will be blocked until the specificied resources are
     *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
     */
    private void addResources(
        int index, inference.ModelConfigOuterClass.ModelRateLimiter.Resource value) {
      value.getClass();
  ensureResourcesIsMutable();
      resources_.add(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
     *&#64;&#64;
     *&#64;&#64;     The resources required to execute the request on a model instance.
     *&#64;&#64;     Resources are just names with a corresponding count. The execution
     *&#64;&#64;     of the instance will be blocked until the specificied resources are
     *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
     */
    private void addAllResources(
        java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelRateLimiter.Resource> values) {
      ensureResourcesIsMutable();
      com.google.protobuf.AbstractMessageLite.addAll(
          values, resources_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
     *&#64;&#64;
     *&#64;&#64;     The resources required to execute the request on a model instance.
     *&#64;&#64;     Resources are just names with a corresponding count. The execution
     *&#64;&#64;     of the instance will be blocked until the specificied resources are
     *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
     */
    private void clearResources() {
      resources_ = emptyProtobufList();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
     *&#64;&#64;
     *&#64;&#64;     The resources required to execute the request on a model instance.
     *&#64;&#64;     Resources are just names with a corresponding count. The execution
     *&#64;&#64;     of the instance will be blocked until the specificied resources are
     *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
     */
    private void removeResources(int index) {
      ensureResourcesIsMutable();
      resources_.remove(index);
    }

    public static final int PRIORITY_FIELD_NUMBER = 2;
    private int priority_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 priority
     *&#64;&#64;
     *&#64;&#64;     The weighting value to be used for prioritizing across instances.
     *&#64;&#64;     An instance with priority 2 will be given 1/2 the number of
     *&#64;&#64;     scheduling chances as an instance_group with priority 1. The
     *&#64;&#64;     default priority is 1.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 priority = 2;</code>
     * @return The priority.
     */
    @java.lang.Override
    public int getPriority() {
      return priority_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 priority
     *&#64;&#64;
     *&#64;&#64;     The weighting value to be used for prioritizing across instances.
     *&#64;&#64;     An instance with priority 2 will be given 1/2 the number of
     *&#64;&#64;     scheduling chances as an instance_group with priority 1. The
     *&#64;&#64;     default priority is 1.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 priority = 2;</code>
     * @param value The priority to set.
     */
    private void setPriority(int value) {
      
      priority_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 priority
     *&#64;&#64;
     *&#64;&#64;     The weighting value to be used for prioritizing across instances.
     *&#64;&#64;     An instance with priority 2 will be given 1/2 the number of
     *&#64;&#64;     scheduling chances as an instance_group with priority 1. The
     *&#64;&#64;     default priority is 1.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 priority = 2;</code>
     */
    private void clearPriority() {
      
      priority_ = 0;
    }

    public static inference.ModelConfigOuterClass.ModelRateLimiter parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelRateLimiter parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelRateLimiter parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelRateLimiter parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelRateLimiter parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelRateLimiter parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelRateLimiter parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelRateLimiter parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelRateLimiter parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelRateLimiter parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelRateLimiter parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelRateLimiter parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }

    public static Builder newBuilder() {
      return (Builder) DEFAULT_INSTANCE.createBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelRateLimiter prototype) {
      return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: message ModelRateLimiter
     *&#64;&#64;
     *&#64;&#64;     The specifications required by the rate limiter to properly
     *&#64;&#64;     schedule the inference requests across the different models
     *&#64;&#64;     and their instances.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelRateLimiter}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageLite.Builder<
          inference.ModelConfigOuterClass.ModelRateLimiter, Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelRateLimiter)
        inference.ModelConfigOuterClass.ModelRateLimiterOrBuilder {
      // Construct using inference.ModelConfigOuterClass.ModelRateLimiter.newBuilder()
      private Builder() {
        super(DEFAULT_INSTANCE);
      }


      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      @java.lang.Override
      public java.util.List<inference.ModelConfigOuterClass.ModelRateLimiter.Resource> getResourcesList() {
        return java.util.Collections.unmodifiableList(
            instance.getResourcesList());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      @java.lang.Override
      public int getResourcesCount() {
        return instance.getResourcesCount();
      }/**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelRateLimiter.Resource getResources(int index) {
        return instance.getResources(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public Builder setResources(
          int index, inference.ModelConfigOuterClass.ModelRateLimiter.Resource value) {
        copyOnWrite();
        instance.setResources(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public Builder setResources(
          int index, inference.ModelConfigOuterClass.ModelRateLimiter.Resource.Builder builderForValue) {
        copyOnWrite();
        instance.setResources(index,
            builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public Builder addResources(inference.ModelConfigOuterClass.ModelRateLimiter.Resource value) {
        copyOnWrite();
        instance.addResources(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public Builder addResources(
          int index, inference.ModelConfigOuterClass.ModelRateLimiter.Resource value) {
        copyOnWrite();
        instance.addResources(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public Builder addResources(
          inference.ModelConfigOuterClass.ModelRateLimiter.Resource.Builder builderForValue) {
        copyOnWrite();
        instance.addResources(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public Builder addResources(
          int index, inference.ModelConfigOuterClass.ModelRateLimiter.Resource.Builder builderForValue) {
        copyOnWrite();
        instance.addResources(index,
            builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public Builder addAllResources(
          java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelRateLimiter.Resource> values) {
        copyOnWrite();
        instance.addAllResources(values);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public Builder clearResources() {
        copyOnWrite();
        instance.clearResources();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public Builder removeResources(int index) {
        copyOnWrite();
        instance.removeResources(index);
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 priority
       *&#64;&#64;
       *&#64;&#64;     The weighting value to be used for prioritizing across instances.
       *&#64;&#64;     An instance with priority 2 will be given 1/2 the number of
       *&#64;&#64;     scheduling chances as an instance_group with priority 1. The
       *&#64;&#64;     default priority is 1.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 priority = 2;</code>
       * @return The priority.
       */
      @java.lang.Override
      public int getPriority() {
        return instance.getPriority();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 priority
       *&#64;&#64;
       *&#64;&#64;     The weighting value to be used for prioritizing across instances.
       *&#64;&#64;     An instance with priority 2 will be given 1/2 the number of
       *&#64;&#64;     scheduling chances as an instance_group with priority 1. The
       *&#64;&#64;     default priority is 1.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 priority = 2;</code>
       * @param value The priority to set.
       * @return This builder for chaining.
       */
      public Builder setPriority(int value) {
        copyOnWrite();
        instance.setPriority(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 priority
       *&#64;&#64;
       *&#64;&#64;     The weighting value to be used for prioritizing across instances.
       *&#64;&#64;     An instance with priority 2 will be given 1/2 the number of
       *&#64;&#64;     scheduling chances as an instance_group with priority 1. The
       *&#64;&#64;     default priority is 1.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 priority = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearPriority() {
        copyOnWrite();
        instance.clearPriority();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:inference.ModelRateLimiter)
    }
    @java.lang.Override
    @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
    protected final java.lang.Object dynamicMethod(
        com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
        java.lang.Object arg0, java.lang.Object arg1) {
      switch (method) {
        case NEW_MUTABLE_INSTANCE: {
          return new inference.ModelConfigOuterClass.ModelRateLimiter();
        }
        case NEW_BUILDER: {
          return new Builder();
        }
        case BUILD_MESSAGE_INFO: {
            java.lang.Object[] objects = new java.lang.Object[] {
              "resources_",
              inference.ModelConfigOuterClass.ModelRateLimiter.Resource.class,
              "priority_",
            };
            java.lang.String info =
                "\u0000\u0002\u0000\u0000\u0001\u0002\u0002\u0000\u0001\u0000\u0001\u001b\u0002\u000b" +
                "";
            return newMessageInfo(DEFAULT_INSTANCE, info, objects);
        }
        // fall through
        case GET_DEFAULT_INSTANCE: {
          return DEFAULT_INSTANCE;
        }
        case GET_PARSER: {
          com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelRateLimiter> parser = PARSER;
          if (parser == null) {
            synchronized (inference.ModelConfigOuterClass.ModelRateLimiter.class) {
              parser = PARSER;
              if (parser == null) {
                parser =
                    new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelRateLimiter>(
                        DEFAULT_INSTANCE);
                PARSER = parser;
              }
            }
          }
          return parser;
      }
      case GET_MEMOIZED_IS_INITIALIZED: {
        return (byte) 1;
      }
      case SET_MEMOIZED_IS_INITIALIZED: {
        return null;
      }
      }
      throw new UnsupportedOperationException();
    }


    // @@protoc_insertion_point(class_scope:inference.ModelRateLimiter)
    private static final inference.ModelConfigOuterClass.ModelRateLimiter DEFAULT_INSTANCE;
    static {
      ModelRateLimiter defaultInstance = new ModelRateLimiter();
      // New instances are implicitly immutable so no need to make
      // immutable.
      DEFAULT_INSTANCE = defaultInstance;
      com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
        ModelRateLimiter.class, defaultInstance);
    }

    public static inference.ModelConfigOuterClass.ModelRateLimiter getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static volatile com.google.protobuf.Parser<ModelRateLimiter> PARSER;

    public static com.google.protobuf.Parser<ModelRateLimiter> parser() {
      return DEFAULT_INSTANCE.getParserForType();
    }
  }

  public interface ModelInstanceGroupOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelInstanceGroup)
      com.google.protobuf.MessageLiteOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     Optional name of this group of instances. If not specified the
     *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
     *&#64;&#64;     individual instances will be further formed by a unique instance
     *&#64;&#64;     number and GPU index:
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     Optional name of this group of instances. If not specified the
     *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
     *&#64;&#64;     individual instances will be further formed by a unique instance
     *&#64;&#64;     number and GPU index:
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
     *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
     *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
     *&#64;&#64;     and 'gpu' cannot be specified.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelInstanceGroup.Kind kind = 4;</code>
     * @return The enum numeric value on the wire for kind.
     */
    int getKindValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
     *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
     *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
     *&#64;&#64;     and 'gpu' cannot be specified.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelInstanceGroup.Kind kind = 4;</code>
     * @return The kind.
     */
    inference.ModelConfigOuterClass.ModelInstanceGroup.Kind getKind();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 count
     *&#64;&#64;
     *&#64;&#64;     For a group assigned to GPU, the number of instances created for
     *&#64;&#64;     each GPU listed in 'gpus'. For a group assigned to CPU the number
     *&#64;&#64;     of instances created. Default is 1.
     * </pre>
     *
     * <code>int32 count = 2;</code>
     * @return The count.
     */
    int getCount();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
     *&#64;&#64;
     *&#64;&#64;     The rate limiter specific settings to be associated with this
     *&#64;&#64;     instance group. Optional, if not specified no rate limiting
     *&#64;&#64;     will be applied to this instance group.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
     * @return Whether the rateLimiter field is set.
     */
    boolean hasRateLimiter();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
     *&#64;&#64;
     *&#64;&#64;     The rate limiter specific settings to be associated with this
     *&#64;&#64;     instance group. Optional, if not specified no rate limiting
     *&#64;&#64;     will be applied to this instance group.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
     * @return The rateLimiter.
     */
    inference.ModelConfigOuterClass.ModelRateLimiter getRateLimiter();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     * @return A list containing the gpus.
     */
    java.util.List<java.lang.Integer> getGpusList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     * @return The count of gpus.
     */
    int getGpusCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     * @param index The index of the element to return.
     * @return The gpus at the given index.
     */
    int getGpus(int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated. If not specified, the server will select the first
     *&#64;&#64;     optimization profile by default.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     * @return A list containing the profile.
     */
    java.util.List<java.lang.String>
        getProfileList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated. If not specified, the server will select the first
     *&#64;&#64;     optimization profile by default.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     * @return The count of profile.
     */
    int getProfileCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated. If not specified, the server will select the first
     *&#64;&#64;     optimization profile by default.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     * @param index The index of the element to return.
     * @return The profile at the given index.
     */
    java.lang.String getProfile(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated. If not specified, the server will select the first
     *&#64;&#64;     optimization profile by default.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     * @param index The index of the element to return.
     * @return The profile at the given index.
     */
    com.google.protobuf.ByteString
        getProfileBytes(int index);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelInstanceGroup
   *&#64;&#64;
   *&#64;&#64;   A group of one or more instances of a model and resources made
   *&#64;&#64;   available for those instances.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelInstanceGroup}
   */
  public  static final class ModelInstanceGroup extends
      com.google.protobuf.GeneratedMessageLite<
          ModelInstanceGroup, ModelInstanceGroup.Builder> implements
      // @@protoc_insertion_point(message_implements:inference.ModelInstanceGroup)
      ModelInstanceGroupOrBuilder {
    private ModelInstanceGroup() {
      name_ = "";
      gpus_ = emptyIntList();
      profile_ = com.google.protobuf.GeneratedMessageLite.emptyProtobufList();
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:enum:: Kind
     *&#64;&#64;
     *&#64;&#64;     Kind of this instance group.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf enum {@code inference.ModelInstanceGroup.Kind}
     */
    public enum Kind
        implements com.google.protobuf.Internal.EnumLite {
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_AUTO = 0
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that can run on either
       *&#64;&#64;       CPU or GPU. If all GPUs listed in 'gpus' are available then
       *&#64;&#64;       instances will be created on GPU(s), otherwise instances will
       *&#64;&#64;       be created on CPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_AUTO = 0;</code>
       */
      KIND_AUTO(0),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_GPU = 1
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that must run on the
       *&#64;&#64;       GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_GPU = 1;</code>
       */
      KIND_GPU(1),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_CPU = 2
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that must run on the
       *&#64;&#64;       CPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_CPU = 2;</code>
       */
      KIND_CPU(2),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_MODEL = 3
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that should run on the
       *&#64;&#64;       CPU and/or GPU(s) as specified by the model or backend itself.
       *&#64;&#64;       The inference server will not override the model/backend
       *&#64;&#64;       settings.
       *&#64;&#64;       Currently, this option is supported only for Tensorflow models.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_MODEL = 3;</code>
       */
      KIND_MODEL(3),
      UNRECOGNIZED(-1),
      ;

      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_AUTO = 0
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that can run on either
       *&#64;&#64;       CPU or GPU. If all GPUs listed in 'gpus' are available then
       *&#64;&#64;       instances will be created on GPU(s), otherwise instances will
       *&#64;&#64;       be created on CPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_AUTO = 0;</code>
       */
      public static final int KIND_AUTO_VALUE = 0;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_GPU = 1
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that must run on the
       *&#64;&#64;       GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_GPU = 1;</code>
       */
      public static final int KIND_GPU_VALUE = 1;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_CPU = 2
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that must run on the
       *&#64;&#64;       CPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_CPU = 2;</code>
       */
      public static final int KIND_CPU_VALUE = 2;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_MODEL = 3
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that should run on the
       *&#64;&#64;       CPU and/or GPU(s) as specified by the model or backend itself.
       *&#64;&#64;       The inference server will not override the model/backend
       *&#64;&#64;       settings.
       *&#64;&#64;       Currently, this option is supported only for Tensorflow models.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_MODEL = 3;</code>
       */
      public static final int KIND_MODEL_VALUE = 3;


      @java.lang.Override
      public final int getNumber() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalArgumentException(
              "Can't get the number of an unknown enum value.");
        }
        return value;
      }

      /**
       * @param value The number of the enum to look for.
       * @return The enum associated with the given number.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static Kind valueOf(int value) {
        return forNumber(value);
      }

      public static Kind forNumber(int value) {
        switch (value) {
          case 0: return KIND_AUTO;
          case 1: return KIND_GPU;
          case 2: return KIND_CPU;
          case 3: return KIND_MODEL;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<Kind>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          Kind> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<Kind>() {
              @java.lang.Override
              public Kind findValueByNumber(int number) {
                return Kind.forNumber(number);
              }
            };

      public static com.google.protobuf.Internal.EnumVerifier 
          internalGetVerifier() {
        return KindVerifier.INSTANCE;
      }

      private static final class KindVerifier implements 
           com.google.protobuf.Internal.EnumVerifier { 
              static final com.google.protobuf.Internal.EnumVerifier           INSTANCE = new KindVerifier();
              @java.lang.Override
              public boolean isInRange(int number) {
                return Kind.forNumber(number) != null;
              }
            };

      private final int value;

      private Kind(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:inference.ModelInstanceGroup.Kind)
    }

    public static final int NAME_FIELD_NUMBER = 1;
    private java.lang.String name_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     Optional name of this group of instances. If not specified the
     *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
     *&#64;&#64;     individual instances will be further formed by a unique instance
     *&#64;&#64;     number and GPU index:
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      return name_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     Optional name of this group of instances. If not specified the
     *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
     *&#64;&#64;     individual instances will be further formed by a unique instance
     *&#64;&#64;     number and GPU index:
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      return com.google.protobuf.ByteString.copyFromUtf8(name_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     Optional name of this group of instances. If not specified the
     *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
     *&#64;&#64;     individual instances will be further formed by a unique instance
     *&#64;&#64;     number and GPU index:
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @param value The name to set.
     */
    private void setName(
        java.lang.String value) {
      java.lang.Class<?> valueClass = value.getClass();
  
      name_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     Optional name of this group of instances. If not specified the
     *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
     *&#64;&#64;     individual instances will be further formed by a unique instance
     *&#64;&#64;     number and GPU index:
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    private void clearName() {
      
      name_ = getDefaultInstance().getName();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     Optional name of this group of instances. If not specified the
     *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
     *&#64;&#64;     individual instances will be further formed by a unique instance
     *&#64;&#64;     number and GPU index:
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @param value The bytes for name to set.
     */
    private void setNameBytes(
        com.google.protobuf.ByteString value) {
      checkByteStringIsUtf8(value);
      name_ = value.toStringUtf8();
      
    }

    public static final int KIND_FIELD_NUMBER = 4;
    private int kind_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
     *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
     *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
     *&#64;&#64;     and 'gpu' cannot be specified.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelInstanceGroup.Kind kind = 4;</code>
     * @return The enum numeric value on the wire for kind.
     */
    @java.lang.Override
    public int getKindValue() {
      return kind_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
     *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
     *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
     *&#64;&#64;     and 'gpu' cannot be specified.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelInstanceGroup.Kind kind = 4;</code>
     * @return The kind.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelInstanceGroup.Kind getKind() {
      inference.ModelConfigOuterClass.ModelInstanceGroup.Kind result = inference.ModelConfigOuterClass.ModelInstanceGroup.Kind.forNumber(kind_);
      return result == null ? inference.ModelConfigOuterClass.ModelInstanceGroup.Kind.UNRECOGNIZED : result;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
     *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
     *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
     *&#64;&#64;     and 'gpu' cannot be specified.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelInstanceGroup.Kind kind = 4;</code>
     * @param value The enum numeric value on the wire for kind to set.
     */
    private void setKindValue(int value) {
        kind_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
     *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
     *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
     *&#64;&#64;     and 'gpu' cannot be specified.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelInstanceGroup.Kind kind = 4;</code>
     * @param value The kind to set.
     */
    private void setKind(inference.ModelConfigOuterClass.ModelInstanceGroup.Kind value) {
      kind_ = value.getNumber();
      
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
     *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
     *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
     *&#64;&#64;     and 'gpu' cannot be specified.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelInstanceGroup.Kind kind = 4;</code>
     */
    private void clearKind() {
      
      kind_ = 0;
    }

    public static final int COUNT_FIELD_NUMBER = 2;
    private int count_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 count
     *&#64;&#64;
     *&#64;&#64;     For a group assigned to GPU, the number of instances created for
     *&#64;&#64;     each GPU listed in 'gpus'. For a group assigned to CPU the number
     *&#64;&#64;     of instances created. Default is 1.
     * </pre>
     *
     * <code>int32 count = 2;</code>
     * @return The count.
     */
    @java.lang.Override
    public int getCount() {
      return count_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 count
     *&#64;&#64;
     *&#64;&#64;     For a group assigned to GPU, the number of instances created for
     *&#64;&#64;     each GPU listed in 'gpus'. For a group assigned to CPU the number
     *&#64;&#64;     of instances created. Default is 1.
     * </pre>
     *
     * <code>int32 count = 2;</code>
     * @param value The count to set.
     */
    private void setCount(int value) {
      
      count_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 count
     *&#64;&#64;
     *&#64;&#64;     For a group assigned to GPU, the number of instances created for
     *&#64;&#64;     each GPU listed in 'gpus'. For a group assigned to CPU the number
     *&#64;&#64;     of instances created. Default is 1.
     * </pre>
     *
     * <code>int32 count = 2;</code>
     */
    private void clearCount() {
      
      count_ = 0;
    }

    public static final int RATE_LIMITER_FIELD_NUMBER = 6;
    private inference.ModelConfigOuterClass.ModelRateLimiter rateLimiter_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
     *&#64;&#64;
     *&#64;&#64;     The rate limiter specific settings to be associated with this
     *&#64;&#64;     instance group. Optional, if not specified no rate limiting
     *&#64;&#64;     will be applied to this instance group.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
     */
    @java.lang.Override
    public boolean hasRateLimiter() {
      return rateLimiter_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
     *&#64;&#64;
     *&#64;&#64;     The rate limiter specific settings to be associated with this
     *&#64;&#64;     instance group. Optional, if not specified no rate limiting
     *&#64;&#64;     will be applied to this instance group.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelRateLimiter getRateLimiter() {
      return rateLimiter_ == null ? inference.ModelConfigOuterClass.ModelRateLimiter.getDefaultInstance() : rateLimiter_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
     *&#64;&#64;
     *&#64;&#64;     The rate limiter specific settings to be associated with this
     *&#64;&#64;     instance group. Optional, if not specified no rate limiting
     *&#64;&#64;     will be applied to this instance group.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
     */
    private void setRateLimiter(inference.ModelConfigOuterClass.ModelRateLimiter value) {
      value.getClass();
  rateLimiter_ = value;
      
      }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
     *&#64;&#64;
     *&#64;&#64;     The rate limiter specific settings to be associated with this
     *&#64;&#64;     instance group. Optional, if not specified no rate limiting
     *&#64;&#64;     will be applied to this instance group.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
     */
    @java.lang.SuppressWarnings({"ReferenceEquality"})
    private void mergeRateLimiter(inference.ModelConfigOuterClass.ModelRateLimiter value) {
      value.getClass();
  if (rateLimiter_ != null &&
          rateLimiter_ != inference.ModelConfigOuterClass.ModelRateLimiter.getDefaultInstance()) {
        rateLimiter_ =
          inference.ModelConfigOuterClass.ModelRateLimiter.newBuilder(rateLimiter_).mergeFrom(value).buildPartial();
      } else {
        rateLimiter_ = value;
      }
      
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
     *&#64;&#64;
     *&#64;&#64;     The rate limiter specific settings to be associated with this
     *&#64;&#64;     instance group. Optional, if not specified no rate limiting
     *&#64;&#64;     will be applied to this instance group.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
     */
    private void clearRateLimiter() {  rateLimiter_ = null;
      
    }

    public static final int GPUS_FIELD_NUMBER = 3;
    private com.google.protobuf.Internal.IntList gpus_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     * @return A list containing the gpus.
     */
    @java.lang.Override
    public java.util.List<java.lang.Integer>
        getGpusList() {
      return gpus_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     * @return The count of gpus.
     */
    @java.lang.Override
    public int getGpusCount() {
      return gpus_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     * @param index The index of the element to return.
     * @return The gpus at the given index.
     */
    @java.lang.Override
    public int getGpus(int index) {
      return gpus_.getInt(index);
    }
    private int gpusMemoizedSerializedSize = -1;
    private void ensureGpusIsMutable() {
      com.google.protobuf.Internal.IntList tmp = gpus_;
      if (!tmp.isModifiable()) {
        gpus_ =
            com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
       }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     * @param index The index to set the value at.
     * @param value The gpus to set.
     */
    private void setGpus(
        int index, int value) {
      ensureGpusIsMutable();
      gpus_.setInt(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     * @param value The gpus to add.
     */
    private void addGpus(int value) {
      ensureGpusIsMutable();
      gpus_.addInt(value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     * @param values The gpus to add.
     */
    private void addAllGpus(
        java.lang.Iterable<? extends java.lang.Integer> values) {
      ensureGpusIsMutable();
      com.google.protobuf.AbstractMessageLite.addAll(
          values, gpus_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     */
    private void clearGpus() {
      gpus_ = emptyIntList();
    }

    public static final int PROFILE_FIELD_NUMBER = 5;
    private com.google.protobuf.Internal.ProtobufList<java.lang.String> profile_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated. If not specified, the server will select the first
     *&#64;&#64;     optimization profile by default.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     * @return A list containing the profile.
     */
    @java.lang.Override
    public java.util.List<java.lang.String> getProfileList() {
      return profile_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated. If not specified, the server will select the first
     *&#64;&#64;     optimization profile by default.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     * @return The count of profile.
     */
    @java.lang.Override
    public int getProfileCount() {
      return profile_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated. If not specified, the server will select the first
     *&#64;&#64;     optimization profile by default.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     * @param index The index of the element to return.
     * @return The profile at the given index.
     */
    @java.lang.Override
    public java.lang.String getProfile(int index) {
      return profile_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated. If not specified, the server will select the first
     *&#64;&#64;     optimization profile by default.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     * @param index The index of the value to return.
     * @return The bytes of the profile at the given index.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getProfileBytes(int index) {
      return com.google.protobuf.ByteString.copyFromUtf8(
          profile_.get(index));
    }
    private void ensureProfileIsMutable() {
      com.google.protobuf.Internal.ProtobufList<java.lang.String> tmp =
          profile_;  if (!tmp.isModifiable()) {
        profile_ =
            com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
       }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated. If not specified, the server will select the first
     *&#64;&#64;     optimization profile by default.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     * @param index The index to set the value at.
     * @param value The profile to set.
     */
    private void setProfile(
        int index, java.lang.String value) {
      java.lang.Class<?> valueClass = value.getClass();
  ensureProfileIsMutable();
      profile_.set(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated. If not specified, the server will select the first
     *&#64;&#64;     optimization profile by default.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     * @param value The profile to add.
     */
    private void addProfile(
        java.lang.String value) {
      java.lang.Class<?> valueClass = value.getClass();
  ensureProfileIsMutable();
      profile_.add(value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated. If not specified, the server will select the first
     *&#64;&#64;     optimization profile by default.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     * @param values The profile to add.
     */
    private void addAllProfile(
        java.lang.Iterable<java.lang.String> values) {
      ensureProfileIsMutable();
      com.google.protobuf.AbstractMessageLite.addAll(
          values, profile_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated. If not specified, the server will select the first
     *&#64;&#64;     optimization profile by default.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     */
    private void clearProfile() {
      profile_ = com.google.protobuf.GeneratedMessageLite.emptyProtobufList();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated. If not specified, the server will select the first
     *&#64;&#64;     optimization profile by default.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     * @param value The bytes of the profile to add.
     */
    private void addProfileBytes(
        com.google.protobuf.ByteString value) {
      checkByteStringIsUtf8(value);
      ensureProfileIsMutable();
      profile_.add(value.toStringUtf8());
    }

    public static inference.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelInstanceGroup parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelInstanceGroup parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelInstanceGroup parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelInstanceGroup parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }

    public static Builder newBuilder() {
      return (Builder) DEFAULT_INSTANCE.createBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelInstanceGroup prototype) {
      return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelInstanceGroup
     *&#64;&#64;
     *&#64;&#64;   A group of one or more instances of a model and resources made
     *&#64;&#64;   available for those instances.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelInstanceGroup}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageLite.Builder<
          inference.ModelConfigOuterClass.ModelInstanceGroup, Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelInstanceGroup)
        inference.ModelConfigOuterClass.ModelInstanceGroupOrBuilder {
      // Construct using inference.ModelConfigOuterClass.ModelInstanceGroup.newBuilder()
      private Builder() {
        super(DEFAULT_INSTANCE);
      }


      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     Optional name of this group of instances. If not specified the
       *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
       *&#64;&#64;     individual instances will be further formed by a unique instance
       *&#64;&#64;     number and GPU index:
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The name.
       */
      @java.lang.Override
      public java.lang.String getName() {
        return instance.getName();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     Optional name of this group of instances. If not specified the
       *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
       *&#64;&#64;     individual instances will be further formed by a unique instance
       *&#64;&#64;     number and GPU index:
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString
          getNameBytes() {
        return instance.getNameBytes();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     Optional name of this group of instances. If not specified the
       *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
       *&#64;&#64;     individual instances will be further formed by a unique instance
       *&#64;&#64;     number and GPU index:
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        copyOnWrite();
        instance.setName(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     Optional name of this group of instances. If not specified the
       *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
       *&#64;&#64;     individual instances will be further formed by a unique instance
       *&#64;&#64;     number and GPU index:
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        copyOnWrite();
        instance.clearName();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     Optional name of this group of instances. If not specified the
       *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
       *&#64;&#64;     individual instances will be further formed by a unique instance
       *&#64;&#64;     number and GPU index:
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        copyOnWrite();
        instance.setNameBytes(value);
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
       *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
       *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
       *&#64;&#64;     and 'gpu' cannot be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelInstanceGroup.Kind kind = 4;</code>
       * @return The enum numeric value on the wire for kind.
       */
      @java.lang.Override
      public int getKindValue() {
        return instance.getKindValue();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
       *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
       *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
       *&#64;&#64;     and 'gpu' cannot be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelInstanceGroup.Kind kind = 4;</code>
       * @param value The kind to set.
       * @return This builder for chaining.
       */
      public Builder setKindValue(int value) {
        copyOnWrite();
        instance.setKindValue(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
       *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
       *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
       *&#64;&#64;     and 'gpu' cannot be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelInstanceGroup.Kind kind = 4;</code>
       * @return The kind.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelInstanceGroup.Kind getKind() {
        return instance.getKind();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
       *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
       *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
       *&#64;&#64;     and 'gpu' cannot be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelInstanceGroup.Kind kind = 4;</code>
       * @param value The enum numeric value on the wire for kind to set.
       * @return This builder for chaining.
       */
      public Builder setKind(inference.ModelConfigOuterClass.ModelInstanceGroup.Kind value) {
        copyOnWrite();
        instance.setKind(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
       *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
       *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
       *&#64;&#64;     and 'gpu' cannot be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelInstanceGroup.Kind kind = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearKind() {
        copyOnWrite();
        instance.clearKind();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 count
       *&#64;&#64;
       *&#64;&#64;     For a group assigned to GPU, the number of instances created for
       *&#64;&#64;     each GPU listed in 'gpus'. For a group assigned to CPU the number
       *&#64;&#64;     of instances created. Default is 1.
       * </pre>
       *
       * <code>int32 count = 2;</code>
       * @return The count.
       */
      @java.lang.Override
      public int getCount() {
        return instance.getCount();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 count
       *&#64;&#64;
       *&#64;&#64;     For a group assigned to GPU, the number of instances created for
       *&#64;&#64;     each GPU listed in 'gpus'. For a group assigned to CPU the number
       *&#64;&#64;     of instances created. Default is 1.
       * </pre>
       *
       * <code>int32 count = 2;</code>
       * @param value The count to set.
       * @return This builder for chaining.
       */
      public Builder setCount(int value) {
        copyOnWrite();
        instance.setCount(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 count
       *&#64;&#64;
       *&#64;&#64;     For a group assigned to GPU, the number of instances created for
       *&#64;&#64;     each GPU listed in 'gpus'. For a group assigned to CPU the number
       *&#64;&#64;     of instances created. Default is 1.
       * </pre>
       *
       * <code>int32 count = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearCount() {
        copyOnWrite();
        instance.clearCount();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
       *&#64;&#64;
       *&#64;&#64;     The rate limiter specific settings to be associated with this
       *&#64;&#64;     instance group. Optional, if not specified no rate limiting
       *&#64;&#64;     will be applied to this instance group.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
       */
      @java.lang.Override
      public boolean hasRateLimiter() {
        return instance.hasRateLimiter();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
       *&#64;&#64;
       *&#64;&#64;     The rate limiter specific settings to be associated with this
       *&#64;&#64;     instance group. Optional, if not specified no rate limiting
       *&#64;&#64;     will be applied to this instance group.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelRateLimiter getRateLimiter() {
        return instance.getRateLimiter();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
       *&#64;&#64;
       *&#64;&#64;     The rate limiter specific settings to be associated with this
       *&#64;&#64;     instance group. Optional, if not specified no rate limiting
       *&#64;&#64;     will be applied to this instance group.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
       */
      public Builder setRateLimiter(inference.ModelConfigOuterClass.ModelRateLimiter value) {
        copyOnWrite();
        instance.setRateLimiter(value);
        return this;
        }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
       *&#64;&#64;
       *&#64;&#64;     The rate limiter specific settings to be associated with this
       *&#64;&#64;     instance group. Optional, if not specified no rate limiting
       *&#64;&#64;     will be applied to this instance group.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
       */
      public Builder setRateLimiter(
          inference.ModelConfigOuterClass.ModelRateLimiter.Builder builderForValue) {
        copyOnWrite();
        instance.setRateLimiter(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
       *&#64;&#64;
       *&#64;&#64;     The rate limiter specific settings to be associated with this
       *&#64;&#64;     instance group. Optional, if not specified no rate limiting
       *&#64;&#64;     will be applied to this instance group.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
       */
      public Builder mergeRateLimiter(inference.ModelConfigOuterClass.ModelRateLimiter value) {
        copyOnWrite();
        instance.mergeRateLimiter(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
       *&#64;&#64;
       *&#64;&#64;     The rate limiter specific settings to be associated with this
       *&#64;&#64;     instance group. Optional, if not specified no rate limiting
       *&#64;&#64;     will be applied to this instance group.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
       */
      public Builder clearRateLimiter() {  copyOnWrite();
        instance.clearRateLimiter();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       * @return A list containing the gpus.
       */
      @java.lang.Override
      public java.util.List<java.lang.Integer>
          getGpusList() {
        return java.util.Collections.unmodifiableList(
            instance.getGpusList());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       * @return The count of gpus.
       */
      @java.lang.Override
      public int getGpusCount() {
        return instance.getGpusCount();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       * @param index The index of the element to return.
       * @return The gpus at the given index.
       */
      @java.lang.Override
      public int getGpus(int index) {
        return instance.getGpus(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       * @param value The gpus to set.
       * @return This builder for chaining.
       */
      public Builder setGpus(
          int index, int value) {
        copyOnWrite();
        instance.setGpus(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       * @param value The gpus to add.
       * @return This builder for chaining.
       */
      public Builder addGpus(int value) {
        copyOnWrite();
        instance.addGpus(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       * @param values The gpus to add.
       * @return This builder for chaining.
       */
      public Builder addAllGpus(
          java.lang.Iterable<? extends java.lang.Integer> values) {
        copyOnWrite();
        instance.addAllGpus(values);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearGpus() {
        copyOnWrite();
        instance.clearGpus();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated. If not specified, the server will select the first
       *&#64;&#64;     optimization profile by default.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       * @return A list containing the profile.
       */
      @java.lang.Override
      public java.util.List<java.lang.String>
          getProfileList() {
        return java.util.Collections.unmodifiableList(
            instance.getProfileList());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated. If not specified, the server will select the first
       *&#64;&#64;     optimization profile by default.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       * @return The count of profile.
       */
      @java.lang.Override
      public int getProfileCount() {
        return instance.getProfileCount();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated. If not specified, the server will select the first
       *&#64;&#64;     optimization profile by default.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       * @param index The index of the element to return.
       * @return The profile at the given index.
       */
      @java.lang.Override
      public java.lang.String getProfile(int index) {
        return instance.getProfile(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated. If not specified, the server will select the first
       *&#64;&#64;     optimization profile by default.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       * @param index The index of the value to return.
       * @return The bytes of the profile at the given index.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString
          getProfileBytes(int index) {
        return instance.getProfileBytes(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated. If not specified, the server will select the first
       *&#64;&#64;     optimization profile by default.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       * @param index The index to set the value at.
       * @param value The profile to set.
       * @return This builder for chaining.
       */
      public Builder setProfile(
          int index, java.lang.String value) {
        copyOnWrite();
        instance.setProfile(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated. If not specified, the server will select the first
       *&#64;&#64;     optimization profile by default.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       * @param value The profile to add.
       * @return This builder for chaining.
       */
      public Builder addProfile(
          java.lang.String value) {
        copyOnWrite();
        instance.addProfile(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated. If not specified, the server will select the first
       *&#64;&#64;     optimization profile by default.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       * @param values The profile to add.
       * @return This builder for chaining.
       */
      public Builder addAllProfile(
          java.lang.Iterable<java.lang.String> values) {
        copyOnWrite();
        instance.addAllProfile(values);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated. If not specified, the server will select the first
       *&#64;&#64;     optimization profile by default.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearProfile() {
        copyOnWrite();
        instance.clearProfile();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated. If not specified, the server will select the first
       *&#64;&#64;     optimization profile by default.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       * @param value The bytes of the profile to add.
       * @return This builder for chaining.
       */
      public Builder addProfileBytes(
          com.google.protobuf.ByteString value) {
        copyOnWrite();
        instance.addProfileBytes(value);
        return this;
      }

      // @@protoc_insertion_point(builder_scope:inference.ModelInstanceGroup)
    }
    @java.lang.Override
    @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
    protected final java.lang.Object dynamicMethod(
        com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
        java.lang.Object arg0, java.lang.Object arg1) {
      switch (method) {
        case NEW_MUTABLE_INSTANCE: {
          return new inference.ModelConfigOuterClass.ModelInstanceGroup();
        }
        case NEW_BUILDER: {
          return new Builder();
        }
        case BUILD_MESSAGE_INFO: {
            java.lang.Object[] objects = new java.lang.Object[] {
              "name_",
              "count_",
              "gpus_",
              "kind_",
              "profile_",
              "rateLimiter_",
            };
            java.lang.String info =
                "\u0000\u0006\u0000\u0000\u0001\u0006\u0006\u0000\u0002\u0000\u0001\u0208\u0002\u0004" +
                "\u0003\'\u0004\f\u0005\u021a\u0006\t";
            return newMessageInfo(DEFAULT_INSTANCE, info, objects);
        }
        // fall through
        case GET_DEFAULT_INSTANCE: {
          return DEFAULT_INSTANCE;
        }
        case GET_PARSER: {
          com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelInstanceGroup> parser = PARSER;
          if (parser == null) {
            synchronized (inference.ModelConfigOuterClass.ModelInstanceGroup.class) {
              parser = PARSER;
              if (parser == null) {
                parser =
                    new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelInstanceGroup>(
                        DEFAULT_INSTANCE);
                PARSER = parser;
              }
            }
          }
          return parser;
      }
      case GET_MEMOIZED_IS_INITIALIZED: {
        return (byte) 1;
      }
      case SET_MEMOIZED_IS_INITIALIZED: {
        return null;
      }
      }
      throw new UnsupportedOperationException();
    }


    // @@protoc_insertion_point(class_scope:inference.ModelInstanceGroup)
    private static final inference.ModelConfigOuterClass.ModelInstanceGroup DEFAULT_INSTANCE;
    static {
      ModelInstanceGroup defaultInstance = new ModelInstanceGroup();
      // New instances are implicitly immutable so no need to make
      // immutable.
      DEFAULT_INSTANCE = defaultInstance;
      com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
        ModelInstanceGroup.class, defaultInstance);
    }

    public static inference.ModelConfigOuterClass.ModelInstanceGroup getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static volatile com.google.protobuf.Parser<ModelInstanceGroup> PARSER;

    public static com.google.protobuf.Parser<ModelInstanceGroup> parser() {
      return DEFAULT_INSTANCE.getParserForType();
    }
  }

  public interface ModelTensorReshapeOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelTensorReshape)
      com.google.protobuf.MessageLiteOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     * @return A list containing the shape.
     */
    java.util.List<java.lang.Long> getShapeList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     * @return The count of shape.
     */
    int getShapeCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     * @param index The index of the element to return.
     * @return The shape at the given index.
     */
    long getShape(int index);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelTensorReshape
   *&#64;&#64;
   *&#64;&#64;   Reshape specification for input and output tensors.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelTensorReshape}
   */
  public  static final class ModelTensorReshape extends
      com.google.protobuf.GeneratedMessageLite<
          ModelTensorReshape, ModelTensorReshape.Builder> implements
      // @@protoc_insertion_point(message_implements:inference.ModelTensorReshape)
      ModelTensorReshapeOrBuilder {
    private ModelTensorReshape() {
      shape_ = emptyLongList();
    }
    public static final int SHAPE_FIELD_NUMBER = 1;
    private com.google.protobuf.Internal.LongList shape_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     * @return A list containing the shape.
     */
    @java.lang.Override
    public java.util.List<java.lang.Long>
        getShapeList() {
      return shape_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     * @return The count of shape.
     */
    @java.lang.Override
    public int getShapeCount() {
      return shape_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     * @param index The index of the element to return.
     * @return The shape at the given index.
     */
    @java.lang.Override
    public long getShape(int index) {
      return shape_.getLong(index);
    }
    private int shapeMemoizedSerializedSize = -1;
    private void ensureShapeIsMutable() {
      com.google.protobuf.Internal.LongList tmp = shape_;
      if (!tmp.isModifiable()) {
        shape_ =
            com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
       }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     * @param index The index to set the value at.
     * @param value The shape to set.
     */
    private void setShape(
        int index, long value) {
      ensureShapeIsMutable();
      shape_.setLong(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     * @param value The shape to add.
     */
    private void addShape(long value) {
      ensureShapeIsMutable();
      shape_.addLong(value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     * @param values The shape to add.
     */
    private void addAllShape(
        java.lang.Iterable<? extends java.lang.Long> values) {
      ensureShapeIsMutable();
      com.google.protobuf.AbstractMessageLite.addAll(
          values, shape_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     */
    private void clearShape() {
      shape_ = emptyLongList();
    }

    public static inference.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelTensorReshape parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelTensorReshape parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelTensorReshape parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelTensorReshape parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }

    public static Builder newBuilder() {
      return (Builder) DEFAULT_INSTANCE.createBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelTensorReshape prototype) {
      return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelTensorReshape
     *&#64;&#64;
     *&#64;&#64;   Reshape specification for input and output tensors.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelTensorReshape}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageLite.Builder<
          inference.ModelConfigOuterClass.ModelTensorReshape, Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelTensorReshape)
        inference.ModelConfigOuterClass.ModelTensorReshapeOrBuilder {
      // Construct using inference.ModelConfigOuterClass.ModelTensorReshape.newBuilder()
      private Builder() {
        super(DEFAULT_INSTANCE);
      }


      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       * @return A list containing the shape.
       */
      @java.lang.Override
      public java.util.List<java.lang.Long>
          getShapeList() {
        return java.util.Collections.unmodifiableList(
            instance.getShapeList());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       * @return The count of shape.
       */
      @java.lang.Override
      public int getShapeCount() {
        return instance.getShapeCount();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       * @param index The index of the element to return.
       * @return The shape at the given index.
       */
      @java.lang.Override
      public long getShape(int index) {
        return instance.getShape(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       * @param value The shape to set.
       * @return This builder for chaining.
       */
      public Builder setShape(
          int index, long value) {
        copyOnWrite();
        instance.setShape(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       * @param value The shape to add.
       * @return This builder for chaining.
       */
      public Builder addShape(long value) {
        copyOnWrite();
        instance.addShape(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       * @param values The shape to add.
       * @return This builder for chaining.
       */
      public Builder addAllShape(
          java.lang.Iterable<? extends java.lang.Long> values) {
        copyOnWrite();
        instance.addAllShape(values);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearShape() {
        copyOnWrite();
        instance.clearShape();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:inference.ModelTensorReshape)
    }
    @java.lang.Override
    @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
    protected final java.lang.Object dynamicMethod(
        com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
        java.lang.Object arg0, java.lang.Object arg1) {
      switch (method) {
        case NEW_MUTABLE_INSTANCE: {
          return new inference.ModelConfigOuterClass.ModelTensorReshape();
        }
        case NEW_BUILDER: {
          return new Builder();
        }
        case BUILD_MESSAGE_INFO: {
            java.lang.Object[] objects = new java.lang.Object[] {
              "shape_",
            };
            java.lang.String info =
                "\u0000\u0001\u0000\u0000\u0001\u0001\u0001\u0000\u0001\u0000\u0001%";
            return newMessageInfo(DEFAULT_INSTANCE, info, objects);
        }
        // fall through
        case GET_DEFAULT_INSTANCE: {
          return DEFAULT_INSTANCE;
        }
        case GET_PARSER: {
          com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelTensorReshape> parser = PARSER;
          if (parser == null) {
            synchronized (inference.ModelConfigOuterClass.ModelTensorReshape.class) {
              parser = PARSER;
              if (parser == null) {
                parser =
                    new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelTensorReshape>(
                        DEFAULT_INSTANCE);
                PARSER = parser;
              }
            }
          }
          return parser;
      }
      case GET_MEMOIZED_IS_INITIALIZED: {
        return (byte) 1;
      }
      case SET_MEMOIZED_IS_INITIALIZED: {
        return null;
      }
      }
      throw new UnsupportedOperationException();
    }


    // @@protoc_insertion_point(class_scope:inference.ModelTensorReshape)
    private static final inference.ModelConfigOuterClass.ModelTensorReshape DEFAULT_INSTANCE;
    static {
      ModelTensorReshape defaultInstance = new ModelTensorReshape();
      // New instances are implicitly immutable so no need to make
      // immutable.
      DEFAULT_INSTANCE = defaultInstance;
      com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
        ModelTensorReshape.class, defaultInstance);
    }

    public static inference.ModelConfigOuterClass.ModelTensorReshape getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static volatile com.google.protobuf.Parser<ModelTensorReshape> PARSER;

    public static com.google.protobuf.Parser<ModelTensorReshape> parser() {
      return DEFAULT_INSTANCE.getParserForType();
    }
  }

  public interface ModelInputOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelInput)
      com.google.protobuf.MessageLiteOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 2;</code>
     * @return The enum numeric value on the wire for dataType.
     */
    int getDataTypeValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 2;</code>
     * @return The dataType.
     */
    inference.ModelConfigOuterClass.DataType getDataType();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Format format
     *&#64;&#64;
     *&#64;&#64;     The format of the input. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelInput.Format format = 3;</code>
     * @return The enum numeric value on the wire for format.
     */
    int getFormatValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Format format
     *&#64;&#64;
     *&#64;&#64;     The format of the input. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelInput.Format format = 3;</code>
     * @return The format.
     */
    inference.ModelConfigOuterClass.ModelInput.Format getFormat();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     * @return A list containing the dims.
     */
    java.util.List<java.lang.Long> getDimsList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     * @return The count of dims.
     */
    int getDimsCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     * @param index The index of the element to return.
     * @return The dims at the given index.
     */
    long getDims(int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape expected for this input by the backend. The input will
     *&#64;&#64;     be reshaped to this before being presented to the backend. The
     *&#64;&#64;     reshape must have the same number of elements as the input shape
     *&#64;&#64;     specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     * @return Whether the reshape field is set.
     */
    boolean hasReshape();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape expected for this input by the backend. The input will
     *&#64;&#64;     be reshaped to this before being presented to the backend. The
     *&#64;&#64;     reshape must have the same number of elements as the input shape
     *&#64;&#64;     specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     * @return The reshape.
     */
    inference.ModelConfigOuterClass.ModelTensorReshape getReshape();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool is_shape_tensor
     *&#64;&#64;
     *&#64;&#64;     Whether or not the input is a shape tensor to the model. This field
     *&#64;&#64;     is currently supported only for the TensorRT model. An error will be
     *&#64;&#64;     generated if this specification does not comply with underlying
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool is_shape_tensor = 6;</code>
     * @return The isShapeTensor.
     */
    boolean getIsShapeTensor();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool allow_ragged_batch
     *&#64;&#64;
     *&#64;&#64;     Whether or not the input is allowed to be "ragged" in a dynamically
     *&#64;&#64;     created batch. Default is false indicating that two requests will
     *&#64;&#64;     only be batched if this tensor has the same shape in both requests.
     *&#64;&#64;     True indicates that two requests can be batched even if this tensor
     *&#64;&#64;     has a different shape in each request. A true value is currently
     *&#64;&#64;     supported only for custom models.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool allow_ragged_batch = 7;</code>
     * @return The allowRaggedBatch.
     */
    boolean getAllowRaggedBatch();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelInput
   *&#64;&#64;
   *&#64;&#64;   An input required by the model.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelInput}
   */
  public  static final class ModelInput extends
      com.google.protobuf.GeneratedMessageLite<
          ModelInput, ModelInput.Builder> implements
      // @@protoc_insertion_point(message_implements:inference.ModelInput)
      ModelInputOrBuilder {
    private ModelInput() {
      name_ = "";
      dims_ = emptyLongList();
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:enum:: Format
     *&#64;&#64;
     *&#64;&#64;     The format for the input.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf enum {@code inference.ModelInput.Format}
     */
    public enum Format
        implements com.google.protobuf.Internal.EnumLite {
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Format::FORMAT_NONE = 0
       *&#64;&#64;
       *&#64;&#64;       The input has no specific format. This is the default.
       *&#64;&#64;
       * </pre>
       *
       * <code>FORMAT_NONE = 0;</code>
       */
      FORMAT_NONE(0),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Format::FORMAT_NHWC = 1
       *&#64;&#64;
       *&#64;&#64;       HWC image format. Tensors with this format require 3 dimensions
       *&#64;&#64;       if the model does not support batching (max_batch_size = 0) or 4
       *&#64;&#64;       dimensions if the model does support batching (max_batch_size
       *&#64;&#64;       &gt;= 1). In either case the 'dims' below should only specify the
       *&#64;&#64;       3 non-batch dimensions (i.e. HWC or CHW).
       *&#64;&#64;
       * </pre>
       *
       * <code>FORMAT_NHWC = 1;</code>
       */
      FORMAT_NHWC(1),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Format::FORMAT_NCHW = 2
       *&#64;&#64;
       *&#64;&#64;       CHW image format. Tensors with this format require 3 dimensions
       *&#64;&#64;       if the model does not support batching (max_batch_size = 0) or 4
       *&#64;&#64;       dimensions if the model does support batching (max_batch_size
       *&#64;&#64;       &gt;= 1). In either case the 'dims' below should only specify the
       *&#64;&#64;       3 non-batch dimensions (i.e. HWC or CHW).
       *&#64;&#64;
       * </pre>
       *
       * <code>FORMAT_NCHW = 2;</code>
       */
      FORMAT_NCHW(2),
      UNRECOGNIZED(-1),
      ;

      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Format::FORMAT_NONE = 0
       *&#64;&#64;
       *&#64;&#64;       The input has no specific format. This is the default.
       *&#64;&#64;
       * </pre>
       *
       * <code>FORMAT_NONE = 0;</code>
       */
      public static final int FORMAT_NONE_VALUE = 0;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Format::FORMAT_NHWC = 1
       *&#64;&#64;
       *&#64;&#64;       HWC image format. Tensors with this format require 3 dimensions
       *&#64;&#64;       if the model does not support batching (max_batch_size = 0) or 4
       *&#64;&#64;       dimensions if the model does support batching (max_batch_size
       *&#64;&#64;       &gt;= 1). In either case the 'dims' below should only specify the
       *&#64;&#64;       3 non-batch dimensions (i.e. HWC or CHW).
       *&#64;&#64;
       * </pre>
       *
       * <code>FORMAT_NHWC = 1;</code>
       */
      public static final int FORMAT_NHWC_VALUE = 1;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Format::FORMAT_NCHW = 2
       *&#64;&#64;
       *&#64;&#64;       CHW image format. Tensors with this format require 3 dimensions
       *&#64;&#64;       if the model does not support batching (max_batch_size = 0) or 4
       *&#64;&#64;       dimensions if the model does support batching (max_batch_size
       *&#64;&#64;       &gt;= 1). In either case the 'dims' below should only specify the
       *&#64;&#64;       3 non-batch dimensions (i.e. HWC or CHW).
       *&#64;&#64;
       * </pre>
       *
       * <code>FORMAT_NCHW = 2;</code>
       */
      public static final int FORMAT_NCHW_VALUE = 2;


      @java.lang.Override
      public final int getNumber() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalArgumentException(
              "Can't get the number of an unknown enum value.");
        }
        return value;
      }

      /**
       * @param value The number of the enum to look for.
       * @return The enum associated with the given number.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static Format valueOf(int value) {
        return forNumber(value);
      }

      public static Format forNumber(int value) {
        switch (value) {
          case 0: return FORMAT_NONE;
          case 1: return FORMAT_NHWC;
          case 2: return FORMAT_NCHW;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<Format>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          Format> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<Format>() {
              @java.lang.Override
              public Format findValueByNumber(int number) {
                return Format.forNumber(number);
              }
            };

      public static com.google.protobuf.Internal.EnumVerifier 
          internalGetVerifier() {
        return FormatVerifier.INSTANCE;
      }

      private static final class FormatVerifier implements 
           com.google.protobuf.Internal.EnumVerifier { 
              static final com.google.protobuf.Internal.EnumVerifier           INSTANCE = new FormatVerifier();
              @java.lang.Override
              public boolean isInRange(int number) {
                return Format.forNumber(number) != null;
              }
            };

      private final int value;

      private Format(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:inference.ModelInput.Format)
    }

    public static final int NAME_FIELD_NUMBER = 1;
    private java.lang.String name_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      return name_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      return com.google.protobuf.ByteString.copyFromUtf8(name_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @param value The name to set.
     */
    private void setName(
        java.lang.String value) {
      java.lang.Class<?> valueClass = value.getClass();
  
      name_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    private void clearName() {
      
      name_ = getDefaultInstance().getName();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @param value The bytes for name to set.
     */
    private void setNameBytes(
        com.google.protobuf.ByteString value) {
      checkByteStringIsUtf8(value);
      name_ = value.toStringUtf8();
      
    }

    public static final int DATA_TYPE_FIELD_NUMBER = 2;
    private int dataType_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 2;</code>
     * @return The enum numeric value on the wire for dataType.
     */
    @java.lang.Override
    public int getDataTypeValue() {
      return dataType_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 2;</code>
     * @return The dataType.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.DataType getDataType() {
      inference.ModelConfigOuterClass.DataType result = inference.ModelConfigOuterClass.DataType.forNumber(dataType_);
      return result == null ? inference.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 2;</code>
     * @param value The enum numeric value on the wire for dataType to set.
     */
    private void setDataTypeValue(int value) {
        dataType_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 2;</code>
     * @param value The dataType to set.
     */
    private void setDataType(inference.ModelConfigOuterClass.DataType value) {
      dataType_ = value.getNumber();
      
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 2;</code>
     */
    private void clearDataType() {
      
      dataType_ = 0;
    }

    public static final int FORMAT_FIELD_NUMBER = 3;
    private int format_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Format format
     *&#64;&#64;
     *&#64;&#64;     The format of the input. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelInput.Format format = 3;</code>
     * @return The enum numeric value on the wire for format.
     */
    @java.lang.Override
    public int getFormatValue() {
      return format_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Format format
     *&#64;&#64;
     *&#64;&#64;     The format of the input. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelInput.Format format = 3;</code>
     * @return The format.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelInput.Format getFormat() {
      inference.ModelConfigOuterClass.ModelInput.Format result = inference.ModelConfigOuterClass.ModelInput.Format.forNumber(format_);
      return result == null ? inference.ModelConfigOuterClass.ModelInput.Format.UNRECOGNIZED : result;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Format format
     *&#64;&#64;
     *&#64;&#64;     The format of the input. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelInput.Format format = 3;</code>
     * @param value The enum numeric value on the wire for format to set.
     */
    private void setFormatValue(int value) {
        format_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Format format
     *&#64;&#64;
     *&#64;&#64;     The format of the input. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelInput.Format format = 3;</code>
     * @param value The format to set.
     */
    private void setFormat(inference.ModelConfigOuterClass.ModelInput.Format value) {
      format_ = value.getNumber();
      
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Format format
     *&#64;&#64;
     *&#64;&#64;     The format of the input. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelInput.Format format = 3;</code>
     */
    private void clearFormat() {
      
      format_ = 0;
    }

    public static final int DIMS_FIELD_NUMBER = 4;
    private com.google.protobuf.Internal.LongList dims_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     * @return A list containing the dims.
     */
    @java.lang.Override
    public java.util.List<java.lang.Long>
        getDimsList() {
      return dims_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     * @return The count of dims.
     */
    @java.lang.Override
    public int getDimsCount() {
      return dims_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     * @param index The index of the element to return.
     * @return The dims at the given index.
     */
    @java.lang.Override
    public long getDims(int index) {
      return dims_.getLong(index);
    }
    private int dimsMemoizedSerializedSize = -1;
    private void ensureDimsIsMutable() {
      com.google.protobuf.Internal.LongList tmp = dims_;
      if (!tmp.isModifiable()) {
        dims_ =
            com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
       }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     * @param index The index to set the value at.
     * @param value The dims to set.
     */
    private void setDims(
        int index, long value) {
      ensureDimsIsMutable();
      dims_.setLong(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     * @param value The dims to add.
     */
    private void addDims(long value) {
      ensureDimsIsMutable();
      dims_.addLong(value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     * @param values The dims to add.
     */
    private void addAllDims(
        java.lang.Iterable<? extends java.lang.Long> values) {
      ensureDimsIsMutable();
      com.google.protobuf.AbstractMessageLite.addAll(
          values, dims_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     */
    private void clearDims() {
      dims_ = emptyLongList();
    }

    public static final int RESHAPE_FIELD_NUMBER = 5;
    private inference.ModelConfigOuterClass.ModelTensorReshape reshape_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape expected for this input by the backend. The input will
     *&#64;&#64;     be reshaped to this before being presented to the backend. The
     *&#64;&#64;     reshape must have the same number of elements as the input shape
     *&#64;&#64;     specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     */
    @java.lang.Override
    public boolean hasReshape() {
      return reshape_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape expected for this input by the backend. The input will
     *&#64;&#64;     be reshaped to this before being presented to the backend. The
     *&#64;&#64;     reshape must have the same number of elements as the input shape
     *&#64;&#64;     specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelTensorReshape getReshape() {
      return reshape_ == null ? inference.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance() : reshape_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape expected for this input by the backend. The input will
     *&#64;&#64;     be reshaped to this before being presented to the backend. The
     *&#64;&#64;     reshape must have the same number of elements as the input shape
     *&#64;&#64;     specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     */
    private void setReshape(inference.ModelConfigOuterClass.ModelTensorReshape value) {
      value.getClass();
  reshape_ = value;
      
      }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape expected for this input by the backend. The input will
     *&#64;&#64;     be reshaped to this before being presented to the backend. The
     *&#64;&#64;     reshape must have the same number of elements as the input shape
     *&#64;&#64;     specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     */
    @java.lang.SuppressWarnings({"ReferenceEquality"})
    private void mergeReshape(inference.ModelConfigOuterClass.ModelTensorReshape value) {
      value.getClass();
  if (reshape_ != null &&
          reshape_ != inference.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance()) {
        reshape_ =
          inference.ModelConfigOuterClass.ModelTensorReshape.newBuilder(reshape_).mergeFrom(value).buildPartial();
      } else {
        reshape_ = value;
      }
      
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape expected for this input by the backend. The input will
     *&#64;&#64;     be reshaped to this before being presented to the backend. The
     *&#64;&#64;     reshape must have the same number of elements as the input shape
     *&#64;&#64;     specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     */
    private void clearReshape() {  reshape_ = null;
      
    }

    public static final int IS_SHAPE_TENSOR_FIELD_NUMBER = 6;
    private boolean isShapeTensor_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool is_shape_tensor
     *&#64;&#64;
     *&#64;&#64;     Whether or not the input is a shape tensor to the model. This field
     *&#64;&#64;     is currently supported only for the TensorRT model. An error will be
     *&#64;&#64;     generated if this specification does not comply with underlying
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool is_shape_tensor = 6;</code>
     * @return The isShapeTensor.
     */
    @java.lang.Override
    public boolean getIsShapeTensor() {
      return isShapeTensor_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool is_shape_tensor
     *&#64;&#64;
     *&#64;&#64;     Whether or not the input is a shape tensor to the model. This field
     *&#64;&#64;     is currently supported only for the TensorRT model. An error will be
     *&#64;&#64;     generated if this specification does not comply with underlying
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool is_shape_tensor = 6;</code>
     * @param value The isShapeTensor to set.
     */
    private void setIsShapeTensor(boolean value) {
      
      isShapeTensor_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool is_shape_tensor
     *&#64;&#64;
     *&#64;&#64;     Whether or not the input is a shape tensor to the model. This field
     *&#64;&#64;     is currently supported only for the TensorRT model. An error will be
     *&#64;&#64;     generated if this specification does not comply with underlying
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool is_shape_tensor = 6;</code>
     */
    private void clearIsShapeTensor() {
      
      isShapeTensor_ = false;
    }

    public static final int ALLOW_RAGGED_BATCH_FIELD_NUMBER = 7;
    private boolean allowRaggedBatch_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool allow_ragged_batch
     *&#64;&#64;
     *&#64;&#64;     Whether or not the input is allowed to be "ragged" in a dynamically
     *&#64;&#64;     created batch. Default is false indicating that two requests will
     *&#64;&#64;     only be batched if this tensor has the same shape in both requests.
     *&#64;&#64;     True indicates that two requests can be batched even if this tensor
     *&#64;&#64;     has a different shape in each request. A true value is currently
     *&#64;&#64;     supported only for custom models.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool allow_ragged_batch = 7;</code>
     * @return The allowRaggedBatch.
     */
    @java.lang.Override
    public boolean getAllowRaggedBatch() {
      return allowRaggedBatch_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool allow_ragged_batch
     *&#64;&#64;
     *&#64;&#64;     Whether or not the input is allowed to be "ragged" in a dynamically
     *&#64;&#64;     created batch. Default is false indicating that two requests will
     *&#64;&#64;     only be batched if this tensor has the same shape in both requests.
     *&#64;&#64;     True indicates that two requests can be batched even if this tensor
     *&#64;&#64;     has a different shape in each request. A true value is currently
     *&#64;&#64;     supported only for custom models.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool allow_ragged_batch = 7;</code>
     * @param value The allowRaggedBatch to set.
     */
    private void setAllowRaggedBatch(boolean value) {
      
      allowRaggedBatch_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool allow_ragged_batch
     *&#64;&#64;
     *&#64;&#64;     Whether or not the input is allowed to be "ragged" in a dynamically
     *&#64;&#64;     created batch. Default is false indicating that two requests will
     *&#64;&#64;     only be batched if this tensor has the same shape in both requests.
     *&#64;&#64;     True indicates that two requests can be batched even if this tensor
     *&#64;&#64;     has a different shape in each request. A true value is currently
     *&#64;&#64;     supported only for custom models.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool allow_ragged_batch = 7;</code>
     */
    private void clearAllowRaggedBatch() {
      
      allowRaggedBatch_ = false;
    }

    public static inference.ModelConfigOuterClass.ModelInput parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelInput parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelInput parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelInput parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelInput parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelInput parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelInput parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelInput parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelInput parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelInput parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelInput parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelInput parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }

    public static Builder newBuilder() {
      return (Builder) DEFAULT_INSTANCE.createBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelInput prototype) {
      return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelInput
     *&#64;&#64;
     *&#64;&#64;   An input required by the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelInput}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageLite.Builder<
          inference.ModelConfigOuterClass.ModelInput, Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelInput)
        inference.ModelConfigOuterClass.ModelInputOrBuilder {
      // Construct using inference.ModelConfigOuterClass.ModelInput.newBuilder()
      private Builder() {
        super(DEFAULT_INSTANCE);
      }


      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The name.
       */
      @java.lang.Override
      public java.lang.String getName() {
        return instance.getName();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString
          getNameBytes() {
        return instance.getNameBytes();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        copyOnWrite();
        instance.setName(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        copyOnWrite();
        instance.clearName();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        copyOnWrite();
        instance.setNameBytes(value);
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 2;</code>
       * @return The enum numeric value on the wire for dataType.
       */
      @java.lang.Override
      public int getDataTypeValue() {
        return instance.getDataTypeValue();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 2;</code>
       * @param value The dataType to set.
       * @return This builder for chaining.
       */
      public Builder setDataTypeValue(int value) {
        copyOnWrite();
        instance.setDataTypeValue(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 2;</code>
       * @return The dataType.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.DataType getDataType() {
        return instance.getDataType();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 2;</code>
       * @param value The enum numeric value on the wire for dataType to set.
       * @return This builder for chaining.
       */
      public Builder setDataType(inference.ModelConfigOuterClass.DataType value) {
        copyOnWrite();
        instance.setDataType(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearDataType() {
        copyOnWrite();
        instance.clearDataType();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Format format
       *&#64;&#64;
       *&#64;&#64;     The format of the input. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelInput.Format format = 3;</code>
       * @return The enum numeric value on the wire for format.
       */
      @java.lang.Override
      public int getFormatValue() {
        return instance.getFormatValue();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Format format
       *&#64;&#64;
       *&#64;&#64;     The format of the input. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelInput.Format format = 3;</code>
       * @param value The format to set.
       * @return This builder for chaining.
       */
      public Builder setFormatValue(int value) {
        copyOnWrite();
        instance.setFormatValue(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Format format
       *&#64;&#64;
       *&#64;&#64;     The format of the input. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelInput.Format format = 3;</code>
       * @return The format.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelInput.Format getFormat() {
        return instance.getFormat();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Format format
       *&#64;&#64;
       *&#64;&#64;     The format of the input. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelInput.Format format = 3;</code>
       * @param value The enum numeric value on the wire for format to set.
       * @return This builder for chaining.
       */
      public Builder setFormat(inference.ModelConfigOuterClass.ModelInput.Format value) {
        copyOnWrite();
        instance.setFormat(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Format format
       *&#64;&#64;
       *&#64;&#64;     The format of the input. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelInput.Format format = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearFormat() {
        copyOnWrite();
        instance.clearFormat();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       * @return A list containing the dims.
       */
      @java.lang.Override
      public java.util.List<java.lang.Long>
          getDimsList() {
        return java.util.Collections.unmodifiableList(
            instance.getDimsList());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       * @return The count of dims.
       */
      @java.lang.Override
      public int getDimsCount() {
        return instance.getDimsCount();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       * @param index The index of the element to return.
       * @return The dims at the given index.
       */
      @java.lang.Override
      public long getDims(int index) {
        return instance.getDims(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       * @param value The dims to set.
       * @return This builder for chaining.
       */
      public Builder setDims(
          int index, long value) {
        copyOnWrite();
        instance.setDims(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       * @param value The dims to add.
       * @return This builder for chaining.
       */
      public Builder addDims(long value) {
        copyOnWrite();
        instance.addDims(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       * @param values The dims to add.
       * @return This builder for chaining.
       */
      public Builder addAllDims(
          java.lang.Iterable<? extends java.lang.Long> values) {
        copyOnWrite();
        instance.addAllDims(values);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearDims() {
        copyOnWrite();
        instance.clearDims();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      @java.lang.Override
      public boolean hasReshape() {
        return instance.hasReshape();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelTensorReshape getReshape() {
        return instance.getReshape();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      public Builder setReshape(inference.ModelConfigOuterClass.ModelTensorReshape value) {
        copyOnWrite();
        instance.setReshape(value);
        return this;
        }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      public Builder setReshape(
          inference.ModelConfigOuterClass.ModelTensorReshape.Builder builderForValue) {
        copyOnWrite();
        instance.setReshape(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      public Builder mergeReshape(inference.ModelConfigOuterClass.ModelTensorReshape value) {
        copyOnWrite();
        instance.mergeReshape(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      public Builder clearReshape() {  copyOnWrite();
        instance.clearReshape();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool is_shape_tensor
       *&#64;&#64;
       *&#64;&#64;     Whether or not the input is a shape tensor to the model. This field
       *&#64;&#64;     is currently supported only for the TensorRT model. An error will be
       *&#64;&#64;     generated if this specification does not comply with underlying
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool is_shape_tensor = 6;</code>
       * @return The isShapeTensor.
       */
      @java.lang.Override
      public boolean getIsShapeTensor() {
        return instance.getIsShapeTensor();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool is_shape_tensor
       *&#64;&#64;
       *&#64;&#64;     Whether or not the input is a shape tensor to the model. This field
       *&#64;&#64;     is currently supported only for the TensorRT model. An error will be
       *&#64;&#64;     generated if this specification does not comply with underlying
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool is_shape_tensor = 6;</code>
       * @param value The isShapeTensor to set.
       * @return This builder for chaining.
       */
      public Builder setIsShapeTensor(boolean value) {
        copyOnWrite();
        instance.setIsShapeTensor(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool is_shape_tensor
       *&#64;&#64;
       *&#64;&#64;     Whether or not the input is a shape tensor to the model. This field
       *&#64;&#64;     is currently supported only for the TensorRT model. An error will be
       *&#64;&#64;     generated if this specification does not comply with underlying
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool is_shape_tensor = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearIsShapeTensor() {
        copyOnWrite();
        instance.clearIsShapeTensor();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool allow_ragged_batch
       *&#64;&#64;
       *&#64;&#64;     Whether or not the input is allowed to be "ragged" in a dynamically
       *&#64;&#64;     created batch. Default is false indicating that two requests will
       *&#64;&#64;     only be batched if this tensor has the same shape in both requests.
       *&#64;&#64;     True indicates that two requests can be batched even if this tensor
       *&#64;&#64;     has a different shape in each request. A true value is currently
       *&#64;&#64;     supported only for custom models.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool allow_ragged_batch = 7;</code>
       * @return The allowRaggedBatch.
       */
      @java.lang.Override
      public boolean getAllowRaggedBatch() {
        return instance.getAllowRaggedBatch();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool allow_ragged_batch
       *&#64;&#64;
       *&#64;&#64;     Whether or not the input is allowed to be "ragged" in a dynamically
       *&#64;&#64;     created batch. Default is false indicating that two requests will
       *&#64;&#64;     only be batched if this tensor has the same shape in both requests.
       *&#64;&#64;     True indicates that two requests can be batched even if this tensor
       *&#64;&#64;     has a different shape in each request. A true value is currently
       *&#64;&#64;     supported only for custom models.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool allow_ragged_batch = 7;</code>
       * @param value The allowRaggedBatch to set.
       * @return This builder for chaining.
       */
      public Builder setAllowRaggedBatch(boolean value) {
        copyOnWrite();
        instance.setAllowRaggedBatch(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool allow_ragged_batch
       *&#64;&#64;
       *&#64;&#64;     Whether or not the input is allowed to be "ragged" in a dynamically
       *&#64;&#64;     created batch. Default is false indicating that two requests will
       *&#64;&#64;     only be batched if this tensor has the same shape in both requests.
       *&#64;&#64;     True indicates that two requests can be batched even if this tensor
       *&#64;&#64;     has a different shape in each request. A true value is currently
       *&#64;&#64;     supported only for custom models.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool allow_ragged_batch = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearAllowRaggedBatch() {
        copyOnWrite();
        instance.clearAllowRaggedBatch();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:inference.ModelInput)
    }
    @java.lang.Override
    @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
    protected final java.lang.Object dynamicMethod(
        com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
        java.lang.Object arg0, java.lang.Object arg1) {
      switch (method) {
        case NEW_MUTABLE_INSTANCE: {
          return new inference.ModelConfigOuterClass.ModelInput();
        }
        case NEW_BUILDER: {
          return new Builder();
        }
        case BUILD_MESSAGE_INFO: {
            java.lang.Object[] objects = new java.lang.Object[] {
              "name_",
              "dataType_",
              "format_",
              "dims_",
              "reshape_",
              "isShapeTensor_",
              "allowRaggedBatch_",
            };
            java.lang.String info =
                "\u0000\u0007\u0000\u0000\u0001\u0007\u0007\u0000\u0001\u0000\u0001\u0208\u0002\f" +
                "\u0003\f\u0004%\u0005\t\u0006\u0007\u0007\u0007";
            return newMessageInfo(DEFAULT_INSTANCE, info, objects);
        }
        // fall through
        case GET_DEFAULT_INSTANCE: {
          return DEFAULT_INSTANCE;
        }
        case GET_PARSER: {
          com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelInput> parser = PARSER;
          if (parser == null) {
            synchronized (inference.ModelConfigOuterClass.ModelInput.class) {
              parser = PARSER;
              if (parser == null) {
                parser =
                    new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelInput>(
                        DEFAULT_INSTANCE);
                PARSER = parser;
              }
            }
          }
          return parser;
      }
      case GET_MEMOIZED_IS_INITIALIZED: {
        return (byte) 1;
      }
      case SET_MEMOIZED_IS_INITIALIZED: {
        return null;
      }
      }
      throw new UnsupportedOperationException();
    }


    // @@protoc_insertion_point(class_scope:inference.ModelInput)
    private static final inference.ModelConfigOuterClass.ModelInput DEFAULT_INSTANCE;
    static {
      ModelInput defaultInstance = new ModelInput();
      // New instances are implicitly immutable so no need to make
      // immutable.
      DEFAULT_INSTANCE = defaultInstance;
      com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
        ModelInput.class, defaultInstance);
    }

    public static inference.ModelConfigOuterClass.ModelInput getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static volatile com.google.protobuf.Parser<ModelInput> PARSER;

    public static com.google.protobuf.Parser<ModelInput> parser() {
      return DEFAULT_INSTANCE.getParserForType();
    }
  }

  public interface ModelOutputOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelOutput)
      com.google.protobuf.MessageLiteOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 2;</code>
     * @return The enum numeric value on the wire for dataType.
     */
    int getDataTypeValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 2;</code>
     * @return The dataType.
     */
    inference.ModelConfigOuterClass.DataType getDataType();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     * @return A list containing the dims.
     */
    java.util.List<java.lang.Long> getDimsList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     * @return The count of dims.
     */
    int getDimsCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     * @param index The index of the element to return.
     * @return The dims at the given index.
     */
    long getDims(int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape produced for this output by the backend. The output will
     *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
     *&#64;&#64;     returned in the inference response. The reshape must have the same
     *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     * @return Whether the reshape field is set.
     */
    boolean hasReshape();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape produced for this output by the backend. The output will
     *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
     *&#64;&#64;     returned in the inference response. The reshape must have the same
     *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     * @return The reshape.
     */
    inference.ModelConfigOuterClass.ModelTensorReshape getReshape();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string label_filename
     *&#64;&#64;
     *&#64;&#64;     The label file associated with this output. Should be specified only
     *&#64;&#64;     for outputs that represent classifications. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>string label_filename = 4;</code>
     * @return The labelFilename.
     */
    java.lang.String getLabelFilename();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string label_filename
     *&#64;&#64;
     *&#64;&#64;     The label file associated with this output. Should be specified only
     *&#64;&#64;     for outputs that represent classifications. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>string label_filename = 4;</code>
     * @return The bytes for labelFilename.
     */
    com.google.protobuf.ByteString
        getLabelFilenameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool is_shape_tensor
     *&#64;&#64;
     *&#64;&#64;     Whether or not the output is a shape tensor to the model. This field
     *&#64;&#64;     is currently supported only for the TensorRT model. An error will be
     *&#64;&#64;     generated if this specification does not comply with underlying
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool is_shape_tensor = 6;</code>
     * @return The isShapeTensor.
     */
    boolean getIsShapeTensor();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelOutput
   *&#64;&#64;
   *&#64;&#64;   An output produced by the model.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelOutput}
   */
  public  static final class ModelOutput extends
      com.google.protobuf.GeneratedMessageLite<
          ModelOutput, ModelOutput.Builder> implements
      // @@protoc_insertion_point(message_implements:inference.ModelOutput)
      ModelOutputOrBuilder {
    private ModelOutput() {
      name_ = "";
      dims_ = emptyLongList();
      labelFilename_ = "";
    }
    public static final int NAME_FIELD_NUMBER = 1;
    private java.lang.String name_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      return name_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      return com.google.protobuf.ByteString.copyFromUtf8(name_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @param value The name to set.
     */
    private void setName(
        java.lang.String value) {
      java.lang.Class<?> valueClass = value.getClass();
  
      name_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    private void clearName() {
      
      name_ = getDefaultInstance().getName();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @param value The bytes for name to set.
     */
    private void setNameBytes(
        com.google.protobuf.ByteString value) {
      checkByteStringIsUtf8(value);
      name_ = value.toStringUtf8();
      
    }

    public static final int DATA_TYPE_FIELD_NUMBER = 2;
    private int dataType_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 2;</code>
     * @return The enum numeric value on the wire for dataType.
     */
    @java.lang.Override
    public int getDataTypeValue() {
      return dataType_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 2;</code>
     * @return The dataType.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.DataType getDataType() {
      inference.ModelConfigOuterClass.DataType result = inference.ModelConfigOuterClass.DataType.forNumber(dataType_);
      return result == null ? inference.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 2;</code>
     * @param value The enum numeric value on the wire for dataType to set.
     */
    private void setDataTypeValue(int value) {
        dataType_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 2;</code>
     * @param value The dataType to set.
     */
    private void setDataType(inference.ModelConfigOuterClass.DataType value) {
      dataType_ = value.getNumber();
      
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 2;</code>
     */
    private void clearDataType() {
      
      dataType_ = 0;
    }

    public static final int DIMS_FIELD_NUMBER = 3;
    private com.google.protobuf.Internal.LongList dims_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     * @return A list containing the dims.
     */
    @java.lang.Override
    public java.util.List<java.lang.Long>
        getDimsList() {
      return dims_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     * @return The count of dims.
     */
    @java.lang.Override
    public int getDimsCount() {
      return dims_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     * @param index The index of the element to return.
     * @return The dims at the given index.
     */
    @java.lang.Override
    public long getDims(int index) {
      return dims_.getLong(index);
    }
    private int dimsMemoizedSerializedSize = -1;
    private void ensureDimsIsMutable() {
      com.google.protobuf.Internal.LongList tmp = dims_;
      if (!tmp.isModifiable()) {
        dims_ =
            com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
       }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     * @param index The index to set the value at.
     * @param value The dims to set.
     */
    private void setDims(
        int index, long value) {
      ensureDimsIsMutable();
      dims_.setLong(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     * @param value The dims to add.
     */
    private void addDims(long value) {
      ensureDimsIsMutable();
      dims_.addLong(value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     * @param values The dims to add.
     */
    private void addAllDims(
        java.lang.Iterable<? extends java.lang.Long> values) {
      ensureDimsIsMutable();
      com.google.protobuf.AbstractMessageLite.addAll(
          values, dims_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     */
    private void clearDims() {
      dims_ = emptyLongList();
    }

    public static final int RESHAPE_FIELD_NUMBER = 5;
    private inference.ModelConfigOuterClass.ModelTensorReshape reshape_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape produced for this output by the backend. The output will
     *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
     *&#64;&#64;     returned in the inference response. The reshape must have the same
     *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     */
    @java.lang.Override
    public boolean hasReshape() {
      return reshape_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape produced for this output by the backend. The output will
     *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
     *&#64;&#64;     returned in the inference response. The reshape must have the same
     *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelTensorReshape getReshape() {
      return reshape_ == null ? inference.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance() : reshape_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape produced for this output by the backend. The output will
     *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
     *&#64;&#64;     returned in the inference response. The reshape must have the same
     *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     */
    private void setReshape(inference.ModelConfigOuterClass.ModelTensorReshape value) {
      value.getClass();
  reshape_ = value;
      
      }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape produced for this output by the backend. The output will
     *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
     *&#64;&#64;     returned in the inference response. The reshape must have the same
     *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     */
    @java.lang.SuppressWarnings({"ReferenceEquality"})
    private void mergeReshape(inference.ModelConfigOuterClass.ModelTensorReshape value) {
      value.getClass();
  if (reshape_ != null &&
          reshape_ != inference.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance()) {
        reshape_ =
          inference.ModelConfigOuterClass.ModelTensorReshape.newBuilder(reshape_).mergeFrom(value).buildPartial();
      } else {
        reshape_ = value;
      }
      
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape produced for this output by the backend. The output will
     *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
     *&#64;&#64;     returned in the inference response. The reshape must have the same
     *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     */
    private void clearReshape() {  reshape_ = null;
      
    }

    public static final int LABEL_FILENAME_FIELD_NUMBER = 4;
    private java.lang.String labelFilename_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string label_filename
     *&#64;&#64;
     *&#64;&#64;     The label file associated with this output. Should be specified only
     *&#64;&#64;     for outputs that represent classifications. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>string label_filename = 4;</code>
     * @return The labelFilename.
     */
    @java.lang.Override
    public java.lang.String getLabelFilename() {
      return labelFilename_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string label_filename
     *&#64;&#64;
     *&#64;&#64;     The label file associated with this output. Should be specified only
     *&#64;&#64;     for outputs that represent classifications. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>string label_filename = 4;</code>
     * @return The bytes for labelFilename.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getLabelFilenameBytes() {
      return com.google.protobuf.ByteString.copyFromUtf8(labelFilename_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string label_filename
     *&#64;&#64;
     *&#64;&#64;     The label file associated with this output. Should be specified only
     *&#64;&#64;     for outputs that represent classifications. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>string label_filename = 4;</code>
     * @param value The labelFilename to set.
     */
    private void setLabelFilename(
        java.lang.String value) {
      java.lang.Class<?> valueClass = value.getClass();
  
      labelFilename_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string label_filename
     *&#64;&#64;
     *&#64;&#64;     The label file associated with this output. Should be specified only
     *&#64;&#64;     for outputs that represent classifications. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>string label_filename = 4;</code>
     */
    private void clearLabelFilename() {
      
      labelFilename_ = getDefaultInstance().getLabelFilename();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string label_filename
     *&#64;&#64;
     *&#64;&#64;     The label file associated with this output. Should be specified only
     *&#64;&#64;     for outputs that represent classifications. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>string label_filename = 4;</code>
     * @param value The bytes for labelFilename to set.
     */
    private void setLabelFilenameBytes(
        com.google.protobuf.ByteString value) {
      checkByteStringIsUtf8(value);
      labelFilename_ = value.toStringUtf8();
      
    }

    public static final int IS_SHAPE_TENSOR_FIELD_NUMBER = 6;
    private boolean isShapeTensor_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool is_shape_tensor
     *&#64;&#64;
     *&#64;&#64;     Whether or not the output is a shape tensor to the model. This field
     *&#64;&#64;     is currently supported only for the TensorRT model. An error will be
     *&#64;&#64;     generated if this specification does not comply with underlying
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool is_shape_tensor = 6;</code>
     * @return The isShapeTensor.
     */
    @java.lang.Override
    public boolean getIsShapeTensor() {
      return isShapeTensor_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool is_shape_tensor
     *&#64;&#64;
     *&#64;&#64;     Whether or not the output is a shape tensor to the model. This field
     *&#64;&#64;     is currently supported only for the TensorRT model. An error will be
     *&#64;&#64;     generated if this specification does not comply with underlying
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool is_shape_tensor = 6;</code>
     * @param value The isShapeTensor to set.
     */
    private void setIsShapeTensor(boolean value) {
      
      isShapeTensor_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool is_shape_tensor
     *&#64;&#64;
     *&#64;&#64;     Whether or not the output is a shape tensor to the model. This field
     *&#64;&#64;     is currently supported only for the TensorRT model. An error will be
     *&#64;&#64;     generated if this specification does not comply with underlying
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool is_shape_tensor = 6;</code>
     */
    private void clearIsShapeTensor() {
      
      isShapeTensor_ = false;
    }

    public static inference.ModelConfigOuterClass.ModelOutput parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelOutput parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOutput parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelOutput parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOutput parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelOutput parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOutput parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelOutput parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOutput parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelOutput parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOutput parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelOutput parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }

    public static Builder newBuilder() {
      return (Builder) DEFAULT_INSTANCE.createBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelOutput prototype) {
      return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelOutput
     *&#64;&#64;
     *&#64;&#64;   An output produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelOutput}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageLite.Builder<
          inference.ModelConfigOuterClass.ModelOutput, Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelOutput)
        inference.ModelConfigOuterClass.ModelOutputOrBuilder {
      // Construct using inference.ModelConfigOuterClass.ModelOutput.newBuilder()
      private Builder() {
        super(DEFAULT_INSTANCE);
      }


      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The name.
       */
      @java.lang.Override
      public java.lang.String getName() {
        return instance.getName();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString
          getNameBytes() {
        return instance.getNameBytes();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        copyOnWrite();
        instance.setName(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        copyOnWrite();
        instance.clearName();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        copyOnWrite();
        instance.setNameBytes(value);
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 2;</code>
       * @return The enum numeric value on the wire for dataType.
       */
      @java.lang.Override
      public int getDataTypeValue() {
        return instance.getDataTypeValue();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 2;</code>
       * @param value The dataType to set.
       * @return This builder for chaining.
       */
      public Builder setDataTypeValue(int value) {
        copyOnWrite();
        instance.setDataTypeValue(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 2;</code>
       * @return The dataType.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.DataType getDataType() {
        return instance.getDataType();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 2;</code>
       * @param value The enum numeric value on the wire for dataType to set.
       * @return This builder for chaining.
       */
      public Builder setDataType(inference.ModelConfigOuterClass.DataType value) {
        copyOnWrite();
        instance.setDataType(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearDataType() {
        copyOnWrite();
        instance.clearDataType();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       * @return A list containing the dims.
       */
      @java.lang.Override
      public java.util.List<java.lang.Long>
          getDimsList() {
        return java.util.Collections.unmodifiableList(
            instance.getDimsList());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       * @return The count of dims.
       */
      @java.lang.Override
      public int getDimsCount() {
        return instance.getDimsCount();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       * @param index The index of the element to return.
       * @return The dims at the given index.
       */
      @java.lang.Override
      public long getDims(int index) {
        return instance.getDims(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       * @param value The dims to set.
       * @return This builder for chaining.
       */
      public Builder setDims(
          int index, long value) {
        copyOnWrite();
        instance.setDims(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       * @param value The dims to add.
       * @return This builder for chaining.
       */
      public Builder addDims(long value) {
        copyOnWrite();
        instance.addDims(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       * @param values The dims to add.
       * @return This builder for chaining.
       */
      public Builder addAllDims(
          java.lang.Iterable<? extends java.lang.Long> values) {
        copyOnWrite();
        instance.addAllDims(values);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearDims() {
        copyOnWrite();
        instance.clearDims();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      @java.lang.Override
      public boolean hasReshape() {
        return instance.hasReshape();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelTensorReshape getReshape() {
        return instance.getReshape();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      public Builder setReshape(inference.ModelConfigOuterClass.ModelTensorReshape value) {
        copyOnWrite();
        instance.setReshape(value);
        return this;
        }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      public Builder setReshape(
          inference.ModelConfigOuterClass.ModelTensorReshape.Builder builderForValue) {
        copyOnWrite();
        instance.setReshape(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      public Builder mergeReshape(inference.ModelConfigOuterClass.ModelTensorReshape value) {
        copyOnWrite();
        instance.mergeReshape(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      public Builder clearReshape() {  copyOnWrite();
        instance.clearReshape();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string label_filename
       *&#64;&#64;
       *&#64;&#64;     The label file associated with this output. Should be specified only
       *&#64;&#64;     for outputs that represent classifications. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>string label_filename = 4;</code>
       * @return The labelFilename.
       */
      @java.lang.Override
      public java.lang.String getLabelFilename() {
        return instance.getLabelFilename();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string label_filename
       *&#64;&#64;
       *&#64;&#64;     The label file associated with this output. Should be specified only
       *&#64;&#64;     for outputs that represent classifications. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>string label_filename = 4;</code>
       * @return The bytes for labelFilename.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString
          getLabelFilenameBytes() {
        return instance.getLabelFilenameBytes();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string label_filename
       *&#64;&#64;
       *&#64;&#64;     The label file associated with this output. Should be specified only
       *&#64;&#64;     for outputs that represent classifications. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>string label_filename = 4;</code>
       * @param value The labelFilename to set.
       * @return This builder for chaining.
       */
      public Builder setLabelFilename(
          java.lang.String value) {
        copyOnWrite();
        instance.setLabelFilename(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string label_filename
       *&#64;&#64;
       *&#64;&#64;     The label file associated with this output. Should be specified only
       *&#64;&#64;     for outputs that represent classifications. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>string label_filename = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearLabelFilename() {
        copyOnWrite();
        instance.clearLabelFilename();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string label_filename
       *&#64;&#64;
       *&#64;&#64;     The label file associated with this output. Should be specified only
       *&#64;&#64;     for outputs that represent classifications. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>string label_filename = 4;</code>
       * @param value The bytes for labelFilename to set.
       * @return This builder for chaining.
       */
      public Builder setLabelFilenameBytes(
          com.google.protobuf.ByteString value) {
        copyOnWrite();
        instance.setLabelFilenameBytes(value);
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool is_shape_tensor
       *&#64;&#64;
       *&#64;&#64;     Whether or not the output is a shape tensor to the model. This field
       *&#64;&#64;     is currently supported only for the TensorRT model. An error will be
       *&#64;&#64;     generated if this specification does not comply with underlying
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool is_shape_tensor = 6;</code>
       * @return The isShapeTensor.
       */
      @java.lang.Override
      public boolean getIsShapeTensor() {
        return instance.getIsShapeTensor();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool is_shape_tensor
       *&#64;&#64;
       *&#64;&#64;     Whether or not the output is a shape tensor to the model. This field
       *&#64;&#64;     is currently supported only for the TensorRT model. An error will be
       *&#64;&#64;     generated if this specification does not comply with underlying
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool is_shape_tensor = 6;</code>
       * @param value The isShapeTensor to set.
       * @return This builder for chaining.
       */
      public Builder setIsShapeTensor(boolean value) {
        copyOnWrite();
        instance.setIsShapeTensor(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool is_shape_tensor
       *&#64;&#64;
       *&#64;&#64;     Whether or not the output is a shape tensor to the model. This field
       *&#64;&#64;     is currently supported only for the TensorRT model. An error will be
       *&#64;&#64;     generated if this specification does not comply with underlying
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool is_shape_tensor = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearIsShapeTensor() {
        copyOnWrite();
        instance.clearIsShapeTensor();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:inference.ModelOutput)
    }
    @java.lang.Override
    @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
    protected final java.lang.Object dynamicMethod(
        com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
        java.lang.Object arg0, java.lang.Object arg1) {
      switch (method) {
        case NEW_MUTABLE_INSTANCE: {
          return new inference.ModelConfigOuterClass.ModelOutput();
        }
        case NEW_BUILDER: {
          return new Builder();
        }
        case BUILD_MESSAGE_INFO: {
            java.lang.Object[] objects = new java.lang.Object[] {
              "name_",
              "dataType_",
              "dims_",
              "labelFilename_",
              "reshape_",
              "isShapeTensor_",
            };
            java.lang.String info =
                "\u0000\u0006\u0000\u0000\u0001\u0006\u0006\u0000\u0001\u0000\u0001\u0208\u0002\f" +
                "\u0003%\u0004\u0208\u0005\t\u0006\u0007";
            return newMessageInfo(DEFAULT_INSTANCE, info, objects);
        }
        // fall through
        case GET_DEFAULT_INSTANCE: {
          return DEFAULT_INSTANCE;
        }
        case GET_PARSER: {
          com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelOutput> parser = PARSER;
          if (parser == null) {
            synchronized (inference.ModelConfigOuterClass.ModelOutput.class) {
              parser = PARSER;
              if (parser == null) {
                parser =
                    new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelOutput>(
                        DEFAULT_INSTANCE);
                PARSER = parser;
              }
            }
          }
          return parser;
      }
      case GET_MEMOIZED_IS_INITIALIZED: {
        return (byte) 1;
      }
      case SET_MEMOIZED_IS_INITIALIZED: {
        return null;
      }
      }
      throw new UnsupportedOperationException();
    }


    // @@protoc_insertion_point(class_scope:inference.ModelOutput)
    private static final inference.ModelConfigOuterClass.ModelOutput DEFAULT_INSTANCE;
    static {
      ModelOutput defaultInstance = new ModelOutput();
      // New instances are implicitly immutable so no need to make
      // immutable.
      DEFAULT_INSTANCE = defaultInstance;
      com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
        ModelOutput.class, defaultInstance);
    }

    public static inference.ModelConfigOuterClass.ModelOutput getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static volatile com.google.protobuf.Parser<ModelOutput> PARSER;

    public static com.google.protobuf.Parser<ModelOutput> parser() {
      return DEFAULT_INSTANCE.getParserForType();
    }
  }

  public interface BatchInputOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.BatchInput)
      com.google.protobuf.MessageLiteOrBuilder {

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;       The kind of this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.BatchInput.Kind kind = 1;</code>
     * @return The enum numeric value on the wire for kind.
     */
    int getKindValue();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;       The kind of this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.BatchInput.Kind kind = 1;</code>
     * @return The kind.
     */
    inference.ModelConfigOuterClass.BatchInput.Kind getKind();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;       The name of the model inputs that the backend will create
     *&#64;&#64;       for this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 2;</code>
     * @return A list containing the targetName.
     */
    java.util.List<java.lang.String>
        getTargetNameList();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;       The name of the model inputs that the backend will create
     *&#64;&#64;       for this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 2;</code>
     * @return The count of targetName.
     */
    int getTargetNameCount();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;       The name of the model inputs that the backend will create
     *&#64;&#64;       for this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 2;</code>
     * @param index The index of the element to return.
     * @return The targetName at the given index.
     */
    java.lang.String getTargetName(int index);
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;       The name of the model inputs that the backend will create
     *&#64;&#64;       for this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 2;</code>
     * @param index The index of the element to return.
     * @return The targetName at the given index.
     */
    com.google.protobuf.ByteString
        getTargetNameBytes(int index);

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;       The input's datatype. The data type can be TYPE_INT32 or
     *&#64;&#64;       TYPE_FP32.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 3;</code>
     * @return The enum numeric value on the wire for dataType.
     */
    int getDataTypeValue();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;       The input's datatype. The data type can be TYPE_INT32 or
     *&#64;&#64;       TYPE_FP32.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 3;</code>
     * @return The dataType.
     */
    inference.ModelConfigOuterClass.DataType getDataType();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;       The backend derives the value for each batch input from one or
     *&#64;&#64;       more other inputs. 'source_input' gives the names of those
     *&#64;&#64;       inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 4;</code>
     * @return A list containing the sourceInput.
     */
    java.util.List<java.lang.String>
        getSourceInputList();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;       The backend derives the value for each batch input from one or
     *&#64;&#64;       more other inputs. 'source_input' gives the names of those
     *&#64;&#64;       inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 4;</code>
     * @return The count of sourceInput.
     */
    int getSourceInputCount();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;       The backend derives the value for each batch input from one or
     *&#64;&#64;       more other inputs. 'source_input' gives the names of those
     *&#64;&#64;       inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 4;</code>
     * @param index The index of the element to return.
     * @return The sourceInput at the given index.
     */
    java.lang.String getSourceInput(int index);
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;       The backend derives the value for each batch input from one or
     *&#64;&#64;       more other inputs. 'source_input' gives the names of those
     *&#64;&#64;       inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 4;</code>
     * @param index The index of the element to return.
     * @return The sourceInput at the given index.
     */
    com.google.protobuf.ByteString
        getSourceInputBytes(int index);
  }
  /**
   * <pre>
   *&#64;&#64;  .. cpp:var:: message BatchInput
   *&#64;&#64;
   *&#64;&#64;     A batch input is an additional input that must be added by
   *&#64;&#64;     the backend based on all the requests in a batch.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.BatchInput}
   */
  public  static final class BatchInput extends
      com.google.protobuf.GeneratedMessageLite<
          BatchInput, BatchInput.Builder> implements
      // @@protoc_insertion_point(message_implements:inference.BatchInput)
      BatchInputOrBuilder {
    private BatchInput() {
      targetName_ = com.google.protobuf.GeneratedMessageLite.emptyProtobufList();
      sourceInput_ = com.google.protobuf.GeneratedMessageLite.emptyProtobufList();
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;    .. cpp:enum:: Kind
     *&#64;&#64;
     *&#64;&#64;       The kind of the batch input.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf enum {@code inference.BatchInput.Kind}
     */
    public enum Kind
        implements com.google.protobuf.Internal.EnumLite {
      /**
       * <pre>
       *&#64;&#64;      .. cpp:enumerator:: Kind::BATCH_ELEMENT_COUNT = 0
       *&#64;&#64;
       *&#64;&#64;         The element count of the 'source_input' will be added as
       *&#64;&#64;         input with shape [1].
       *&#64;&#64;
       * </pre>
       *
       * <code>BATCH_ELEMENT_COUNT = 0;</code>
       */
      BATCH_ELEMENT_COUNT(0),
      /**
       * <pre>
       *&#64;&#64;      .. cpp:enumerator:: Kind::BATCH_ACCUMULATED_ELEMENT_COUNT = 1
       *&#64;&#64;
       *&#64;&#64;         The accumulated element count of the 'source_input' will be
       *&#64;&#64;         added as input with shape [1]. For example, if there is a
       *&#64;&#64;         batch of two request, each with 2 elements, an input of value
       *&#64;&#64;         2 will be added to the first request, and an input of value
       *&#64;&#64;         4 will be added to the second request.
       *&#64;&#64;
       * </pre>
       *
       * <code>BATCH_ACCUMULATED_ELEMENT_COUNT = 1;</code>
       */
      BATCH_ACCUMULATED_ELEMENT_COUNT(1),
      /**
       * <pre>
       *&#64;&#64;      .. cpp:enumerator::
       *&#64;&#64;         Kind::BATCH_ACCUMULATED_ELEMENT_COUNT_WITH_ZERO = 2
       *&#64;&#64;
       *&#64;&#64;         The accumulated element count of the 'source_input' will be
       *&#64;&#64;         added as input with shape [1], except for the first request
       *&#64;&#64;         in the batch. For the first request in the batch, the input
       *&#64;&#64;         will have shape [2] where the first element is value 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>BATCH_ACCUMULATED_ELEMENT_COUNT_WITH_ZERO = 2;</code>
       */
      BATCH_ACCUMULATED_ELEMENT_COUNT_WITH_ZERO(2),
      /**
       * <pre>
       *&#64;&#64;      .. cpp:enumerator:: Kind::BATCH_MAX_ELEMENT_COUNT_AS_SHAPE = 3
       *&#64;&#64;
       *&#64;&#64;         Among the requests in the batch, the max element count of the
       *&#64;&#64;         'source_input' will be added as input with shape
       *&#64;&#64;         [max_element_count] for the first request in the batch.
       *&#64;&#64;         For other requests, such input will be with shape [0].
       *&#64;&#64;         The data of the tensor will be uninitialized.
       *&#64;&#64;
       * </pre>
       *
       * <code>BATCH_MAX_ELEMENT_COUNT_AS_SHAPE = 3;</code>
       */
      BATCH_MAX_ELEMENT_COUNT_AS_SHAPE(3),
      UNRECOGNIZED(-1),
      ;

      /**
       * <pre>
       *&#64;&#64;      .. cpp:enumerator:: Kind::BATCH_ELEMENT_COUNT = 0
       *&#64;&#64;
       *&#64;&#64;         The element count of the 'source_input' will be added as
       *&#64;&#64;         input with shape [1].
       *&#64;&#64;
       * </pre>
       *
       * <code>BATCH_ELEMENT_COUNT = 0;</code>
       */
      public static final int BATCH_ELEMENT_COUNT_VALUE = 0;
      /**
       * <pre>
       *&#64;&#64;      .. cpp:enumerator:: Kind::BATCH_ACCUMULATED_ELEMENT_COUNT = 1
       *&#64;&#64;
       *&#64;&#64;         The accumulated element count of the 'source_input' will be
       *&#64;&#64;         added as input with shape [1]. For example, if there is a
       *&#64;&#64;         batch of two request, each with 2 elements, an input of value
       *&#64;&#64;         2 will be added to the first request, and an input of value
       *&#64;&#64;         4 will be added to the second request.
       *&#64;&#64;
       * </pre>
       *
       * <code>BATCH_ACCUMULATED_ELEMENT_COUNT = 1;</code>
       */
      public static final int BATCH_ACCUMULATED_ELEMENT_COUNT_VALUE = 1;
      /**
       * <pre>
       *&#64;&#64;      .. cpp:enumerator::
       *&#64;&#64;         Kind::BATCH_ACCUMULATED_ELEMENT_COUNT_WITH_ZERO = 2
       *&#64;&#64;
       *&#64;&#64;         The accumulated element count of the 'source_input' will be
       *&#64;&#64;         added as input with shape [1], except for the first request
       *&#64;&#64;         in the batch. For the first request in the batch, the input
       *&#64;&#64;         will have shape [2] where the first element is value 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>BATCH_ACCUMULATED_ELEMENT_COUNT_WITH_ZERO = 2;</code>
       */
      public static final int BATCH_ACCUMULATED_ELEMENT_COUNT_WITH_ZERO_VALUE = 2;
      /**
       * <pre>
       *&#64;&#64;      .. cpp:enumerator:: Kind::BATCH_MAX_ELEMENT_COUNT_AS_SHAPE = 3
       *&#64;&#64;
       *&#64;&#64;         Among the requests in the batch, the max element count of the
       *&#64;&#64;         'source_input' will be added as input with shape
       *&#64;&#64;         [max_element_count] for the first request in the batch.
       *&#64;&#64;         For other requests, such input will be with shape [0].
       *&#64;&#64;         The data of the tensor will be uninitialized.
       *&#64;&#64;
       * </pre>
       *
       * <code>BATCH_MAX_ELEMENT_COUNT_AS_SHAPE = 3;</code>
       */
      public static final int BATCH_MAX_ELEMENT_COUNT_AS_SHAPE_VALUE = 3;


      @java.lang.Override
      public final int getNumber() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalArgumentException(
              "Can't get the number of an unknown enum value.");
        }
        return value;
      }

      /**
       * @param value The number of the enum to look for.
       * @return The enum associated with the given number.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static Kind valueOf(int value) {
        return forNumber(value);
      }

      public static Kind forNumber(int value) {
        switch (value) {
          case 0: return BATCH_ELEMENT_COUNT;
          case 1: return BATCH_ACCUMULATED_ELEMENT_COUNT;
          case 2: return BATCH_ACCUMULATED_ELEMENT_COUNT_WITH_ZERO;
          case 3: return BATCH_MAX_ELEMENT_COUNT_AS_SHAPE;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<Kind>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          Kind> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<Kind>() {
              @java.lang.Override
              public Kind findValueByNumber(int number) {
                return Kind.forNumber(number);
              }
            };

      public static com.google.protobuf.Internal.EnumVerifier 
          internalGetVerifier() {
        return KindVerifier.INSTANCE;
      }

      private static final class KindVerifier implements 
           com.google.protobuf.Internal.EnumVerifier { 
              static final com.google.protobuf.Internal.EnumVerifier           INSTANCE = new KindVerifier();
              @java.lang.Override
              public boolean isInRange(int number) {
                return Kind.forNumber(number) != null;
              }
            };

      private final int value;

      private Kind(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:inference.BatchInput.Kind)
    }

    public static final int KIND_FIELD_NUMBER = 1;
    private int kind_;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;       The kind of this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.BatchInput.Kind kind = 1;</code>
     * @return The enum numeric value on the wire for kind.
     */
    @java.lang.Override
    public int getKindValue() {
      return kind_;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;       The kind of this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.BatchInput.Kind kind = 1;</code>
     * @return The kind.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.BatchInput.Kind getKind() {
      inference.ModelConfigOuterClass.BatchInput.Kind result = inference.ModelConfigOuterClass.BatchInput.Kind.forNumber(kind_);
      return result == null ? inference.ModelConfigOuterClass.BatchInput.Kind.UNRECOGNIZED : result;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;       The kind of this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.BatchInput.Kind kind = 1;</code>
     * @param value The enum numeric value on the wire for kind to set.
     */
    private void setKindValue(int value) {
        kind_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;       The kind of this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.BatchInput.Kind kind = 1;</code>
     * @param value The kind to set.
     */
    private void setKind(inference.ModelConfigOuterClass.BatchInput.Kind value) {
      kind_ = value.getNumber();
      
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;       The kind of this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.BatchInput.Kind kind = 1;</code>
     */
    private void clearKind() {
      
      kind_ = 0;
    }

    public static final int TARGET_NAME_FIELD_NUMBER = 2;
    private com.google.protobuf.Internal.ProtobufList<java.lang.String> targetName_;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;       The name of the model inputs that the backend will create
     *&#64;&#64;       for this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 2;</code>
     * @return A list containing the targetName.
     */
    @java.lang.Override
    public java.util.List<java.lang.String> getTargetNameList() {
      return targetName_;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;       The name of the model inputs that the backend will create
     *&#64;&#64;       for this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 2;</code>
     * @return The count of targetName.
     */
    @java.lang.Override
    public int getTargetNameCount() {
      return targetName_.size();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;       The name of the model inputs that the backend will create
     *&#64;&#64;       for this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 2;</code>
     * @param index The index of the element to return.
     * @return The targetName at the given index.
     */
    @java.lang.Override
    public java.lang.String getTargetName(int index) {
      return targetName_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;       The name of the model inputs that the backend will create
     *&#64;&#64;       for this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 2;</code>
     * @param index The index of the value to return.
     * @return The bytes of the targetName at the given index.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getTargetNameBytes(int index) {
      return com.google.protobuf.ByteString.copyFromUtf8(
          targetName_.get(index));
    }
    private void ensureTargetNameIsMutable() {
      com.google.protobuf.Internal.ProtobufList<java.lang.String> tmp =
          targetName_;  if (!tmp.isModifiable()) {
        targetName_ =
            com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
       }
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;       The name of the model inputs that the backend will create
     *&#64;&#64;       for this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 2;</code>
     * @param index The index to set the value at.
     * @param value The targetName to set.
     */
    private void setTargetName(
        int index, java.lang.String value) {
      java.lang.Class<?> valueClass = value.getClass();
  ensureTargetNameIsMutable();
      targetName_.set(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;       The name of the model inputs that the backend will create
     *&#64;&#64;       for this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 2;</code>
     * @param value The targetName to add.
     */
    private void addTargetName(
        java.lang.String value) {
      java.lang.Class<?> valueClass = value.getClass();
  ensureTargetNameIsMutable();
      targetName_.add(value);
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;       The name of the model inputs that the backend will create
     *&#64;&#64;       for this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 2;</code>
     * @param values The targetName to add.
     */
    private void addAllTargetName(
        java.lang.Iterable<java.lang.String> values) {
      ensureTargetNameIsMutable();
      com.google.protobuf.AbstractMessageLite.addAll(
          values, targetName_);
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;       The name of the model inputs that the backend will create
     *&#64;&#64;       for this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 2;</code>
     */
    private void clearTargetName() {
      targetName_ = com.google.protobuf.GeneratedMessageLite.emptyProtobufList();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;       The name of the model inputs that the backend will create
     *&#64;&#64;       for this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 2;</code>
     * @param value The bytes of the targetName to add.
     */
    private void addTargetNameBytes(
        com.google.protobuf.ByteString value) {
      checkByteStringIsUtf8(value);
      ensureTargetNameIsMutable();
      targetName_.add(value.toStringUtf8());
    }

    public static final int DATA_TYPE_FIELD_NUMBER = 3;
    private int dataType_;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;       The input's datatype. The data type can be TYPE_INT32 or
     *&#64;&#64;       TYPE_FP32.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 3;</code>
     * @return The enum numeric value on the wire for dataType.
     */
    @java.lang.Override
    public int getDataTypeValue() {
      return dataType_;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;       The input's datatype. The data type can be TYPE_INT32 or
     *&#64;&#64;       TYPE_FP32.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 3;</code>
     * @return The dataType.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.DataType getDataType() {
      inference.ModelConfigOuterClass.DataType result = inference.ModelConfigOuterClass.DataType.forNumber(dataType_);
      return result == null ? inference.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;       The input's datatype. The data type can be TYPE_INT32 or
     *&#64;&#64;       TYPE_FP32.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 3;</code>
     * @param value The enum numeric value on the wire for dataType to set.
     */
    private void setDataTypeValue(int value) {
        dataType_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;       The input's datatype. The data type can be TYPE_INT32 or
     *&#64;&#64;       TYPE_FP32.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 3;</code>
     * @param value The dataType to set.
     */
    private void setDataType(inference.ModelConfigOuterClass.DataType value) {
      dataType_ = value.getNumber();
      
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;       The input's datatype. The data type can be TYPE_INT32 or
     *&#64;&#64;       TYPE_FP32.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 3;</code>
     */
    private void clearDataType() {
      
      dataType_ = 0;
    }

    public static final int SOURCE_INPUT_FIELD_NUMBER = 4;
    private com.google.protobuf.Internal.ProtobufList<java.lang.String> sourceInput_;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;       The backend derives the value for each batch input from one or
     *&#64;&#64;       more other inputs. 'source_input' gives the names of those
     *&#64;&#64;       inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 4;</code>
     * @return A list containing the sourceInput.
     */
    @java.lang.Override
    public java.util.List<java.lang.String> getSourceInputList() {
      return sourceInput_;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;       The backend derives the value for each batch input from one or
     *&#64;&#64;       more other inputs. 'source_input' gives the names of those
     *&#64;&#64;       inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 4;</code>
     * @return The count of sourceInput.
     */
    @java.lang.Override
    public int getSourceInputCount() {
      return sourceInput_.size();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;       The backend derives the value for each batch input from one or
     *&#64;&#64;       more other inputs. 'source_input' gives the names of those
     *&#64;&#64;       inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 4;</code>
     * @param index The index of the element to return.
     * @return The sourceInput at the given index.
     */
    @java.lang.Override
    public java.lang.String getSourceInput(int index) {
      return sourceInput_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;       The backend derives the value for each batch input from one or
     *&#64;&#64;       more other inputs. 'source_input' gives the names of those
     *&#64;&#64;       inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 4;</code>
     * @param index The index of the value to return.
     * @return The bytes of the sourceInput at the given index.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getSourceInputBytes(int index) {
      return com.google.protobuf.ByteString.copyFromUtf8(
          sourceInput_.get(index));
    }
    private void ensureSourceInputIsMutable() {
      com.google.protobuf.Internal.ProtobufList<java.lang.String> tmp =
          sourceInput_;  if (!tmp.isModifiable()) {
        sourceInput_ =
            com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
       }
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;       The backend derives the value for each batch input from one or
     *&#64;&#64;       more other inputs. 'source_input' gives the names of those
     *&#64;&#64;       inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 4;</code>
     * @param index The index to set the value at.
     * @param value The sourceInput to set.
     */
    private void setSourceInput(
        int index, java.lang.String value) {
      java.lang.Class<?> valueClass = value.getClass();
  ensureSourceInputIsMutable();
      sourceInput_.set(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;       The backend derives the value for each batch input from one or
     *&#64;&#64;       more other inputs. 'source_input' gives the names of those
     *&#64;&#64;       inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 4;</code>
     * @param value The sourceInput to add.
     */
    private void addSourceInput(
        java.lang.String value) {
      java.lang.Class<?> valueClass = value.getClass();
  ensureSourceInputIsMutable();
      sourceInput_.add(value);
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;       The backend derives the value for each batch input from one or
     *&#64;&#64;       more other inputs. 'source_input' gives the names of those
     *&#64;&#64;       inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 4;</code>
     * @param values The sourceInput to add.
     */
    private void addAllSourceInput(
        java.lang.Iterable<java.lang.String> values) {
      ensureSourceInputIsMutable();
      com.google.protobuf.AbstractMessageLite.addAll(
          values, sourceInput_);
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;       The backend derives the value for each batch input from one or
     *&#64;&#64;       more other inputs. 'source_input' gives the names of those
     *&#64;&#64;       inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 4;</code>
     */
    private void clearSourceInput() {
      sourceInput_ = com.google.protobuf.GeneratedMessageLite.emptyProtobufList();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;       The backend derives the value for each batch input from one or
     *&#64;&#64;       more other inputs. 'source_input' gives the names of those
     *&#64;&#64;       inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 4;</code>
     * @param value The bytes of the sourceInput to add.
     */
    private void addSourceInputBytes(
        com.google.protobuf.ByteString value) {
      checkByteStringIsUtf8(value);
      ensureSourceInputIsMutable();
      sourceInput_.add(value.toStringUtf8());
    }

    public static inference.ModelConfigOuterClass.BatchInput parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.BatchInput parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.BatchInput parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.BatchInput parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.BatchInput parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.BatchInput parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.BatchInput parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.BatchInput parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.BatchInput parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.BatchInput parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.BatchInput parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.BatchInput parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }

    public static Builder newBuilder() {
      return (Builder) DEFAULT_INSTANCE.createBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.BatchInput prototype) {
      return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
    }

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message BatchInput
     *&#64;&#64;
     *&#64;&#64;     A batch input is an additional input that must be added by
     *&#64;&#64;     the backend based on all the requests in a batch.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.BatchInput}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageLite.Builder<
          inference.ModelConfigOuterClass.BatchInput, Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.BatchInput)
        inference.ModelConfigOuterClass.BatchInputOrBuilder {
      // Construct using inference.ModelConfigOuterClass.BatchInput.newBuilder()
      private Builder() {
        super(DEFAULT_INSTANCE);
      }


      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.BatchInput.Kind kind = 1;</code>
       * @return The enum numeric value on the wire for kind.
       */
      @java.lang.Override
      public int getKindValue() {
        return instance.getKindValue();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.BatchInput.Kind kind = 1;</code>
       * @param value The kind to set.
       * @return This builder for chaining.
       */
      public Builder setKindValue(int value) {
        copyOnWrite();
        instance.setKindValue(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.BatchInput.Kind kind = 1;</code>
       * @return The kind.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.BatchInput.Kind getKind() {
        return instance.getKind();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.BatchInput.Kind kind = 1;</code>
       * @param value The enum numeric value on the wire for kind to set.
       * @return This builder for chaining.
       */
      public Builder setKind(inference.ModelConfigOuterClass.BatchInput.Kind value) {
        copyOnWrite();
        instance.setKind(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.BatchInput.Kind kind = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearKind() {
        copyOnWrite();
        instance.clearKind();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;       The name of the model inputs that the backend will create
       *&#64;&#64;       for this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 2;</code>
       * @return A list containing the targetName.
       */
      @java.lang.Override
      public java.util.List<java.lang.String>
          getTargetNameList() {
        return java.util.Collections.unmodifiableList(
            instance.getTargetNameList());
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;       The name of the model inputs that the backend will create
       *&#64;&#64;       for this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 2;</code>
       * @return The count of targetName.
       */
      @java.lang.Override
      public int getTargetNameCount() {
        return instance.getTargetNameCount();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;       The name of the model inputs that the backend will create
       *&#64;&#64;       for this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 2;</code>
       * @param index The index of the element to return.
       * @return The targetName at the given index.
       */
      @java.lang.Override
      public java.lang.String getTargetName(int index) {
        return instance.getTargetName(index);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;       The name of the model inputs that the backend will create
       *&#64;&#64;       for this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 2;</code>
       * @param index The index of the value to return.
       * @return The bytes of the targetName at the given index.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString
          getTargetNameBytes(int index) {
        return instance.getTargetNameBytes(index);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;       The name of the model inputs that the backend will create
       *&#64;&#64;       for this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 2;</code>
       * @param index The index to set the value at.
       * @param value The targetName to set.
       * @return This builder for chaining.
       */
      public Builder setTargetName(
          int index, java.lang.String value) {
        copyOnWrite();
        instance.setTargetName(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;       The name of the model inputs that the backend will create
       *&#64;&#64;       for this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 2;</code>
       * @param value The targetName to add.
       * @return This builder for chaining.
       */
      public Builder addTargetName(
          java.lang.String value) {
        copyOnWrite();
        instance.addTargetName(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;       The name of the model inputs that the backend will create
       *&#64;&#64;       for this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 2;</code>
       * @param values The targetName to add.
       * @return This builder for chaining.
       */
      public Builder addAllTargetName(
          java.lang.Iterable<java.lang.String> values) {
        copyOnWrite();
        instance.addAllTargetName(values);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;       The name of the model inputs that the backend will create
       *&#64;&#64;       for this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearTargetName() {
        copyOnWrite();
        instance.clearTargetName();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;       The name of the model inputs that the backend will create
       *&#64;&#64;       for this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 2;</code>
       * @param value The bytes of the targetName to add.
       * @return This builder for chaining.
       */
      public Builder addTargetNameBytes(
          com.google.protobuf.ByteString value) {
        copyOnWrite();
        instance.addTargetNameBytes(value);
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The input's datatype. The data type can be TYPE_INT32 or
       *&#64;&#64;       TYPE_FP32.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 3;</code>
       * @return The enum numeric value on the wire for dataType.
       */
      @java.lang.Override
      public int getDataTypeValue() {
        return instance.getDataTypeValue();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The input's datatype. The data type can be TYPE_INT32 or
       *&#64;&#64;       TYPE_FP32.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 3;</code>
       * @param value The dataType to set.
       * @return This builder for chaining.
       */
      public Builder setDataTypeValue(int value) {
        copyOnWrite();
        instance.setDataTypeValue(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The input's datatype. The data type can be TYPE_INT32 or
       *&#64;&#64;       TYPE_FP32.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 3;</code>
       * @return The dataType.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.DataType getDataType() {
        return instance.getDataType();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The input's datatype. The data type can be TYPE_INT32 or
       *&#64;&#64;       TYPE_FP32.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 3;</code>
       * @param value The enum numeric value on the wire for dataType to set.
       * @return This builder for chaining.
       */
      public Builder setDataType(inference.ModelConfigOuterClass.DataType value) {
        copyOnWrite();
        instance.setDataType(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The input's datatype. The data type can be TYPE_INT32 or
       *&#64;&#64;       TYPE_FP32.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearDataType() {
        copyOnWrite();
        instance.clearDataType();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;       The backend derives the value for each batch input from one or
       *&#64;&#64;       more other inputs. 'source_input' gives the names of those
       *&#64;&#64;       inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 4;</code>
       * @return A list containing the sourceInput.
       */
      @java.lang.Override
      public java.util.List<java.lang.String>
          getSourceInputList() {
        return java.util.Collections.unmodifiableList(
            instance.getSourceInputList());
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;       The backend derives the value for each batch input from one or
       *&#64;&#64;       more other inputs. 'source_input' gives the names of those
       *&#64;&#64;       inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 4;</code>
       * @return The count of sourceInput.
       */
      @java.lang.Override
      public int getSourceInputCount() {
        return instance.getSourceInputCount();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;       The backend derives the value for each batch input from one or
       *&#64;&#64;       more other inputs. 'source_input' gives the names of those
       *&#64;&#64;       inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 4;</code>
       * @param index The index of the element to return.
       * @return The sourceInput at the given index.
       */
      @java.lang.Override
      public java.lang.String getSourceInput(int index) {
        return instance.getSourceInput(index);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;       The backend derives the value for each batch input from one or
       *&#64;&#64;       more other inputs. 'source_input' gives the names of those
       *&#64;&#64;       inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 4;</code>
       * @param index The index of the value to return.
       * @return The bytes of the sourceInput at the given index.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString
          getSourceInputBytes(int index) {
        return instance.getSourceInputBytes(index);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;       The backend derives the value for each batch input from one or
       *&#64;&#64;       more other inputs. 'source_input' gives the names of those
       *&#64;&#64;       inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 4;</code>
       * @param index The index to set the value at.
       * @param value The sourceInput to set.
       * @return This builder for chaining.
       */
      public Builder setSourceInput(
          int index, java.lang.String value) {
        copyOnWrite();
        instance.setSourceInput(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;       The backend derives the value for each batch input from one or
       *&#64;&#64;       more other inputs. 'source_input' gives the names of those
       *&#64;&#64;       inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 4;</code>
       * @param value The sourceInput to add.
       * @return This builder for chaining.
       */
      public Builder addSourceInput(
          java.lang.String value) {
        copyOnWrite();
        instance.addSourceInput(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;       The backend derives the value for each batch input from one or
       *&#64;&#64;       more other inputs. 'source_input' gives the names of those
       *&#64;&#64;       inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 4;</code>
       * @param values The sourceInput to add.
       * @return This builder for chaining.
       */
      public Builder addAllSourceInput(
          java.lang.Iterable<java.lang.String> values) {
        copyOnWrite();
        instance.addAllSourceInput(values);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;       The backend derives the value for each batch input from one or
       *&#64;&#64;       more other inputs. 'source_input' gives the names of those
       *&#64;&#64;       inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearSourceInput() {
        copyOnWrite();
        instance.clearSourceInput();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;       The backend derives the value for each batch input from one or
       *&#64;&#64;       more other inputs. 'source_input' gives the names of those
       *&#64;&#64;       inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 4;</code>
       * @param value The bytes of the sourceInput to add.
       * @return This builder for chaining.
       */
      public Builder addSourceInputBytes(
          com.google.protobuf.ByteString value) {
        copyOnWrite();
        instance.addSourceInputBytes(value);
        return this;
      }

      // @@protoc_insertion_point(builder_scope:inference.BatchInput)
    }
    @java.lang.Override
    @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
    protected final java.lang.Object dynamicMethod(
        com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
        java.lang.Object arg0, java.lang.Object arg1) {
      switch (method) {
        case NEW_MUTABLE_INSTANCE: {
          return new inference.ModelConfigOuterClass.BatchInput();
        }
        case NEW_BUILDER: {
          return new Builder();
        }
        case BUILD_MESSAGE_INFO: {
            java.lang.Object[] objects = new java.lang.Object[] {
              "kind_",
              "targetName_",
              "dataType_",
              "sourceInput_",
            };
            java.lang.String info =
                "\u0000\u0004\u0000\u0000\u0001\u0004\u0004\u0000\u0002\u0000\u0001\f\u0002\u021a" +
                "\u0003\f\u0004\u021a";
            return newMessageInfo(DEFAULT_INSTANCE, info, objects);
        }
        // fall through
        case GET_DEFAULT_INSTANCE: {
          return DEFAULT_INSTANCE;
        }
        case GET_PARSER: {
          com.google.protobuf.Parser<inference.ModelConfigOuterClass.BatchInput> parser = PARSER;
          if (parser == null) {
            synchronized (inference.ModelConfigOuterClass.BatchInput.class) {
              parser = PARSER;
              if (parser == null) {
                parser =
                    new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.BatchInput>(
                        DEFAULT_INSTANCE);
                PARSER = parser;
              }
            }
          }
          return parser;
      }
      case GET_MEMOIZED_IS_INITIALIZED: {
        return (byte) 1;
      }
      case SET_MEMOIZED_IS_INITIALIZED: {
        return null;
      }
      }
      throw new UnsupportedOperationException();
    }


    // @@protoc_insertion_point(class_scope:inference.BatchInput)
    private static final inference.ModelConfigOuterClass.BatchInput DEFAULT_INSTANCE;
    static {
      BatchInput defaultInstance = new BatchInput();
      // New instances are implicitly immutable so no need to make
      // immutable.
      DEFAULT_INSTANCE = defaultInstance;
      com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
        BatchInput.class, defaultInstance);
    }

    public static inference.ModelConfigOuterClass.BatchInput getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static volatile com.google.protobuf.Parser<BatchInput> PARSER;

    public static com.google.protobuf.Parser<BatchInput> parser() {
      return DEFAULT_INSTANCE.getParserForType();
    }
  }

  public interface BatchOutputOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.BatchOutput)
      com.google.protobuf.MessageLiteOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;     The name of the outputs to be produced by this batch output
     *&#64;&#64;     specification.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 1;</code>
     * @return A list containing the targetName.
     */
    java.util.List<java.lang.String>
        getTargetNameList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;     The name of the outputs to be produced by this batch output
     *&#64;&#64;     specification.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 1;</code>
     * @return The count of targetName.
     */
    int getTargetNameCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;     The name of the outputs to be produced by this batch output
     *&#64;&#64;     specification.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 1;</code>
     * @param index The index of the element to return.
     * @return The targetName at the given index.
     */
    java.lang.String getTargetName(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;     The name of the outputs to be produced by this batch output
     *&#64;&#64;     specification.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 1;</code>
     * @param index The index of the element to return.
     * @return The targetName at the given index.
     */
    com.google.protobuf.ByteString
        getTargetNameBytes(int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this batch output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.BatchOutput.Kind kind = 2;</code>
     * @return The enum numeric value on the wire for kind.
     */
    int getKindValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this batch output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.BatchOutput.Kind kind = 2;</code>
     * @return The kind.
     */
    inference.ModelConfigOuterClass.BatchOutput.Kind getKind();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The backend derives each batch output from one or more inputs.
     *&#64;&#64;     'source_input' gives the names of those inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 3;</code>
     * @return A list containing the sourceInput.
     */
    java.util.List<java.lang.String>
        getSourceInputList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The backend derives each batch output from one or more inputs.
     *&#64;&#64;     'source_input' gives the names of those inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 3;</code>
     * @return The count of sourceInput.
     */
    int getSourceInputCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The backend derives each batch output from one or more inputs.
     *&#64;&#64;     'source_input' gives the names of those inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 3;</code>
     * @param index The index of the element to return.
     * @return The sourceInput at the given index.
     */
    java.lang.String getSourceInput(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The backend derives each batch output from one or more inputs.
     *&#64;&#64;     'source_input' gives the names of those inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 3;</code>
     * @param index The index of the element to return.
     * @return The sourceInput at the given index.
     */
    com.google.protobuf.ByteString
        getSourceInputBytes(int index);
  }
  /**
   * <pre>
   *&#64;&#64;.. cpp:var:: message BatchOutput
   *&#64;&#64;
   *&#64;&#64;   A batch output is an output produced by the model that must be handled
   *&#64;&#64;   differently by the backend based on all the requests in a batch.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.BatchOutput}
   */
  public  static final class BatchOutput extends
      com.google.protobuf.GeneratedMessageLite<
          BatchOutput, BatchOutput.Builder> implements
      // @@protoc_insertion_point(message_implements:inference.BatchOutput)
      BatchOutputOrBuilder {
    private BatchOutput() {
      targetName_ = com.google.protobuf.GeneratedMessageLite.emptyProtobufList();
      sourceInput_ = com.google.protobuf.GeneratedMessageLite.emptyProtobufList();
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:enum:: Kind
     *&#64;&#64;
     *&#64;&#64;     The kind of the batch output.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf enum {@code inference.BatchOutput.Kind}
     */
    public enum Kind
        implements com.google.protobuf.Internal.EnumLite {
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::BATCH_SCATTER_WITH_INPUT_SHAPE = 0
       *&#64;&#64;
       *&#64;&#64;       The output should be scattered according to the shape of
       *&#64;&#64;       'source_input'. The dynamic dimension of the output will
       *&#64;&#64;       be set to the value of the same dimension in the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>BATCH_SCATTER_WITH_INPUT_SHAPE = 0;</code>
       */
      BATCH_SCATTER_WITH_INPUT_SHAPE(0),
      UNRECOGNIZED(-1),
      ;

      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::BATCH_SCATTER_WITH_INPUT_SHAPE = 0
       *&#64;&#64;
       *&#64;&#64;       The output should be scattered according to the shape of
       *&#64;&#64;       'source_input'. The dynamic dimension of the output will
       *&#64;&#64;       be set to the value of the same dimension in the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>BATCH_SCATTER_WITH_INPUT_SHAPE = 0;</code>
       */
      public static final int BATCH_SCATTER_WITH_INPUT_SHAPE_VALUE = 0;


      @java.lang.Override
      public final int getNumber() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalArgumentException(
              "Can't get the number of an unknown enum value.");
        }
        return value;
      }

      /**
       * @param value The number of the enum to look for.
       * @return The enum associated with the given number.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static Kind valueOf(int value) {
        return forNumber(value);
      }

      public static Kind forNumber(int value) {
        switch (value) {
          case 0: return BATCH_SCATTER_WITH_INPUT_SHAPE;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<Kind>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          Kind> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<Kind>() {
              @java.lang.Override
              public Kind findValueByNumber(int number) {
                return Kind.forNumber(number);
              }
            };

      public static com.google.protobuf.Internal.EnumVerifier 
          internalGetVerifier() {
        return KindVerifier.INSTANCE;
      }

      private static final class KindVerifier implements 
           com.google.protobuf.Internal.EnumVerifier { 
              static final com.google.protobuf.Internal.EnumVerifier           INSTANCE = new KindVerifier();
              @java.lang.Override
              public boolean isInRange(int number) {
                return Kind.forNumber(number) != null;
              }
            };

      private final int value;

      private Kind(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:inference.BatchOutput.Kind)
    }

    public static final int TARGET_NAME_FIELD_NUMBER = 1;
    private com.google.protobuf.Internal.ProtobufList<java.lang.String> targetName_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;     The name of the outputs to be produced by this batch output
     *&#64;&#64;     specification.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 1;</code>
     * @return A list containing the targetName.
     */
    @java.lang.Override
    public java.util.List<java.lang.String> getTargetNameList() {
      return targetName_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;     The name of the outputs to be produced by this batch output
     *&#64;&#64;     specification.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 1;</code>
     * @return The count of targetName.
     */
    @java.lang.Override
    public int getTargetNameCount() {
      return targetName_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;     The name of the outputs to be produced by this batch output
     *&#64;&#64;     specification.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 1;</code>
     * @param index The index of the element to return.
     * @return The targetName at the given index.
     */
    @java.lang.Override
    public java.lang.String getTargetName(int index) {
      return targetName_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;     The name of the outputs to be produced by this batch output
     *&#64;&#64;     specification.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 1;</code>
     * @param index The index of the value to return.
     * @return The bytes of the targetName at the given index.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getTargetNameBytes(int index) {
      return com.google.protobuf.ByteString.copyFromUtf8(
          targetName_.get(index));
    }
    private void ensureTargetNameIsMutable() {
      com.google.protobuf.Internal.ProtobufList<java.lang.String> tmp =
          targetName_;  if (!tmp.isModifiable()) {
        targetName_ =
            com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
       }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;     The name of the outputs to be produced by this batch output
     *&#64;&#64;     specification.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 1;</code>
     * @param index The index to set the value at.
     * @param value The targetName to set.
     */
    private void setTargetName(
        int index, java.lang.String value) {
      java.lang.Class<?> valueClass = value.getClass();
  ensureTargetNameIsMutable();
      targetName_.set(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;     The name of the outputs to be produced by this batch output
     *&#64;&#64;     specification.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 1;</code>
     * @param value The targetName to add.
     */
    private void addTargetName(
        java.lang.String value) {
      java.lang.Class<?> valueClass = value.getClass();
  ensureTargetNameIsMutable();
      targetName_.add(value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;     The name of the outputs to be produced by this batch output
     *&#64;&#64;     specification.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 1;</code>
     * @param values The targetName to add.
     */
    private void addAllTargetName(
        java.lang.Iterable<java.lang.String> values) {
      ensureTargetNameIsMutable();
      com.google.protobuf.AbstractMessageLite.addAll(
          values, targetName_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;     The name of the outputs to be produced by this batch output
     *&#64;&#64;     specification.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 1;</code>
     */
    private void clearTargetName() {
      targetName_ = com.google.protobuf.GeneratedMessageLite.emptyProtobufList();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;     The name of the outputs to be produced by this batch output
     *&#64;&#64;     specification.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 1;</code>
     * @param value The bytes of the targetName to add.
     */
    private void addTargetNameBytes(
        com.google.protobuf.ByteString value) {
      checkByteStringIsUtf8(value);
      ensureTargetNameIsMutable();
      targetName_.add(value.toStringUtf8());
    }

    public static final int KIND_FIELD_NUMBER = 2;
    private int kind_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this batch output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.BatchOutput.Kind kind = 2;</code>
     * @return The enum numeric value on the wire for kind.
     */
    @java.lang.Override
    public int getKindValue() {
      return kind_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this batch output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.BatchOutput.Kind kind = 2;</code>
     * @return The kind.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.BatchOutput.Kind getKind() {
      inference.ModelConfigOuterClass.BatchOutput.Kind result = inference.ModelConfigOuterClass.BatchOutput.Kind.forNumber(kind_);
      return result == null ? inference.ModelConfigOuterClass.BatchOutput.Kind.UNRECOGNIZED : result;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this batch output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.BatchOutput.Kind kind = 2;</code>
     * @param value The enum numeric value on the wire for kind to set.
     */
    private void setKindValue(int value) {
        kind_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this batch output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.BatchOutput.Kind kind = 2;</code>
     * @param value The kind to set.
     */
    private void setKind(inference.ModelConfigOuterClass.BatchOutput.Kind value) {
      kind_ = value.getNumber();
      
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this batch output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.BatchOutput.Kind kind = 2;</code>
     */
    private void clearKind() {
      
      kind_ = 0;
    }

    public static final int SOURCE_INPUT_FIELD_NUMBER = 3;
    private com.google.protobuf.Internal.ProtobufList<java.lang.String> sourceInput_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The backend derives each batch output from one or more inputs.
     *&#64;&#64;     'source_input' gives the names of those inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 3;</code>
     * @return A list containing the sourceInput.
     */
    @java.lang.Override
    public java.util.List<java.lang.String> getSourceInputList() {
      return sourceInput_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The backend derives each batch output from one or more inputs.
     *&#64;&#64;     'source_input' gives the names of those inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 3;</code>
     * @return The count of sourceInput.
     */
    @java.lang.Override
    public int getSourceInputCount() {
      return sourceInput_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The backend derives each batch output from one or more inputs.
     *&#64;&#64;     'source_input' gives the names of those inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 3;</code>
     * @param index The index of the element to return.
     * @return The sourceInput at the given index.
     */
    @java.lang.Override
    public java.lang.String getSourceInput(int index) {
      return sourceInput_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The backend derives each batch output from one or more inputs.
     *&#64;&#64;     'source_input' gives the names of those inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 3;</code>
     * @param index The index of the value to return.
     * @return The bytes of the sourceInput at the given index.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getSourceInputBytes(int index) {
      return com.google.protobuf.ByteString.copyFromUtf8(
          sourceInput_.get(index));
    }
    private void ensureSourceInputIsMutable() {
      com.google.protobuf.Internal.ProtobufList<java.lang.String> tmp =
          sourceInput_;  if (!tmp.isModifiable()) {
        sourceInput_ =
            com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
       }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The backend derives each batch output from one or more inputs.
     *&#64;&#64;     'source_input' gives the names of those inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 3;</code>
     * @param index The index to set the value at.
     * @param value The sourceInput to set.
     */
    private void setSourceInput(
        int index, java.lang.String value) {
      java.lang.Class<?> valueClass = value.getClass();
  ensureSourceInputIsMutable();
      sourceInput_.set(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The backend derives each batch output from one or more inputs.
     *&#64;&#64;     'source_input' gives the names of those inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 3;</code>
     * @param value The sourceInput to add.
     */
    private void addSourceInput(
        java.lang.String value) {
      java.lang.Class<?> valueClass = value.getClass();
  ensureSourceInputIsMutable();
      sourceInput_.add(value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The backend derives each batch output from one or more inputs.
     *&#64;&#64;     'source_input' gives the names of those inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 3;</code>
     * @param values The sourceInput to add.
     */
    private void addAllSourceInput(
        java.lang.Iterable<java.lang.String> values) {
      ensureSourceInputIsMutable();
      com.google.protobuf.AbstractMessageLite.addAll(
          values, sourceInput_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The backend derives each batch output from one or more inputs.
     *&#64;&#64;     'source_input' gives the names of those inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 3;</code>
     */
    private void clearSourceInput() {
      sourceInput_ = com.google.protobuf.GeneratedMessageLite.emptyProtobufList();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The backend derives each batch output from one or more inputs.
     *&#64;&#64;     'source_input' gives the names of those inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 3;</code>
     * @param value The bytes of the sourceInput to add.
     */
    private void addSourceInputBytes(
        com.google.protobuf.ByteString value) {
      checkByteStringIsUtf8(value);
      ensureSourceInputIsMutable();
      sourceInput_.add(value.toStringUtf8());
    }

    public static inference.ModelConfigOuterClass.BatchOutput parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.BatchOutput parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.BatchOutput parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.BatchOutput parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.BatchOutput parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.BatchOutput parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.BatchOutput parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.BatchOutput parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.BatchOutput parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.BatchOutput parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.BatchOutput parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.BatchOutput parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }

    public static Builder newBuilder() {
      return (Builder) DEFAULT_INSTANCE.createBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.BatchOutput prototype) {
      return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
    }

    /**
     * <pre>
     *&#64;&#64;.. cpp:var:: message BatchOutput
     *&#64;&#64;
     *&#64;&#64;   A batch output is an output produced by the model that must be handled
     *&#64;&#64;   differently by the backend based on all the requests in a batch.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.BatchOutput}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageLite.Builder<
          inference.ModelConfigOuterClass.BatchOutput, Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.BatchOutput)
        inference.ModelConfigOuterClass.BatchOutputOrBuilder {
      // Construct using inference.ModelConfigOuterClass.BatchOutput.newBuilder()
      private Builder() {
        super(DEFAULT_INSTANCE);
      }


      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;     The name of the outputs to be produced by this batch output
       *&#64;&#64;     specification.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 1;</code>
       * @return A list containing the targetName.
       */
      @java.lang.Override
      public java.util.List<java.lang.String>
          getTargetNameList() {
        return java.util.Collections.unmodifiableList(
            instance.getTargetNameList());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;     The name of the outputs to be produced by this batch output
       *&#64;&#64;     specification.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 1;</code>
       * @return The count of targetName.
       */
      @java.lang.Override
      public int getTargetNameCount() {
        return instance.getTargetNameCount();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;     The name of the outputs to be produced by this batch output
       *&#64;&#64;     specification.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 1;</code>
       * @param index The index of the element to return.
       * @return The targetName at the given index.
       */
      @java.lang.Override
      public java.lang.String getTargetName(int index) {
        return instance.getTargetName(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;     The name of the outputs to be produced by this batch output
       *&#64;&#64;     specification.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 1;</code>
       * @param index The index of the value to return.
       * @return The bytes of the targetName at the given index.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString
          getTargetNameBytes(int index) {
        return instance.getTargetNameBytes(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;     The name of the outputs to be produced by this batch output
       *&#64;&#64;     specification.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 1;</code>
       * @param index The index to set the value at.
       * @param value The targetName to set.
       * @return This builder for chaining.
       */
      public Builder setTargetName(
          int index, java.lang.String value) {
        copyOnWrite();
        instance.setTargetName(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;     The name of the outputs to be produced by this batch output
       *&#64;&#64;     specification.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 1;</code>
       * @param value The targetName to add.
       * @return This builder for chaining.
       */
      public Builder addTargetName(
          java.lang.String value) {
        copyOnWrite();
        instance.addTargetName(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;     The name of the outputs to be produced by this batch output
       *&#64;&#64;     specification.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 1;</code>
       * @param values The targetName to add.
       * @return This builder for chaining.
       */
      public Builder addAllTargetName(
          java.lang.Iterable<java.lang.String> values) {
        copyOnWrite();
        instance.addAllTargetName(values);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;     The name of the outputs to be produced by this batch output
       *&#64;&#64;     specification.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearTargetName() {
        copyOnWrite();
        instance.clearTargetName();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;     The name of the outputs to be produced by this batch output
       *&#64;&#64;     specification.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 1;</code>
       * @param value The bytes of the targetName to add.
       * @return This builder for chaining.
       */
      public Builder addTargetNameBytes(
          com.google.protobuf.ByteString value) {
        copyOnWrite();
        instance.addTargetNameBytes(value);
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this batch output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.BatchOutput.Kind kind = 2;</code>
       * @return The enum numeric value on the wire for kind.
       */
      @java.lang.Override
      public int getKindValue() {
        return instance.getKindValue();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this batch output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.BatchOutput.Kind kind = 2;</code>
       * @param value The kind to set.
       * @return This builder for chaining.
       */
      public Builder setKindValue(int value) {
        copyOnWrite();
        instance.setKindValue(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this batch output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.BatchOutput.Kind kind = 2;</code>
       * @return The kind.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.BatchOutput.Kind getKind() {
        return instance.getKind();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this batch output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.BatchOutput.Kind kind = 2;</code>
       * @param value The enum numeric value on the wire for kind to set.
       * @return This builder for chaining.
       */
      public Builder setKind(inference.ModelConfigOuterClass.BatchOutput.Kind value) {
        copyOnWrite();
        instance.setKind(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this batch output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.BatchOutput.Kind kind = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearKind() {
        copyOnWrite();
        instance.clearKind();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The backend derives each batch output from one or more inputs.
       *&#64;&#64;     'source_input' gives the names of those inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 3;</code>
       * @return A list containing the sourceInput.
       */
      @java.lang.Override
      public java.util.List<java.lang.String>
          getSourceInputList() {
        return java.util.Collections.unmodifiableList(
            instance.getSourceInputList());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The backend derives each batch output from one or more inputs.
       *&#64;&#64;     'source_input' gives the names of those inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 3;</code>
       * @return The count of sourceInput.
       */
      @java.lang.Override
      public int getSourceInputCount() {
        return instance.getSourceInputCount();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The backend derives each batch output from one or more inputs.
       *&#64;&#64;     'source_input' gives the names of those inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 3;</code>
       * @param index The index of the element to return.
       * @return The sourceInput at the given index.
       */
      @java.lang.Override
      public java.lang.String getSourceInput(int index) {
        return instance.getSourceInput(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The backend derives each batch output from one or more inputs.
       *&#64;&#64;     'source_input' gives the names of those inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 3;</code>
       * @param index The index of the value to return.
       * @return The bytes of the sourceInput at the given index.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString
          getSourceInputBytes(int index) {
        return instance.getSourceInputBytes(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The backend derives each batch output from one or more inputs.
       *&#64;&#64;     'source_input' gives the names of those inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 3;</code>
       * @param index The index to set the value at.
       * @param value The sourceInput to set.
       * @return This builder for chaining.
       */
      public Builder setSourceInput(
          int index, java.lang.String value) {
        copyOnWrite();
        instance.setSourceInput(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The backend derives each batch output from one or more inputs.
       *&#64;&#64;     'source_input' gives the names of those inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 3;</code>
       * @param value The sourceInput to add.
       * @return This builder for chaining.
       */
      public Builder addSourceInput(
          java.lang.String value) {
        copyOnWrite();
        instance.addSourceInput(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The backend derives each batch output from one or more inputs.
       *&#64;&#64;     'source_input' gives the names of those inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 3;</code>
       * @param values The sourceInput to add.
       * @return This builder for chaining.
       */
      public Builder addAllSourceInput(
          java.lang.Iterable<java.lang.String> values) {
        copyOnWrite();
        instance.addAllSourceInput(values);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The backend derives each batch output from one or more inputs.
       *&#64;&#64;     'source_input' gives the names of those inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearSourceInput() {
        copyOnWrite();
        instance.clearSourceInput();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The backend derives each batch output from one or more inputs.
       *&#64;&#64;     'source_input' gives the names of those inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 3;</code>
       * @param value The bytes of the sourceInput to add.
       * @return This builder for chaining.
       */
      public Builder addSourceInputBytes(
          com.google.protobuf.ByteString value) {
        copyOnWrite();
        instance.addSourceInputBytes(value);
        return this;
      }

      // @@protoc_insertion_point(builder_scope:inference.BatchOutput)
    }
    @java.lang.Override
    @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
    protected final java.lang.Object dynamicMethod(
        com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
        java.lang.Object arg0, java.lang.Object arg1) {
      switch (method) {
        case NEW_MUTABLE_INSTANCE: {
          return new inference.ModelConfigOuterClass.BatchOutput();
        }
        case NEW_BUILDER: {
          return new Builder();
        }
        case BUILD_MESSAGE_INFO: {
            java.lang.Object[] objects = new java.lang.Object[] {
              "targetName_",
              "kind_",
              "sourceInput_",
            };
            java.lang.String info =
                "\u0000\u0003\u0000\u0000\u0001\u0003\u0003\u0000\u0002\u0000\u0001\u021a\u0002\f" +
                "\u0003\u021a";
            return newMessageInfo(DEFAULT_INSTANCE, info, objects);
        }
        // fall through
        case GET_DEFAULT_INSTANCE: {
          return DEFAULT_INSTANCE;
        }
        case GET_PARSER: {
          com.google.protobuf.Parser<inference.ModelConfigOuterClass.BatchOutput> parser = PARSER;
          if (parser == null) {
            synchronized (inference.ModelConfigOuterClass.BatchOutput.class) {
              parser = PARSER;
              if (parser == null) {
                parser =
                    new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.BatchOutput>(
                        DEFAULT_INSTANCE);
                PARSER = parser;
              }
            }
          }
          return parser;
      }
      case GET_MEMOIZED_IS_INITIALIZED: {
        return (byte) 1;
      }
      case SET_MEMOIZED_IS_INITIALIZED: {
        return null;
      }
      }
      throw new UnsupportedOperationException();
    }


    // @@protoc_insertion_point(class_scope:inference.BatchOutput)
    private static final inference.ModelConfigOuterClass.BatchOutput DEFAULT_INSTANCE;
    static {
      BatchOutput defaultInstance = new BatchOutput();
      // New instances are implicitly immutable so no need to make
      // immutable.
      DEFAULT_INSTANCE = defaultInstance;
      com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
        BatchOutput.class, defaultInstance);
    }

    public static inference.ModelConfigOuterClass.BatchOutput getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static volatile com.google.protobuf.Parser<BatchOutput> PARSER;

    public static com.google.protobuf.Parser<BatchOutput> parser() {
      return DEFAULT_INSTANCE.getParserForType();
    }
  }

  public interface ModelVersionPolicyOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelVersionPolicy)
      com.google.protobuf.MessageLiteOrBuilder {

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Latest latest
     *&#64;&#64;
     *&#64;&#64;       Serve only latest version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
     * @return Whether the latest field is set.
     */
    boolean hasLatest();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Latest latest
     *&#64;&#64;
     *&#64;&#64;       Serve only latest version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
     * @return The latest.
     */
    inference.ModelConfigOuterClass.ModelVersionPolicy.Latest getLatest();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: All all
     *&#64;&#64;
     *&#64;&#64;       Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.All all = 2;</code>
     * @return Whether the all field is set.
     */
    boolean hasAll();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: All all
     *&#64;&#64;
     *&#64;&#64;       Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.All all = 2;</code>
     * @return The all.
     */
    inference.ModelConfigOuterClass.ModelVersionPolicy.All getAll();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Specific specific
     *&#64;&#64;
     *&#64;&#64;       Serve only specific version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
     * @return Whether the specific field is set.
     */
    boolean hasSpecific();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Specific specific
     *&#64;&#64;
     *&#64;&#64;       Serve only specific version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
     * @return The specific.
     */
    inference.ModelConfigOuterClass.ModelVersionPolicy.Specific getSpecific();

    public inference.ModelConfigOuterClass.ModelVersionPolicy.PolicyChoiceCase getPolicyChoiceCase();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelVersionPolicy
   *&#64;&#64;
   *&#64;&#64;   Policy indicating which versions of a model should be made
   *&#64;&#64;   available by the inference server.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelVersionPolicy}
   */
  public  static final class ModelVersionPolicy extends
      com.google.protobuf.GeneratedMessageLite<
          ModelVersionPolicy, ModelVersionPolicy.Builder> implements
      // @@protoc_insertion_point(message_implements:inference.ModelVersionPolicy)
      ModelVersionPolicyOrBuilder {
    private ModelVersionPolicy() {
    }
    public interface LatestOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelVersionPolicy.Latest)
        com.google.protobuf.MessageLiteOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint32 num_versions
       *&#64;&#64;
       *&#64;&#64;       Serve only the 'num_versions' highest-numbered versions. T
       *&#64;&#64;       The default value of 'num_versions' is 1, indicating that by
       *&#64;&#64;       default only the single highest-number version of a
       *&#64;&#64;       model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 num_versions = 1;</code>
       * @return The numVersions.
       */
      int getNumVersions();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message Latest
     *&#64;&#64;
     *&#64;&#64;     Serve only the latest version(s) of a model. This is
     *&#64;&#64;     the default policy.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelVersionPolicy.Latest}
     */
    public  static final class Latest extends
        com.google.protobuf.GeneratedMessageLite<
            Latest, Latest.Builder> implements
        // @@protoc_insertion_point(message_implements:inference.ModelVersionPolicy.Latest)
        LatestOrBuilder {
      private Latest() {
      }
      public static final int NUM_VERSIONS_FIELD_NUMBER = 1;
      private int numVersions_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint32 num_versions
       *&#64;&#64;
       *&#64;&#64;       Serve only the 'num_versions' highest-numbered versions. T
       *&#64;&#64;       The default value of 'num_versions' is 1, indicating that by
       *&#64;&#64;       default only the single highest-number version of a
       *&#64;&#64;       model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 num_versions = 1;</code>
       * @return The numVersions.
       */
      @java.lang.Override
      public int getNumVersions() {
        return numVersions_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint32 num_versions
       *&#64;&#64;
       *&#64;&#64;       Serve only the 'num_versions' highest-numbered versions. T
       *&#64;&#64;       The default value of 'num_versions' is 1, indicating that by
       *&#64;&#64;       default only the single highest-number version of a
       *&#64;&#64;       model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 num_versions = 1;</code>
       * @param value The numVersions to set.
       */
      private void setNumVersions(int value) {
        
        numVersions_ = value;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint32 num_versions
       *&#64;&#64;
       *&#64;&#64;       Serve only the 'num_versions' highest-numbered versions. T
       *&#64;&#64;       The default value of 'num_versions' is 1, indicating that by
       *&#64;&#64;       default only the single highest-number version of a
       *&#64;&#64;       model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 num_versions = 1;</code>
       */
      private void clearNumVersions() {
        
        numVersions_ = 0;
      }

      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }

      public static Builder newBuilder() {
        return (Builder) DEFAULT_INSTANCE.createBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelVersionPolicy.Latest prototype) {
        return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message Latest
       *&#64;&#64;
       *&#64;&#64;     Serve only the latest version(s) of a model. This is
       *&#64;&#64;     the default policy.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelVersionPolicy.Latest}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageLite.Builder<
            inference.ModelConfigOuterClass.ModelVersionPolicy.Latest, Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelVersionPolicy.Latest)
          inference.ModelConfigOuterClass.ModelVersionPolicy.LatestOrBuilder {
        // Construct using inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.newBuilder()
        private Builder() {
          super(DEFAULT_INSTANCE);
        }


        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint32 num_versions
         *&#64;&#64;
         *&#64;&#64;       Serve only the 'num_versions' highest-numbered versions. T
         *&#64;&#64;       The default value of 'num_versions' is 1, indicating that by
         *&#64;&#64;       default only the single highest-number version of a
         *&#64;&#64;       model will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint32 num_versions = 1;</code>
         * @return The numVersions.
         */
        @java.lang.Override
        public int getNumVersions() {
          return instance.getNumVersions();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint32 num_versions
         *&#64;&#64;
         *&#64;&#64;       Serve only the 'num_versions' highest-numbered versions. T
         *&#64;&#64;       The default value of 'num_versions' is 1, indicating that by
         *&#64;&#64;       default only the single highest-number version of a
         *&#64;&#64;       model will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint32 num_versions = 1;</code>
         * @param value The numVersions to set.
         * @return This builder for chaining.
         */
        public Builder setNumVersions(int value) {
          copyOnWrite();
          instance.setNumVersions(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint32 num_versions
         *&#64;&#64;
         *&#64;&#64;       Serve only the 'num_versions' highest-numbered versions. T
         *&#64;&#64;       The default value of 'num_versions' is 1, indicating that by
         *&#64;&#64;       default only the single highest-number version of a
         *&#64;&#64;       model will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint32 num_versions = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearNumVersions() {
          copyOnWrite();
          instance.clearNumVersions();
          return this;
        }

        // @@protoc_insertion_point(builder_scope:inference.ModelVersionPolicy.Latest)
      }
      @java.lang.Override
      @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
      protected final java.lang.Object dynamicMethod(
          com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
          java.lang.Object arg0, java.lang.Object arg1) {
        switch (method) {
          case NEW_MUTABLE_INSTANCE: {
            return new inference.ModelConfigOuterClass.ModelVersionPolicy.Latest();
          }
          case NEW_BUILDER: {
            return new Builder();
          }
          case BUILD_MESSAGE_INFO: {
              java.lang.Object[] objects = new java.lang.Object[] {
                "numVersions_",
              };
              java.lang.String info =
                  "\u0000\u0001\u0000\u0000\u0001\u0001\u0001\u0000\u0000\u0000\u0001\u000b";
              return newMessageInfo(DEFAULT_INSTANCE, info, objects);
          }
          // fall through
          case GET_DEFAULT_INSTANCE: {
            return DEFAULT_INSTANCE;
          }
          case GET_PARSER: {
            com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelVersionPolicy.Latest> parser = PARSER;
            if (parser == null) {
              synchronized (inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.class) {
                parser = PARSER;
                if (parser == null) {
                  parser =
                      new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelVersionPolicy.Latest>(
                          DEFAULT_INSTANCE);
                  PARSER = parser;
                }
              }
            }
            return parser;
        }
        case GET_MEMOIZED_IS_INITIALIZED: {
          return (byte) 1;
        }
        case SET_MEMOIZED_IS_INITIALIZED: {
          return null;
        }
        }
        throw new UnsupportedOperationException();
      }


      // @@protoc_insertion_point(class_scope:inference.ModelVersionPolicy.Latest)
      private static final inference.ModelConfigOuterClass.ModelVersionPolicy.Latest DEFAULT_INSTANCE;
      static {
        Latest defaultInstance = new Latest();
        // New instances are implicitly immutable so no need to make
        // immutable.
        DEFAULT_INSTANCE = defaultInstance;
        com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
          Latest.class, defaultInstance);
      }

      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static volatile com.google.protobuf.Parser<Latest> PARSER;

      public static com.google.protobuf.Parser<Latest> parser() {
        return DEFAULT_INSTANCE.getParserForType();
      }
    }

    public interface AllOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelVersionPolicy.All)
        com.google.protobuf.MessageLiteOrBuilder {
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message All
     *&#64;&#64;
     *&#64;&#64;     Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelVersionPolicy.All}
     */
    public  static final class All extends
        com.google.protobuf.GeneratedMessageLite<
            All, All.Builder> implements
        // @@protoc_insertion_point(message_implements:inference.ModelVersionPolicy.All)
        AllOrBuilder {
      private All() {
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }

      public static Builder newBuilder() {
        return (Builder) DEFAULT_INSTANCE.createBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelVersionPolicy.All prototype) {
        return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message All
       *&#64;&#64;
       *&#64;&#64;     Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelVersionPolicy.All}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageLite.Builder<
            inference.ModelConfigOuterClass.ModelVersionPolicy.All, Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelVersionPolicy.All)
          inference.ModelConfigOuterClass.ModelVersionPolicy.AllOrBuilder {
        // Construct using inference.ModelConfigOuterClass.ModelVersionPolicy.All.newBuilder()
        private Builder() {
          super(DEFAULT_INSTANCE);
        }


        // @@protoc_insertion_point(builder_scope:inference.ModelVersionPolicy.All)
      }
      @java.lang.Override
      @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
      protected final java.lang.Object dynamicMethod(
          com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
          java.lang.Object arg0, java.lang.Object arg1) {
        switch (method) {
          case NEW_MUTABLE_INSTANCE: {
            return new inference.ModelConfigOuterClass.ModelVersionPolicy.All();
          }
          case NEW_BUILDER: {
            return new Builder();
          }
          case BUILD_MESSAGE_INFO: {
              java.lang.Object[] objects = null;java.lang.String info =
                  "\u0000\u0000";
              return newMessageInfo(DEFAULT_INSTANCE, info, objects);
          }
          // fall through
          case GET_DEFAULT_INSTANCE: {
            return DEFAULT_INSTANCE;
          }
          case GET_PARSER: {
            com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelVersionPolicy.All> parser = PARSER;
            if (parser == null) {
              synchronized (inference.ModelConfigOuterClass.ModelVersionPolicy.All.class) {
                parser = PARSER;
                if (parser == null) {
                  parser =
                      new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelVersionPolicy.All>(
                          DEFAULT_INSTANCE);
                  PARSER = parser;
                }
              }
            }
            return parser;
        }
        case GET_MEMOIZED_IS_INITIALIZED: {
          return (byte) 1;
        }
        case SET_MEMOIZED_IS_INITIALIZED: {
          return null;
        }
        }
        throw new UnsupportedOperationException();
      }


      // @@protoc_insertion_point(class_scope:inference.ModelVersionPolicy.All)
      private static final inference.ModelConfigOuterClass.ModelVersionPolicy.All DEFAULT_INSTANCE;
      static {
        All defaultInstance = new All();
        // New instances are implicitly immutable so no need to make
        // immutable.
        DEFAULT_INSTANCE = defaultInstance;
        com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
          All.class, defaultInstance);
      }

      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static volatile com.google.protobuf.Parser<All> PARSER;

      public static com.google.protobuf.Parser<All> parser() {
        return DEFAULT_INSTANCE.getParserForType();
      }
    }

    public interface SpecificOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelVersionPolicy.Specific)
        com.google.protobuf.MessageLiteOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       * @return A list containing the versions.
       */
      java.util.List<java.lang.Long> getVersionsList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       * @return The count of versions.
       */
      int getVersionsCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       * @param index The index of the element to return.
       * @return The versions at the given index.
       */
      long getVersions(int index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message Specific
     *&#64;&#64;
     *&#64;&#64;     Serve only specific versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelVersionPolicy.Specific}
     */
    public  static final class Specific extends
        com.google.protobuf.GeneratedMessageLite<
            Specific, Specific.Builder> implements
        // @@protoc_insertion_point(message_implements:inference.ModelVersionPolicy.Specific)
        SpecificOrBuilder {
      private Specific() {
        versions_ = emptyLongList();
      }
      public static final int VERSIONS_FIELD_NUMBER = 1;
      private com.google.protobuf.Internal.LongList versions_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       * @return A list containing the versions.
       */
      @java.lang.Override
      public java.util.List<java.lang.Long>
          getVersionsList() {
        return versions_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       * @return The count of versions.
       */
      @java.lang.Override
      public int getVersionsCount() {
        return versions_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       * @param index The index of the element to return.
       * @return The versions at the given index.
       */
      @java.lang.Override
      public long getVersions(int index) {
        return versions_.getLong(index);
      }
      private int versionsMemoizedSerializedSize = -1;
      private void ensureVersionsIsMutable() {
        com.google.protobuf.Internal.LongList tmp = versions_;
        if (!tmp.isModifiable()) {
          versions_ =
              com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
         }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       * @param index The index to set the value at.
       * @param value The versions to set.
       */
      private void setVersions(
          int index, long value) {
        ensureVersionsIsMutable();
        versions_.setLong(index, value);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       * @param value The versions to add.
       */
      private void addVersions(long value) {
        ensureVersionsIsMutable();
        versions_.addLong(value);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       * @param values The versions to add.
       */
      private void addAllVersions(
          java.lang.Iterable<? extends java.lang.Long> values) {
        ensureVersionsIsMutable();
        com.google.protobuf.AbstractMessageLite.addAll(
            values, versions_);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       */
      private void clearVersions() {
        versions_ = emptyLongList();
      }

      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }

      public static Builder newBuilder() {
        return (Builder) DEFAULT_INSTANCE.createBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelVersionPolicy.Specific prototype) {
        return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message Specific
       *&#64;&#64;
       *&#64;&#64;     Serve only specific versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelVersionPolicy.Specific}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageLite.Builder<
            inference.ModelConfigOuterClass.ModelVersionPolicy.Specific, Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelVersionPolicy.Specific)
          inference.ModelConfigOuterClass.ModelVersionPolicy.SpecificOrBuilder {
        // Construct using inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.newBuilder()
        private Builder() {
          super(DEFAULT_INSTANCE);
        }


        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         * @return A list containing the versions.
         */
        @java.lang.Override
        public java.util.List<java.lang.Long>
            getVersionsList() {
          return java.util.Collections.unmodifiableList(
              instance.getVersionsList());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         * @return The count of versions.
         */
        @java.lang.Override
        public int getVersionsCount() {
          return instance.getVersionsCount();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         * @param index The index of the element to return.
         * @return The versions at the given index.
         */
        @java.lang.Override
        public long getVersions(int index) {
          return instance.getVersions(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         * @param value The versions to set.
         * @return This builder for chaining.
         */
        public Builder setVersions(
            int index, long value) {
          copyOnWrite();
          instance.setVersions(index, value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         * @param value The versions to add.
         * @return This builder for chaining.
         */
        public Builder addVersions(long value) {
          copyOnWrite();
          instance.addVersions(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         * @param values The versions to add.
         * @return This builder for chaining.
         */
        public Builder addAllVersions(
            java.lang.Iterable<? extends java.lang.Long> values) {
          copyOnWrite();
          instance.addAllVersions(values);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearVersions() {
          copyOnWrite();
          instance.clearVersions();
          return this;
        }

        // @@protoc_insertion_point(builder_scope:inference.ModelVersionPolicy.Specific)
      }
      @java.lang.Override
      @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
      protected final java.lang.Object dynamicMethod(
          com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
          java.lang.Object arg0, java.lang.Object arg1) {
        switch (method) {
          case NEW_MUTABLE_INSTANCE: {
            return new inference.ModelConfigOuterClass.ModelVersionPolicy.Specific();
          }
          case NEW_BUILDER: {
            return new Builder();
          }
          case BUILD_MESSAGE_INFO: {
              java.lang.Object[] objects = new java.lang.Object[] {
                "versions_",
              };
              java.lang.String info =
                  "\u0000\u0001\u0000\u0000\u0001\u0001\u0001\u0000\u0001\u0000\u0001%";
              return newMessageInfo(DEFAULT_INSTANCE, info, objects);
          }
          // fall through
          case GET_DEFAULT_INSTANCE: {
            return DEFAULT_INSTANCE;
          }
          case GET_PARSER: {
            com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelVersionPolicy.Specific> parser = PARSER;
            if (parser == null) {
              synchronized (inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.class) {
                parser = PARSER;
                if (parser == null) {
                  parser =
                      new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelVersionPolicy.Specific>(
                          DEFAULT_INSTANCE);
                  PARSER = parser;
                }
              }
            }
            return parser;
        }
        case GET_MEMOIZED_IS_INITIALIZED: {
          return (byte) 1;
        }
        case SET_MEMOIZED_IS_INITIALIZED: {
          return null;
        }
        }
        throw new UnsupportedOperationException();
      }


      // @@protoc_insertion_point(class_scope:inference.ModelVersionPolicy.Specific)
      private static final inference.ModelConfigOuterClass.ModelVersionPolicy.Specific DEFAULT_INSTANCE;
      static {
        Specific defaultInstance = new Specific();
        // New instances are implicitly immutable so no need to make
        // immutable.
        DEFAULT_INSTANCE = defaultInstance;
        com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
          Specific.class, defaultInstance);
      }

      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static volatile com.google.protobuf.Parser<Specific> PARSER;

      public static com.google.protobuf.Parser<Specific> parser() {
        return DEFAULT_INSTANCE.getParserForType();
      }
    }

    private int policyChoiceCase_ = 0;
    private java.lang.Object policyChoice_;
    public enum PolicyChoiceCase {
      LATEST(1),
      ALL(2),
      SPECIFIC(3),
      POLICYCHOICE_NOT_SET(0);
      private final int value;
      private PolicyChoiceCase(int value) {
        this.value = value;
      }
      /**
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static PolicyChoiceCase valueOf(int value) {
        return forNumber(value);
      }

      public static PolicyChoiceCase forNumber(int value) {
        switch (value) {
          case 1: return LATEST;
          case 2: return ALL;
          case 3: return SPECIFIC;
          case 0: return POLICYCHOICE_NOT_SET;
          default: return null;
        }
      }
      public int getNumber() {
        return this.value;
      }
    };

    @java.lang.Override
    public PolicyChoiceCase
    getPolicyChoiceCase() {
      return PolicyChoiceCase.forNumber(
          policyChoiceCase_);
    }

    private void clearPolicyChoice() {
      policyChoiceCase_ = 0;
      policyChoice_ = null;
    }

    public static final int LATEST_FIELD_NUMBER = 1;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Latest latest
     *&#64;&#64;
     *&#64;&#64;       Serve only latest version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
     */
    @java.lang.Override
    public boolean hasLatest() {
      return policyChoiceCase_ == 1;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Latest latest
     *&#64;&#64;
     *&#64;&#64;       Serve only latest version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelVersionPolicy.Latest getLatest() {
      if (policyChoiceCase_ == 1) {
         return (inference.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_;
      }
      return inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Latest latest
     *&#64;&#64;
     *&#64;&#64;       Serve only latest version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
     */
    private void setLatest(inference.ModelConfigOuterClass.ModelVersionPolicy.Latest value) {
      value.getClass();
  policyChoice_ = value;
      policyChoiceCase_ = 1;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Latest latest
     *&#64;&#64;
     *&#64;&#64;       Serve only latest version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
     */
    private void mergeLatest(inference.ModelConfigOuterClass.ModelVersionPolicy.Latest value) {
      value.getClass();
  if (policyChoiceCase_ == 1 &&
          policyChoice_ != inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance()) {
        policyChoice_ = inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.newBuilder((inference.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_)
            .mergeFrom(value).buildPartial();
      } else {
        policyChoice_ = value;
      }
      policyChoiceCase_ = 1;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Latest latest
     *&#64;&#64;
     *&#64;&#64;       Serve only latest version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
     */
    private void clearLatest() {
      if (policyChoiceCase_ == 1) {
        policyChoiceCase_ = 0;
        policyChoice_ = null;
      }
    }

    public static final int ALL_FIELD_NUMBER = 2;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: All all
     *&#64;&#64;
     *&#64;&#64;       Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.All all = 2;</code>
     */
    @java.lang.Override
    public boolean hasAll() {
      return policyChoiceCase_ == 2;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: All all
     *&#64;&#64;
     *&#64;&#64;       Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.All all = 2;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelVersionPolicy.All getAll() {
      if (policyChoiceCase_ == 2) {
         return (inference.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_;
      }
      return inference.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: All all
     *&#64;&#64;
     *&#64;&#64;       Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.All all = 2;</code>
     */
    private void setAll(inference.ModelConfigOuterClass.ModelVersionPolicy.All value) {
      value.getClass();
  policyChoice_ = value;
      policyChoiceCase_ = 2;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: All all
     *&#64;&#64;
     *&#64;&#64;       Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.All all = 2;</code>
     */
    private void mergeAll(inference.ModelConfigOuterClass.ModelVersionPolicy.All value) {
      value.getClass();
  if (policyChoiceCase_ == 2 &&
          policyChoice_ != inference.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance()) {
        policyChoice_ = inference.ModelConfigOuterClass.ModelVersionPolicy.All.newBuilder((inference.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_)
            .mergeFrom(value).buildPartial();
      } else {
        policyChoice_ = value;
      }
      policyChoiceCase_ = 2;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: All all
     *&#64;&#64;
     *&#64;&#64;       Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.All all = 2;</code>
     */
    private void clearAll() {
      if (policyChoiceCase_ == 2) {
        policyChoiceCase_ = 0;
        policyChoice_ = null;
      }
    }

    public static final int SPECIFIC_FIELD_NUMBER = 3;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Specific specific
     *&#64;&#64;
     *&#64;&#64;       Serve only specific version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
     */
    @java.lang.Override
    public boolean hasSpecific() {
      return policyChoiceCase_ == 3;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Specific specific
     *&#64;&#64;
     *&#64;&#64;       Serve only specific version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelVersionPolicy.Specific getSpecific() {
      if (policyChoiceCase_ == 3) {
         return (inference.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_;
      }
      return inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Specific specific
     *&#64;&#64;
     *&#64;&#64;       Serve only specific version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
     */
    private void setSpecific(inference.ModelConfigOuterClass.ModelVersionPolicy.Specific value) {
      value.getClass();
  policyChoice_ = value;
      policyChoiceCase_ = 3;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Specific specific
     *&#64;&#64;
     *&#64;&#64;       Serve only specific version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
     */
    private void mergeSpecific(inference.ModelConfigOuterClass.ModelVersionPolicy.Specific value) {
      value.getClass();
  if (policyChoiceCase_ == 3 &&
          policyChoice_ != inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance()) {
        policyChoice_ = inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.newBuilder((inference.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_)
            .mergeFrom(value).buildPartial();
      } else {
        policyChoice_ = value;
      }
      policyChoiceCase_ = 3;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Specific specific
     *&#64;&#64;
     *&#64;&#64;       Serve only specific version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
     */
    private void clearSpecific() {
      if (policyChoiceCase_ == 3) {
        policyChoiceCase_ = 0;
        policyChoice_ = null;
      }
    }

    public static inference.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelVersionPolicy parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelVersionPolicy parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelVersionPolicy parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelVersionPolicy parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }

    public static Builder newBuilder() {
      return (Builder) DEFAULT_INSTANCE.createBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelVersionPolicy prototype) {
      return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelVersionPolicy
     *&#64;&#64;
     *&#64;&#64;   Policy indicating which versions of a model should be made
     *&#64;&#64;   available by the inference server.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelVersionPolicy}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageLite.Builder<
          inference.ModelConfigOuterClass.ModelVersionPolicy, Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelVersionPolicy)
        inference.ModelConfigOuterClass.ModelVersionPolicyOrBuilder {
      // Construct using inference.ModelConfigOuterClass.ModelVersionPolicy.newBuilder()
      private Builder() {
        super(DEFAULT_INSTANCE);
      }

      @java.lang.Override
      public PolicyChoiceCase
          getPolicyChoiceCase() {
        return instance.getPolicyChoiceCase();
      }

      public Builder clearPolicyChoice() {
        copyOnWrite();
        instance.clearPolicyChoice();
        return this;
      }


      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
       */
      @java.lang.Override
      public boolean hasLatest() {
        return instance.hasLatest();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelVersionPolicy.Latest getLatest() {
        return instance.getLatest();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
       */
      public Builder setLatest(inference.ModelConfigOuterClass.ModelVersionPolicy.Latest value) {
        copyOnWrite();
        instance.setLatest(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
       */
      public Builder setLatest(
          inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.Builder builderForValue) {
        copyOnWrite();
        instance.setLatest(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
       */
      public Builder mergeLatest(inference.ModelConfigOuterClass.ModelVersionPolicy.Latest value) {
        copyOnWrite();
        instance.mergeLatest(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
       */
      public Builder clearLatest() {
        copyOnWrite();
        instance.clearLatest();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.All all = 2;</code>
       */
      @java.lang.Override
      public boolean hasAll() {
        return instance.hasAll();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.All all = 2;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelVersionPolicy.All getAll() {
        return instance.getAll();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.All all = 2;</code>
       */
      public Builder setAll(inference.ModelConfigOuterClass.ModelVersionPolicy.All value) {
        copyOnWrite();
        instance.setAll(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.All all = 2;</code>
       */
      public Builder setAll(
          inference.ModelConfigOuterClass.ModelVersionPolicy.All.Builder builderForValue) {
        copyOnWrite();
        instance.setAll(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.All all = 2;</code>
       */
      public Builder mergeAll(inference.ModelConfigOuterClass.ModelVersionPolicy.All value) {
        copyOnWrite();
        instance.mergeAll(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.All all = 2;</code>
       */
      public Builder clearAll() {
        copyOnWrite();
        instance.clearAll();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
       */
      @java.lang.Override
      public boolean hasSpecific() {
        return instance.hasSpecific();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelVersionPolicy.Specific getSpecific() {
        return instance.getSpecific();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
       */
      public Builder setSpecific(inference.ModelConfigOuterClass.ModelVersionPolicy.Specific value) {
        copyOnWrite();
        instance.setSpecific(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
       */
      public Builder setSpecific(
          inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.Builder builderForValue) {
        copyOnWrite();
        instance.setSpecific(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
       */
      public Builder mergeSpecific(inference.ModelConfigOuterClass.ModelVersionPolicy.Specific value) {
        copyOnWrite();
        instance.mergeSpecific(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
       */
      public Builder clearSpecific() {
        copyOnWrite();
        instance.clearSpecific();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:inference.ModelVersionPolicy)
    }
    @java.lang.Override
    @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
    protected final java.lang.Object dynamicMethod(
        com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
        java.lang.Object arg0, java.lang.Object arg1) {
      switch (method) {
        case NEW_MUTABLE_INSTANCE: {
          return new inference.ModelConfigOuterClass.ModelVersionPolicy();
        }
        case NEW_BUILDER: {
          return new Builder();
        }
        case BUILD_MESSAGE_INFO: {
            java.lang.Object[] objects = new java.lang.Object[] {
              "policyChoice_",
              "policyChoiceCase_",
              inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.class,
              inference.ModelConfigOuterClass.ModelVersionPolicy.All.class,
              inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.class,
            };
            java.lang.String info =
                "\u0000\u0003\u0001\u0000\u0001\u0003\u0003\u0000\u0000\u0000\u0001<\u0000\u0002<" +
                "\u0000\u0003<\u0000";
            return newMessageInfo(DEFAULT_INSTANCE, info, objects);
        }
        // fall through
        case GET_DEFAULT_INSTANCE: {
          return DEFAULT_INSTANCE;
        }
        case GET_PARSER: {
          com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelVersionPolicy> parser = PARSER;
          if (parser == null) {
            synchronized (inference.ModelConfigOuterClass.ModelVersionPolicy.class) {
              parser = PARSER;
              if (parser == null) {
                parser =
                    new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelVersionPolicy>(
                        DEFAULT_INSTANCE);
                PARSER = parser;
              }
            }
          }
          return parser;
      }
      case GET_MEMOIZED_IS_INITIALIZED: {
        return (byte) 1;
      }
      case SET_MEMOIZED_IS_INITIALIZED: {
        return null;
      }
      }
      throw new UnsupportedOperationException();
    }


    // @@protoc_insertion_point(class_scope:inference.ModelVersionPolicy)
    private static final inference.ModelConfigOuterClass.ModelVersionPolicy DEFAULT_INSTANCE;
    static {
      ModelVersionPolicy defaultInstance = new ModelVersionPolicy();
      // New instances are implicitly immutable so no need to make
      // immutable.
      DEFAULT_INSTANCE = defaultInstance;
      com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
        ModelVersionPolicy.class, defaultInstance);
    }

    public static inference.ModelConfigOuterClass.ModelVersionPolicy getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static volatile com.google.protobuf.Parser<ModelVersionPolicy> PARSER;

    public static com.google.protobuf.Parser<ModelVersionPolicy> parser() {
      return DEFAULT_INSTANCE.getParserForType();
    }
  }

  public interface ModelOptimizationPolicyOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelOptimizationPolicy)
      com.google.protobuf.MessageLiteOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
     * @return Whether the graph field is set.
     */
    boolean hasGraph();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
     * @return The graph.
     */
    inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph getGraph();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelPriority priority
     *&#64;&#64;
     *&#64;&#64;     The priority setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
     * @return The enum numeric value on the wire for priority.
     */
    int getPriorityValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelPriority priority
     *&#64;&#64;
     *&#64;&#64;     The priority setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
     * @return The priority.
     */
    inference.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority getPriority();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     * @return Whether the cuda field is set.
     */
    boolean hasCuda();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     * @return The cuda.
     */
    inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda getCuda();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *&#64;&#64;
     *&#64;&#64;     The accelerators used for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
     * @return Whether the executionAccelerators field is set.
     */
    boolean hasExecutionAccelerators();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *&#64;&#64;
     *&#64;&#64;     The accelerators used for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
     * @return The executionAccelerators.
     */
    inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators getExecutionAccelerators();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
     * @return Whether the inputPinnedMemory field is set.
     */
    boolean hasInputPinnedMemory();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
     * @return The inputPinnedMemory.
     */
    inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer getInputPinnedMemory();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
     * @return Whether the outputPinnedMemory field is set.
     */
    boolean hasOutputPinnedMemory();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
     * @return The outputPinnedMemory.
     */
    inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer getOutputPinnedMemory();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 gather_kernel_buffer_threshold
     *&#64;&#64;
     *&#64;&#64;     The backend may use a gather kernel to gather input data if the
     *&#64;&#64;     device has direct access to the source buffer and the destination
     *&#64;&#64;     buffer. In such case, the gather kernel will be used only if the
     *&#64;&#64;     number of buffers to be gathered is greater or equal to
     *&#64;&#64;     the specifed value. If 0, the gather kernel will be disabled.
     *&#64;&#64;     Default value is 0.
     *&#64;&#64;     Currently only recognized by TensorRT backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 gather_kernel_buffer_threshold = 7;</code>
     * @return The gatherKernelBufferThreshold.
     */
    int getGatherKernelBufferThreshold();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool eager_batching
     *&#64;&#64;
     *&#64;&#64;     Start preparing the next batch before the model instance is ready
     *&#64;&#64;     for the next inference. This option can be used to overlap the
     *&#64;&#64;     batch preparation with model execution, with the trade-off that
     *&#64;&#64;     the next batch might be smaller than what it could have been.
     *&#64;&#64;     Default value is false.
     *&#64;&#64;     Currently only recognized by TensorRT backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool eager_batching = 8;</code>
     * @return The eagerBatching.
     */
    boolean getEagerBatching();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelOptimizationPolicy
   *&#64;&#64;
   *&#64;&#64;   Optimization settings for a model. These settings control if/how a
   *&#64;&#64;   model is optimized and prioritized by the backend framework when
   *&#64;&#64;   it is loaded.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelOptimizationPolicy}
   */
  public  static final class ModelOptimizationPolicy extends
      com.google.protobuf.GeneratedMessageLite<
          ModelOptimizationPolicy, ModelOptimizationPolicy.Builder> implements
      // @@protoc_insertion_point(message_implements:inference.ModelOptimizationPolicy)
      ModelOptimizationPolicyOrBuilder {
    private ModelOptimizationPolicy() {
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:enum:: ModelPriority
     *&#64;&#64;
     *&#64;&#64;     Model priorities. A model will be given scheduling and execution
     *&#64;&#64;     preference over models at lower priorities. Current model
     *&#64;&#64;     priorities only work for TensorRT models.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf enum {@code inference.ModelOptimizationPolicy.ModelPriority}
     */
    public enum ModelPriority
        implements com.google.protobuf.Internal.EnumLite {
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: ModelPriority::PRIORITY_DEFAULT = 0
       *&#64;&#64;
       *&#64;&#64;       The default model priority.
       *&#64;&#64;
       * </pre>
       *
       * <code>PRIORITY_DEFAULT = 0;</code>
       */
      PRIORITY_DEFAULT(0),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: ModelPriority::PRIORITY_MAX = 1
       *&#64;&#64;
       *&#64;&#64;       The maximum model priority.
       *&#64;&#64;
       * </pre>
       *
       * <code>PRIORITY_MAX = 1;</code>
       */
      PRIORITY_MAX(1),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: ModelPriority::PRIORITY_MIN = 2
       *&#64;&#64;
       *&#64;&#64;       The minimum model priority.
       *&#64;&#64;
       * </pre>
       *
       * <code>PRIORITY_MIN = 2;</code>
       */
      PRIORITY_MIN(2),
      UNRECOGNIZED(-1),
      ;

      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: ModelPriority::PRIORITY_DEFAULT = 0
       *&#64;&#64;
       *&#64;&#64;       The default model priority.
       *&#64;&#64;
       * </pre>
       *
       * <code>PRIORITY_DEFAULT = 0;</code>
       */
      public static final int PRIORITY_DEFAULT_VALUE = 0;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: ModelPriority::PRIORITY_MAX = 1
       *&#64;&#64;
       *&#64;&#64;       The maximum model priority.
       *&#64;&#64;
       * </pre>
       *
       * <code>PRIORITY_MAX = 1;</code>
       */
      public static final int PRIORITY_MAX_VALUE = 1;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: ModelPriority::PRIORITY_MIN = 2
       *&#64;&#64;
       *&#64;&#64;       The minimum model priority.
       *&#64;&#64;
       * </pre>
       *
       * <code>PRIORITY_MIN = 2;</code>
       */
      public static final int PRIORITY_MIN_VALUE = 2;


      @java.lang.Override
      public final int getNumber() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalArgumentException(
              "Can't get the number of an unknown enum value.");
        }
        return value;
      }

      /**
       * @param value The number of the enum to look for.
       * @return The enum associated with the given number.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static ModelPriority valueOf(int value) {
        return forNumber(value);
      }

      public static ModelPriority forNumber(int value) {
        switch (value) {
          case 0: return PRIORITY_DEFAULT;
          case 1: return PRIORITY_MAX;
          case 2: return PRIORITY_MIN;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<ModelPriority>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          ModelPriority> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<ModelPriority>() {
              @java.lang.Override
              public ModelPriority findValueByNumber(int number) {
                return ModelPriority.forNumber(number);
              }
            };

      public static com.google.protobuf.Internal.EnumVerifier 
          internalGetVerifier() {
        return ModelPriorityVerifier.INSTANCE;
      }

      private static final class ModelPriorityVerifier implements 
           com.google.protobuf.Internal.EnumVerifier { 
              static final com.google.protobuf.Internal.EnumVerifier           INSTANCE = new ModelPriorityVerifier();
              @java.lang.Override
              public boolean isInRange(int number) {
                return ModelPriority.forNumber(number) != null;
              }
            };

      private final int value;

      private ModelPriority(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:inference.ModelOptimizationPolicy.ModelPriority)
    }

    public interface GraphOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelOptimizationPolicy.Graph)
        com.google.protobuf.MessageLiteOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 level
       *&#64;&#64;
       *&#64;&#64;       The optimization level. Defaults to 0 (zero) if not specified.
       *&#64;&#64;
       *&#64;&#64;         - -1: Disabled
       *&#64;&#64;         -  0: Framework default
       *&#64;&#64;         -  1+: Enable optimization level (greater values indicate
       *&#64;&#64;            higher optimization levels)
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 level = 1;</code>
       * @return The level.
       */
      int getLevel();
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: message Graph
     *&#64;&#64;
     *&#64;&#64;     Enable generic graph optimization of the model. If not specified
     *&#64;&#64;     the framework's default level of optimization is used. Supports
     *&#64;&#64;     TensorFlow graphdef and savedmodel and Onnx models. For TensorFlow
     *&#64;&#64;     causes XLA to be enabled/disabled for the model. For Onnx defaults
     *&#64;&#64;     to enabling all optimizations, -1 enables only basic optimizations,
     *&#64;&#64;     +1 enables only basic and extended optimizations.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelOptimizationPolicy.Graph}
     */
    public  static final class Graph extends
        com.google.protobuf.GeneratedMessageLite<
            Graph, Graph.Builder> implements
        // @@protoc_insertion_point(message_implements:inference.ModelOptimizationPolicy.Graph)
        GraphOrBuilder {
      private Graph() {
      }
      public static final int LEVEL_FIELD_NUMBER = 1;
      private int level_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 level
       *&#64;&#64;
       *&#64;&#64;       The optimization level. Defaults to 0 (zero) if not specified.
       *&#64;&#64;
       *&#64;&#64;         - -1: Disabled
       *&#64;&#64;         -  0: Framework default
       *&#64;&#64;         -  1+: Enable optimization level (greater values indicate
       *&#64;&#64;            higher optimization levels)
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 level = 1;</code>
       * @return The level.
       */
      @java.lang.Override
      public int getLevel() {
        return level_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 level
       *&#64;&#64;
       *&#64;&#64;       The optimization level. Defaults to 0 (zero) if not specified.
       *&#64;&#64;
       *&#64;&#64;         - -1: Disabled
       *&#64;&#64;         -  0: Framework default
       *&#64;&#64;         -  1+: Enable optimization level (greater values indicate
       *&#64;&#64;            higher optimization levels)
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 level = 1;</code>
       * @param value The level to set.
       */
      private void setLevel(int value) {
        
        level_ = value;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 level
       *&#64;&#64;
       *&#64;&#64;       The optimization level. Defaults to 0 (zero) if not specified.
       *&#64;&#64;
       *&#64;&#64;         - -1: Disabled
       *&#64;&#64;         -  0: Framework default
       *&#64;&#64;         -  1+: Enable optimization level (greater values indicate
       *&#64;&#64;            higher optimization levels)
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 level = 1;</code>
       */
      private void clearLevel() {
        
        level_ = 0;
      }

      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }

      public static Builder newBuilder() {
        return (Builder) DEFAULT_INSTANCE.createBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph prototype) {
        return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
      }

      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: message Graph
       *&#64;&#64;
       *&#64;&#64;     Enable generic graph optimization of the model. If not specified
       *&#64;&#64;     the framework's default level of optimization is used. Supports
       *&#64;&#64;     TensorFlow graphdef and savedmodel and Onnx models. For TensorFlow
       *&#64;&#64;     causes XLA to be enabled/disabled for the model. For Onnx defaults
       *&#64;&#64;     to enabling all optimizations, -1 enables only basic optimizations,
       *&#64;&#64;     +1 enables only basic and extended optimizations.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelOptimizationPolicy.Graph}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageLite.Builder<
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph, Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelOptimizationPolicy.Graph)
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.GraphOrBuilder {
        // Construct using inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.newBuilder()
        private Builder() {
          super(DEFAULT_INSTANCE);
        }


        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 level
         *&#64;&#64;
         *&#64;&#64;       The optimization level. Defaults to 0 (zero) if not specified.
         *&#64;&#64;
         *&#64;&#64;         - -1: Disabled
         *&#64;&#64;         -  0: Framework default
         *&#64;&#64;         -  1+: Enable optimization level (greater values indicate
         *&#64;&#64;            higher optimization levels)
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 level = 1;</code>
         * @return The level.
         */
        @java.lang.Override
        public int getLevel() {
          return instance.getLevel();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 level
         *&#64;&#64;
         *&#64;&#64;       The optimization level. Defaults to 0 (zero) if not specified.
         *&#64;&#64;
         *&#64;&#64;         - -1: Disabled
         *&#64;&#64;         -  0: Framework default
         *&#64;&#64;         -  1+: Enable optimization level (greater values indicate
         *&#64;&#64;            higher optimization levels)
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 level = 1;</code>
         * @param value The level to set.
         * @return This builder for chaining.
         */
        public Builder setLevel(int value) {
          copyOnWrite();
          instance.setLevel(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 level
         *&#64;&#64;
         *&#64;&#64;       The optimization level. Defaults to 0 (zero) if not specified.
         *&#64;&#64;
         *&#64;&#64;         - -1: Disabled
         *&#64;&#64;         -  0: Framework default
         *&#64;&#64;         -  1+: Enable optimization level (greater values indicate
         *&#64;&#64;            higher optimization levels)
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 level = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearLevel() {
          copyOnWrite();
          instance.clearLevel();
          return this;
        }

        // @@protoc_insertion_point(builder_scope:inference.ModelOptimizationPolicy.Graph)
      }
      @java.lang.Override
      @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
      protected final java.lang.Object dynamicMethod(
          com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
          java.lang.Object arg0, java.lang.Object arg1) {
        switch (method) {
          case NEW_MUTABLE_INSTANCE: {
            return new inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph();
          }
          case NEW_BUILDER: {
            return new Builder();
          }
          case BUILD_MESSAGE_INFO: {
              java.lang.Object[] objects = new java.lang.Object[] {
                "level_",
              };
              java.lang.String info =
                  "\u0000\u0001\u0000\u0000\u0001\u0001\u0001\u0000\u0000\u0000\u0001\u0004";
              return newMessageInfo(DEFAULT_INSTANCE, info, objects);
          }
          // fall through
          case GET_DEFAULT_INSTANCE: {
            return DEFAULT_INSTANCE;
          }
          case GET_PARSER: {
            com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph> parser = PARSER;
            if (parser == null) {
              synchronized (inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.class) {
                parser = PARSER;
                if (parser == null) {
                  parser =
                      new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph>(
                          DEFAULT_INSTANCE);
                  PARSER = parser;
                }
              }
            }
            return parser;
        }
        case GET_MEMOIZED_IS_INITIALIZED: {
          return (byte) 1;
        }
        case SET_MEMOIZED_IS_INITIALIZED: {
          return null;
        }
        }
        throw new UnsupportedOperationException();
      }


      // @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.Graph)
      private static final inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph DEFAULT_INSTANCE;
      static {
        Graph defaultInstance = new Graph();
        // New instances are implicitly immutable so no need to make
        // immutable.
        DEFAULT_INSTANCE = defaultInstance;
        com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
          Graph.class, defaultInstance);
      }

      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static volatile com.google.protobuf.Parser<Graph> PARSER;

      public static com.google.protobuf.Parser<Graph> parser() {
        return DEFAULT_INSTANCE.getParserForType();
      }
    }

    public interface CudaOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelOptimizationPolicy.Cuda)
        com.google.protobuf.MessageLiteOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool graphs
       *&#64;&#64;
       *&#64;&#64;       Use CUDA graphs API to capture model operations and execute
       *&#64;&#64;       them more efficiently. Default value is false.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool graphs = 1;</code>
       * @return The graphs.
       */
      boolean getGraphs();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool busy_wait_events
       *&#64;&#64;
       *&#64;&#64;       Use busy-waiting to synchronize CUDA events to achieve minimum
       *&#64;&#64;       latency from event complete to host thread to be notified, with
       *&#64;&#64;       the cost of high CPU load. Default value is false.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool busy_wait_events = 2;</code>
       * @return The busyWaitEvents.
       */
      boolean getBusyWaitEvents();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
       *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
       *&#64;&#64;       based on model settings.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
       */
      java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec> 
          getGraphSpecList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
       *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
       *&#64;&#64;       based on model settings.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
       */
      inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec getGraphSpec(int index);
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
       *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
       *&#64;&#64;       based on model settings.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
       */
      int getGraphSpecCount();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool output_copy_stream
       *&#64;&#64;
       *&#64;&#64;       Uses a CUDA stream separate from the inference stream to copy the
       *&#64;&#64;       output to host. Default value is false.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool output_copy_stream = 4;</code>
       * @return The outputCopyStream.
       */
      boolean getOutputCopyStream();
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: message Cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelOptimizationPolicy.Cuda}
     */
    public  static final class Cuda extends
        com.google.protobuf.GeneratedMessageLite<
            Cuda, Cuda.Builder> implements
        // @@protoc_insertion_point(message_implements:inference.ModelOptimizationPolicy.Cuda)
        CudaOrBuilder {
      private Cuda() {
        graphSpec_ = emptyProtobufList();
      }
      public interface GraphSpecOrBuilder extends
          // @@protoc_insertion_point(interface_extends:inference.ModelOptimizationPolicy.Cuda.GraphSpec)
          com.google.protobuf.MessageLiteOrBuilder {

        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: int32 batch_size
         *&#64;&#64;
         *&#64;&#64;         The batch size of the CUDA graph. If 'max_batch_size' is 0,
         *&#64;&#64;         'batch_size' must be set to 0. Otherwise, 'batch_size' must
         *&#64;&#64;         be set to value between 1 and 'max_batch_size'.
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 batch_size = 1;</code>
         * @return The batchSize.
         */
        int getBatchSize();

        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
         *&#64;&#64;
         *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
         *&#64;&#64;         input without batching dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
         */
        int getInputCount();
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
         *&#64;&#64;
         *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
         *&#64;&#64;         input without batching dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
         */
        boolean containsInput(
            java.lang.String key);
        /**
         * Use {@link #getInputMap()} instead.
         */
        @java.lang.Deprecated
        java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
        getInput();
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
         *&#64;&#64;
         *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
         *&#64;&#64;         input without batching dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
         */
        java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
        getInputMap();
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
         *&#64;&#64;
         *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
         *&#64;&#64;         input without batching dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
         */

        inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getInputOrDefault(
            java.lang.String key,
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape defaultValue);
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
         *&#64;&#64;
         *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
         *&#64;&#64;         input without batching dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
         */

        inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getInputOrThrow(
            java.lang.String key);

        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
         *&#64;&#64;
         *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
         *&#64;&#64;         If specified, the graph can be used for input shapes and
         *&#64;&#64;         batch sizes that are in closed interval between the lower
         *&#64;&#64;         bound specification and graph specification. For dynamic
         *&#64;&#64;         shape model, this allows CUDA graphs to be launched
         *&#64;&#64;         frequently without capturing all possible shape combinations.
         *&#64;&#64;         However, using graph for shape combinations different from
         *&#64;&#64;         the one used for capturing introduces uninitialized data for
         *&#64;&#64;         execution and it may distort the inference result if
         *&#64;&#64;         the model is sensitive to uninitialized data.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
         * @return Whether the graphLowerBound field is set.
         */
        boolean hasGraphLowerBound();
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
         *&#64;&#64;
         *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
         *&#64;&#64;         If specified, the graph can be used for input shapes and
         *&#64;&#64;         batch sizes that are in closed interval between the lower
         *&#64;&#64;         bound specification and graph specification. For dynamic
         *&#64;&#64;         shape model, this allows CUDA graphs to be launched
         *&#64;&#64;         frequently without capturing all possible shape combinations.
         *&#64;&#64;         However, using graph for shape combinations different from
         *&#64;&#64;         the one used for capturing introduces uninitialized data for
         *&#64;&#64;         execution and it may distort the inference result if
         *&#64;&#64;         the model is sensitive to uninitialized data.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
         * @return The graphLowerBound.
         */
        inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound getGraphLowerBound();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: message GraphSpec
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelOptimizationPolicy.Cuda.GraphSpec}
       */
      public  static final class GraphSpec extends
          com.google.protobuf.GeneratedMessageLite<
              GraphSpec, GraphSpec.Builder> implements
          // @@protoc_insertion_point(message_implements:inference.ModelOptimizationPolicy.Cuda.GraphSpec)
          GraphSpecOrBuilder {
        private GraphSpec() {
        }
        public interface ShapeOrBuilder extends
            // @@protoc_insertion_point(interface_extends:inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape)
            com.google.protobuf.MessageLiteOrBuilder {

          /**
           * <pre>
           *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
           *&#64;&#64;
           *&#64;&#64;           The dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated int64 dim = 1;</code>
           * @return A list containing the dim.
           */
          java.util.List<java.lang.Long> getDimList();
          /**
           * <pre>
           *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
           *&#64;&#64;
           *&#64;&#64;           The dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated int64 dim = 1;</code>
           * @return The count of dim.
           */
          int getDimCount();
          /**
           * <pre>
           *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
           *&#64;&#64;
           *&#64;&#64;           The dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated int64 dim = 1;</code>
           * @param index The index of the element to return.
           * @return The dim at the given index.
           */
          long getDim(int index);
        }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: message Dims
         *&#64;&#64;
         *&#64;&#64;         Specification of tensor dimension.
         *&#64;&#64;
         * </pre>
         *
         * Protobuf type {@code inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape}
         */
        public  static final class Shape extends
            com.google.protobuf.GeneratedMessageLite<
                Shape, Shape.Builder> implements
            // @@protoc_insertion_point(message_implements:inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape)
            ShapeOrBuilder {
          private Shape() {
            dim_ = emptyLongList();
          }
          public static final int DIM_FIELD_NUMBER = 1;
          private com.google.protobuf.Internal.LongList dim_;
          /**
           * <pre>
           *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
           *&#64;&#64;
           *&#64;&#64;           The dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated int64 dim = 1;</code>
           * @return A list containing the dim.
           */
          @java.lang.Override
          public java.util.List<java.lang.Long>
              getDimList() {
            return dim_;
          }
          /**
           * <pre>
           *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
           *&#64;&#64;
           *&#64;&#64;           The dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated int64 dim = 1;</code>
           * @return The count of dim.
           */
          @java.lang.Override
          public int getDimCount() {
            return dim_.size();
          }
          /**
           * <pre>
           *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
           *&#64;&#64;
           *&#64;&#64;           The dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated int64 dim = 1;</code>
           * @param index The index of the element to return.
           * @return The dim at the given index.
           */
          @java.lang.Override
          public long getDim(int index) {
            return dim_.getLong(index);
          }
          private int dimMemoizedSerializedSize = -1;
          private void ensureDimIsMutable() {
            com.google.protobuf.Internal.LongList tmp = dim_;
            if (!tmp.isModifiable()) {
              dim_ =
                  com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
             }
          }
          /**
           * <pre>
           *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
           *&#64;&#64;
           *&#64;&#64;           The dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated int64 dim = 1;</code>
           * @param index The index to set the value at.
           * @param value The dim to set.
           */
          private void setDim(
              int index, long value) {
            ensureDimIsMutable();
            dim_.setLong(index, value);
          }
          /**
           * <pre>
           *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
           *&#64;&#64;
           *&#64;&#64;           The dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated int64 dim = 1;</code>
           * @param value The dim to add.
           */
          private void addDim(long value) {
            ensureDimIsMutable();
            dim_.addLong(value);
          }
          /**
           * <pre>
           *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
           *&#64;&#64;
           *&#64;&#64;           The dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated int64 dim = 1;</code>
           * @param values The dim to add.
           */
          private void addAllDim(
              java.lang.Iterable<? extends java.lang.Long> values) {
            ensureDimIsMutable();
            com.google.protobuf.AbstractMessageLite.addAll(
                values, dim_);
          }
          /**
           * <pre>
           *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
           *&#64;&#64;
           *&#64;&#64;           The dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated int64 dim = 1;</code>
           */
          private void clearDim() {
            dim_ = emptyLongList();
          }

          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape parseFrom(
              java.nio.ByteBuffer data)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return com.google.protobuf.GeneratedMessageLite.parseFrom(
                DEFAULT_INSTANCE, data);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape parseFrom(
              java.nio.ByteBuffer data,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return com.google.protobuf.GeneratedMessageLite.parseFrom(
                DEFAULT_INSTANCE, data, extensionRegistry);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape parseFrom(
              com.google.protobuf.ByteString data)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return com.google.protobuf.GeneratedMessageLite.parseFrom(
                DEFAULT_INSTANCE, data);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape parseFrom(
              com.google.protobuf.ByteString data,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return com.google.protobuf.GeneratedMessageLite.parseFrom(
                DEFAULT_INSTANCE, data, extensionRegistry);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape parseFrom(byte[] data)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return com.google.protobuf.GeneratedMessageLite.parseFrom(
                DEFAULT_INSTANCE, data);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape parseFrom(
              byte[] data,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return com.google.protobuf.GeneratedMessageLite.parseFrom(
                DEFAULT_INSTANCE, data, extensionRegistry);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape parseFrom(java.io.InputStream input)
              throws java.io.IOException {
            return com.google.protobuf.GeneratedMessageLite.parseFrom(
                DEFAULT_INSTANCE, input);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape parseFrom(
              java.io.InputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws java.io.IOException {
            return com.google.protobuf.GeneratedMessageLite.parseFrom(
                DEFAULT_INSTANCE, input, extensionRegistry);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape parseDelimitedFrom(java.io.InputStream input)
              throws java.io.IOException {
            return parseDelimitedFrom(DEFAULT_INSTANCE, input);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape parseDelimitedFrom(
              java.io.InputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws java.io.IOException {
            return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape parseFrom(
              com.google.protobuf.CodedInputStream input)
              throws java.io.IOException {
            return com.google.protobuf.GeneratedMessageLite.parseFrom(
                DEFAULT_INSTANCE, input);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape parseFrom(
              com.google.protobuf.CodedInputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws java.io.IOException {
            return com.google.protobuf.GeneratedMessageLite.parseFrom(
                DEFAULT_INSTANCE, input, extensionRegistry);
          }

          public static Builder newBuilder() {
            return (Builder) DEFAULT_INSTANCE.createBuilder();
          }
          public static Builder newBuilder(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape prototype) {
            return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
          }

          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: message Dims
           *&#64;&#64;
           *&#64;&#64;         Specification of tensor dimension.
           *&#64;&#64;
           * </pre>
           *
           * Protobuf type {@code inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape}
           */
          public static final class Builder extends
              com.google.protobuf.GeneratedMessageLite.Builder<
                inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape, Builder> implements
              // @@protoc_insertion_point(builder_implements:inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape)
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.ShapeOrBuilder {
            // Construct using inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape.newBuilder()
            private Builder() {
              super(DEFAULT_INSTANCE);
            }


            /**
             * <pre>
             *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
             *&#64;&#64;
             *&#64;&#64;           The dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>repeated int64 dim = 1;</code>
             * @return A list containing the dim.
             */
            @java.lang.Override
            public java.util.List<java.lang.Long>
                getDimList() {
              return java.util.Collections.unmodifiableList(
                  instance.getDimList());
            }
            /**
             * <pre>
             *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
             *&#64;&#64;
             *&#64;&#64;           The dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>repeated int64 dim = 1;</code>
             * @return The count of dim.
             */
            @java.lang.Override
            public int getDimCount() {
              return instance.getDimCount();
            }
            /**
             * <pre>
             *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
             *&#64;&#64;
             *&#64;&#64;           The dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>repeated int64 dim = 1;</code>
             * @param index The index of the element to return.
             * @return The dim at the given index.
             */
            @java.lang.Override
            public long getDim(int index) {
              return instance.getDim(index);
            }
            /**
             * <pre>
             *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
             *&#64;&#64;
             *&#64;&#64;           The dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>repeated int64 dim = 1;</code>
             * @param value The dim to set.
             * @return This builder for chaining.
             */
            public Builder setDim(
                int index, long value) {
              copyOnWrite();
              instance.setDim(index, value);
              return this;
            }
            /**
             * <pre>
             *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
             *&#64;&#64;
             *&#64;&#64;           The dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>repeated int64 dim = 1;</code>
             * @param value The dim to add.
             * @return This builder for chaining.
             */
            public Builder addDim(long value) {
              copyOnWrite();
              instance.addDim(value);
              return this;
            }
            /**
             * <pre>
             *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
             *&#64;&#64;
             *&#64;&#64;           The dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>repeated int64 dim = 1;</code>
             * @param values The dim to add.
             * @return This builder for chaining.
             */
            public Builder addAllDim(
                java.lang.Iterable<? extends java.lang.Long> values) {
              copyOnWrite();
              instance.addAllDim(values);
              return this;
            }
            /**
             * <pre>
             *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
             *&#64;&#64;
             *&#64;&#64;           The dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>repeated int64 dim = 1;</code>
             * @return This builder for chaining.
             */
            public Builder clearDim() {
              copyOnWrite();
              instance.clearDim();
              return this;
            }

            // @@protoc_insertion_point(builder_scope:inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape)
          }
          @java.lang.Override
          @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
          protected final java.lang.Object dynamicMethod(
              com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
              java.lang.Object arg0, java.lang.Object arg1) {
            switch (method) {
              case NEW_MUTABLE_INSTANCE: {
                return new inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape();
              }
              case NEW_BUILDER: {
                return new Builder();
              }
              case BUILD_MESSAGE_INFO: {
                  java.lang.Object[] objects = new java.lang.Object[] {
                    "dim_",
                  };
                  java.lang.String info =
                      "\u0000\u0001\u0000\u0000\u0001\u0001\u0001\u0000\u0001\u0000\u0001%";
                  return newMessageInfo(DEFAULT_INSTANCE, info, objects);
              }
              // fall through
              case GET_DEFAULT_INSTANCE: {
                return DEFAULT_INSTANCE;
              }
              case GET_PARSER: {
                com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> parser = PARSER;
                if (parser == null) {
                  synchronized (inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape.class) {
                    parser = PARSER;
                    if (parser == null) {
                      parser =
                          new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>(
                              DEFAULT_INSTANCE);
                      PARSER = parser;
                    }
                  }
                }
                return parser;
            }
            case GET_MEMOIZED_IS_INITIALIZED: {
              return (byte) 1;
            }
            case SET_MEMOIZED_IS_INITIALIZED: {
              return null;
            }
            }
            throw new UnsupportedOperationException();
          }


          // @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape)
          private static final inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape DEFAULT_INSTANCE;
          static {
            Shape defaultInstance = new Shape();
            // New instances are implicitly immutable so no need to make
            // immutable.
            DEFAULT_INSTANCE = defaultInstance;
            com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
              Shape.class, defaultInstance);
          }

          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getDefaultInstance() {
            return DEFAULT_INSTANCE;
          }

          private static volatile com.google.protobuf.Parser<Shape> PARSER;

          public static com.google.protobuf.Parser<Shape> parser() {
            return DEFAULT_INSTANCE.getParserForType();
          }
        }

        public interface LowerBoundOrBuilder extends
            // @@protoc_insertion_point(interface_extends:inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound)
            com.google.protobuf.MessageLiteOrBuilder {

          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: int32 batch_size
           *&#64;&#64;
           *&#64;&#64;         The batch size of the CUDA graph. If 'max_batch_size' is 0,
           *&#64;&#64;         'batch_size' must be set to 0. Otherwise, 'batch_size' must
           *&#64;&#64;         be set to value between 1 and 'max_batch_size'.
           *&#64;&#64;
           * </pre>
           *
           * <code>int32 batch_size = 1;</code>
           * @return The batchSize.
           */
          int getBatchSize();

          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
           *&#64;&#64;         the input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */
          int getInputCount();
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
           *&#64;&#64;         the input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */
          boolean containsInput(
              java.lang.String key);
          /**
           * Use {@link #getInputMap()} instead.
           */
          @java.lang.Deprecated
          java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
          getInput();
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
           *&#64;&#64;         the input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */
          java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
          getInputMap();
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
           *&#64;&#64;         the input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */

          inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getInputOrDefault(
              java.lang.String key,
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape defaultValue);
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
           *&#64;&#64;         the input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */

          inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getInputOrThrow(
              java.lang.String key);
        }
        /**
         * Protobuf type {@code inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound}
         */
        public  static final class LowerBound extends
            com.google.protobuf.GeneratedMessageLite<
                LowerBound, LowerBound.Builder> implements
            // @@protoc_insertion_point(message_implements:inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound)
            LowerBoundOrBuilder {
          private LowerBound() {
          }
          public static final int BATCH_SIZE_FIELD_NUMBER = 1;
          private int batchSize_;
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: int32 batch_size
           *&#64;&#64;
           *&#64;&#64;         The batch size of the CUDA graph. If 'max_batch_size' is 0,
           *&#64;&#64;         'batch_size' must be set to 0. Otherwise, 'batch_size' must
           *&#64;&#64;         be set to value between 1 and 'max_batch_size'.
           *&#64;&#64;
           * </pre>
           *
           * <code>int32 batch_size = 1;</code>
           * @return The batchSize.
           */
          @java.lang.Override
          public int getBatchSize() {
            return batchSize_;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: int32 batch_size
           *&#64;&#64;
           *&#64;&#64;         The batch size of the CUDA graph. If 'max_batch_size' is 0,
           *&#64;&#64;         'batch_size' must be set to 0. Otherwise, 'batch_size' must
           *&#64;&#64;         be set to value between 1 and 'max_batch_size'.
           *&#64;&#64;
           * </pre>
           *
           * <code>int32 batch_size = 1;</code>
           * @param value The batchSize to set.
           */
          private void setBatchSize(int value) {
            
            batchSize_ = value;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: int32 batch_size
           *&#64;&#64;
           *&#64;&#64;         The batch size of the CUDA graph. If 'max_batch_size' is 0,
           *&#64;&#64;         'batch_size' must be set to 0. Otherwise, 'batch_size' must
           *&#64;&#64;         be set to value between 1 and 'max_batch_size'.
           *&#64;&#64;
           * </pre>
           *
           * <code>int32 batch_size = 1;</code>
           */
          private void clearBatchSize() {
            
            batchSize_ = 0;
          }

          public static final int INPUT_FIELD_NUMBER = 2;
          private static final class InputDefaultEntryHolder {
            static final com.google.protobuf.MapEntryLite<
                java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> defaultEntry =
                    com.google.protobuf.MapEntryLite
                    .<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>newDefaultInstance(
                        com.google.protobuf.WireFormat.FieldType.STRING,
                        "",
                        com.google.protobuf.WireFormat.FieldType.MESSAGE,
                        inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape.getDefaultInstance());
          }
          private com.google.protobuf.MapFieldLite<
              java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> input_ =
                  com.google.protobuf.MapFieldLite.emptyMapField();
          private com.google.protobuf.MapFieldLite<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
          internalGetInput() {
            return input_;
          }
          private com.google.protobuf.MapFieldLite<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
          internalGetMutableInput() {
            if (!input_.isMutable()) {
              input_ = input_.mutableCopy();
            }
            return input_;
          }
          @java.lang.Override

          public int getInputCount() {
            return internalGetInput().size();
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
           *&#64;&#64;         the input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */
          @java.lang.Override

          public boolean containsInput(
              java.lang.String key) {
            java.lang.Class<?> keyClass = key.getClass();
            return internalGetInput().containsKey(key);
          }
          /**
           * Use {@link #getInputMap()} instead.
           */
          @java.lang.Override
          @java.lang.Deprecated
          public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> getInput() {
            return getInputMap();
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
           *&#64;&#64;         the input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */
          @java.lang.Override

          public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> getInputMap() {
            return java.util.Collections.unmodifiableMap(
                internalGetInput());
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
           *&#64;&#64;         the input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */
          @java.lang.Override

          public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getInputOrDefault(
              java.lang.String key,
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape defaultValue) {
            java.lang.Class<?> keyClass = key.getClass();
            java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> map =
                internalGetInput();
            return map.containsKey(key) ? map.get(key) : defaultValue;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
           *&#64;&#64;         the input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */
          @java.lang.Override

          public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getInputOrThrow(
              java.lang.String key) {
            java.lang.Class<?> keyClass = key.getClass();
            java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> map =
                internalGetInput();
            if (!map.containsKey(key)) {
              throw new java.lang.IllegalArgumentException();
            }
            return map.get(key);
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
           *&#64;&#64;         the input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */
          private java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
          getMutableInputMap() {
            return internalGetMutableInput();
          }

          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound parseFrom(
              java.nio.ByteBuffer data)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return com.google.protobuf.GeneratedMessageLite.parseFrom(
                DEFAULT_INSTANCE, data);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound parseFrom(
              java.nio.ByteBuffer data,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return com.google.protobuf.GeneratedMessageLite.parseFrom(
                DEFAULT_INSTANCE, data, extensionRegistry);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound parseFrom(
              com.google.protobuf.ByteString data)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return com.google.protobuf.GeneratedMessageLite.parseFrom(
                DEFAULT_INSTANCE, data);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound parseFrom(
              com.google.protobuf.ByteString data,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return com.google.protobuf.GeneratedMessageLite.parseFrom(
                DEFAULT_INSTANCE, data, extensionRegistry);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound parseFrom(byte[] data)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return com.google.protobuf.GeneratedMessageLite.parseFrom(
                DEFAULT_INSTANCE, data);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound parseFrom(
              byte[] data,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return com.google.protobuf.GeneratedMessageLite.parseFrom(
                DEFAULT_INSTANCE, data, extensionRegistry);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound parseFrom(java.io.InputStream input)
              throws java.io.IOException {
            return com.google.protobuf.GeneratedMessageLite.parseFrom(
                DEFAULT_INSTANCE, input);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound parseFrom(
              java.io.InputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws java.io.IOException {
            return com.google.protobuf.GeneratedMessageLite.parseFrom(
                DEFAULT_INSTANCE, input, extensionRegistry);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound parseDelimitedFrom(java.io.InputStream input)
              throws java.io.IOException {
            return parseDelimitedFrom(DEFAULT_INSTANCE, input);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound parseDelimitedFrom(
              java.io.InputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws java.io.IOException {
            return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound parseFrom(
              com.google.protobuf.CodedInputStream input)
              throws java.io.IOException {
            return com.google.protobuf.GeneratedMessageLite.parseFrom(
                DEFAULT_INSTANCE, input);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound parseFrom(
              com.google.protobuf.CodedInputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws java.io.IOException {
            return com.google.protobuf.GeneratedMessageLite.parseFrom(
                DEFAULT_INSTANCE, input, extensionRegistry);
          }

          public static Builder newBuilder() {
            return (Builder) DEFAULT_INSTANCE.createBuilder();
          }
          public static Builder newBuilder(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound prototype) {
            return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
          }

          /**
           * Protobuf type {@code inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound}
           */
          public static final class Builder extends
              com.google.protobuf.GeneratedMessageLite.Builder<
                inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound, Builder> implements
              // @@protoc_insertion_point(builder_implements:inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound)
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBoundOrBuilder {
            // Construct using inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.newBuilder()
            private Builder() {
              super(DEFAULT_INSTANCE);
            }


            /**
             * <pre>
             *&#64;&#64;      .. cpp:var:: int32 batch_size
             *&#64;&#64;
             *&#64;&#64;         The batch size of the CUDA graph. If 'max_batch_size' is 0,
             *&#64;&#64;         'batch_size' must be set to 0. Otherwise, 'batch_size' must
             *&#64;&#64;         be set to value between 1 and 'max_batch_size'.
             *&#64;&#64;
             * </pre>
             *
             * <code>int32 batch_size = 1;</code>
             * @return The batchSize.
             */
            @java.lang.Override
            public int getBatchSize() {
              return instance.getBatchSize();
            }
            /**
             * <pre>
             *&#64;&#64;      .. cpp:var:: int32 batch_size
             *&#64;&#64;
             *&#64;&#64;         The batch size of the CUDA graph. If 'max_batch_size' is 0,
             *&#64;&#64;         'batch_size' must be set to 0. Otherwise, 'batch_size' must
             *&#64;&#64;         be set to value between 1 and 'max_batch_size'.
             *&#64;&#64;
             * </pre>
             *
             * <code>int32 batch_size = 1;</code>
             * @param value The batchSize to set.
             * @return This builder for chaining.
             */
            public Builder setBatchSize(int value) {
              copyOnWrite();
              instance.setBatchSize(value);
              return this;
            }
            /**
             * <pre>
             *&#64;&#64;      .. cpp:var:: int32 batch_size
             *&#64;&#64;
             *&#64;&#64;         The batch size of the CUDA graph. If 'max_batch_size' is 0,
             *&#64;&#64;         'batch_size' must be set to 0. Otherwise, 'batch_size' must
             *&#64;&#64;         be set to value between 1 and 'max_batch_size'.
             *&#64;&#64;
             * </pre>
             *
             * <code>int32 batch_size = 1;</code>
             * @return This builder for chaining.
             */
            public Builder clearBatchSize() {
              copyOnWrite();
              instance.clearBatchSize();
              return this;
            }

            @java.lang.Override

            public int getInputCount() {
              return instance.getInputMap().size();
            }
            /**
             * <pre>
             *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
             *&#64;&#64;
             *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
             *&#64;&#64;         the input without batching dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
             */
            @java.lang.Override

            public boolean containsInput(
                java.lang.String key) {
              java.lang.Class<?> keyClass = key.getClass();
              return instance.getInputMap().containsKey(key);
            }

            public Builder clearInput() {
              copyOnWrite();
              instance.getMutableInputMap().clear();
              return this;
            }
            /**
             * <pre>
             *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
             *&#64;&#64;
             *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
             *&#64;&#64;         the input without batching dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
             */

            public Builder removeInput(
                java.lang.String key) {
              java.lang.Class<?> keyClass = key.getClass();
              copyOnWrite();
              instance.getMutableInputMap().remove(key);
              return this;
            }
            /**
             * Use {@link #getInputMap()} instead.
             */
            @java.lang.Override
            @java.lang.Deprecated
            public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> getInput() {
              return getInputMap();
            }
            /**
             * <pre>
             *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
             *&#64;&#64;
             *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
             *&#64;&#64;         the input without batching dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
             */
            @java.lang.Override
            public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> getInputMap() {
              return java.util.Collections.unmodifiableMap(
                  instance.getInputMap());
            }
            /**
             * <pre>
             *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
             *&#64;&#64;
             *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
             *&#64;&#64;         the input without batching dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
             */
            @java.lang.Override

            public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getInputOrDefault(
                java.lang.String key,
                inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape defaultValue) {
              java.lang.Class<?> keyClass = key.getClass();
              java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> map =
                  instance.getInputMap();
              return map.containsKey(key) ? map.get(key) : defaultValue;
            }
            /**
             * <pre>
             *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
             *&#64;&#64;
             *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
             *&#64;&#64;         the input without batching dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
             */
            @java.lang.Override

            public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getInputOrThrow(
                java.lang.String key) {
              java.lang.Class<?> keyClass = key.getClass();
              java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> map =
                  instance.getInputMap();
              if (!map.containsKey(key)) {
                throw new java.lang.IllegalArgumentException();
              }
              return map.get(key);
            }
            /**
             * <pre>
             *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
             *&#64;&#64;
             *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
             *&#64;&#64;         the input without batching dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
             */
            public Builder putInput(
                java.lang.String key,
                inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape value) {
              java.lang.Class<?> keyClass = key.getClass();
              java.lang.Class<?> valueClass = value.getClass();
              copyOnWrite();
              instance.getMutableInputMap().put(key, value);
              return this;
            }
            /**
             * <pre>
             *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
             *&#64;&#64;
             *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
             *&#64;&#64;         the input without batching dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
             */
            public Builder putAllInput(
                java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> values) {
              copyOnWrite();
              instance.getMutableInputMap().putAll(values);
              return this;
            }

            // @@protoc_insertion_point(builder_scope:inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound)
          }
          @java.lang.Override
          @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
          protected final java.lang.Object dynamicMethod(
              com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
              java.lang.Object arg0, java.lang.Object arg1) {
            switch (method) {
              case NEW_MUTABLE_INSTANCE: {
                return new inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound();
              }
              case NEW_BUILDER: {
                return new Builder();
              }
              case BUILD_MESSAGE_INFO: {
                  java.lang.Object[] objects = new java.lang.Object[] {
                    "batchSize_",
                    "input_",
                    InputDefaultEntryHolder.defaultEntry,
                  };
                  java.lang.String info =
                      "\u0000\u0002\u0000\u0000\u0001\u0002\u0002\u0001\u0000\u0000\u0001\u0004\u00022";
                  return newMessageInfo(DEFAULT_INSTANCE, info, objects);
              }
              // fall through
              case GET_DEFAULT_INSTANCE: {
                return DEFAULT_INSTANCE;
              }
              case GET_PARSER: {
                com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound> parser = PARSER;
                if (parser == null) {
                  synchronized (inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.class) {
                    parser = PARSER;
                    if (parser == null) {
                      parser =
                          new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound>(
                              DEFAULT_INSTANCE);
                      PARSER = parser;
                    }
                  }
                }
                return parser;
            }
            case GET_MEMOIZED_IS_INITIALIZED: {
              return (byte) 1;
            }
            case SET_MEMOIZED_IS_INITIALIZED: {
              return null;
            }
            }
            throw new UnsupportedOperationException();
          }


          // @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound)
          private static final inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound DEFAULT_INSTANCE;
          static {
            LowerBound defaultInstance = new LowerBound();
            // New instances are implicitly immutable so no need to make
            // immutable.
            DEFAULT_INSTANCE = defaultInstance;
            com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
              LowerBound.class, defaultInstance);
          }

          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound getDefaultInstance() {
            return DEFAULT_INSTANCE;
          }

          private static volatile com.google.protobuf.Parser<LowerBound> PARSER;

          public static com.google.protobuf.Parser<LowerBound> parser() {
            return DEFAULT_INSTANCE.getParserForType();
          }
        }

        public static final int BATCH_SIZE_FIELD_NUMBER = 1;
        private int batchSize_;
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: int32 batch_size
         *&#64;&#64;
         *&#64;&#64;         The batch size of the CUDA graph. If 'max_batch_size' is 0,
         *&#64;&#64;         'batch_size' must be set to 0. Otherwise, 'batch_size' must
         *&#64;&#64;         be set to value between 1 and 'max_batch_size'.
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 batch_size = 1;</code>
         * @return The batchSize.
         */
        @java.lang.Override
        public int getBatchSize() {
          return batchSize_;
        }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: int32 batch_size
         *&#64;&#64;
         *&#64;&#64;         The batch size of the CUDA graph. If 'max_batch_size' is 0,
         *&#64;&#64;         'batch_size' must be set to 0. Otherwise, 'batch_size' must
         *&#64;&#64;         be set to value between 1 and 'max_batch_size'.
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 batch_size = 1;</code>
         * @param value The batchSize to set.
         */
        private void setBatchSize(int value) {
          
          batchSize_ = value;
        }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: int32 batch_size
         *&#64;&#64;
         *&#64;&#64;         The batch size of the CUDA graph. If 'max_batch_size' is 0,
         *&#64;&#64;         'batch_size' must be set to 0. Otherwise, 'batch_size' must
         *&#64;&#64;         be set to value between 1 and 'max_batch_size'.
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 batch_size = 1;</code>
         */
        private void clearBatchSize() {
          
          batchSize_ = 0;
        }

        public static final int INPUT_FIELD_NUMBER = 2;
        private static final class InputDefaultEntryHolder {
          static final com.google.protobuf.MapEntryLite<
              java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> defaultEntry =
                  com.google.protobuf.MapEntryLite
                  .<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>newDefaultInstance(
                      com.google.protobuf.WireFormat.FieldType.STRING,
                      "",
                      com.google.protobuf.WireFormat.FieldType.MESSAGE,
                      inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape.getDefaultInstance());
        }
        private com.google.protobuf.MapFieldLite<
            java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> input_ =
                com.google.protobuf.MapFieldLite.emptyMapField();
        private com.google.protobuf.MapFieldLite<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
        internalGetInput() {
          return input_;
        }
        private com.google.protobuf.MapFieldLite<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
        internalGetMutableInput() {
          if (!input_.isMutable()) {
            input_ = input_.mutableCopy();
          }
          return input_;
        }
        @java.lang.Override

        public int getInputCount() {
          return internalGetInput().size();
        }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
         *&#64;&#64;
         *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
         *&#64;&#64;         input without batching dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
         */
        @java.lang.Override

        public boolean containsInput(
            java.lang.String key) {
          java.lang.Class<?> keyClass = key.getClass();
          return internalGetInput().containsKey(key);
        }
        /**
         * Use {@link #getInputMap()} instead.
         */
        @java.lang.Override
        @java.lang.Deprecated
        public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> getInput() {
          return getInputMap();
        }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
         *&#64;&#64;
         *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
         *&#64;&#64;         input without batching dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
         */
        @java.lang.Override

        public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> getInputMap() {
          return java.util.Collections.unmodifiableMap(
              internalGetInput());
        }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
         *&#64;&#64;
         *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
         *&#64;&#64;         input without batching dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
         */
        @java.lang.Override

        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getInputOrDefault(
            java.lang.String key,
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape defaultValue) {
          java.lang.Class<?> keyClass = key.getClass();
          java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> map =
              internalGetInput();
          return map.containsKey(key) ? map.get(key) : defaultValue;
        }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
         *&#64;&#64;
         *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
         *&#64;&#64;         input without batching dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
         */
        @java.lang.Override

        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getInputOrThrow(
            java.lang.String key) {
          java.lang.Class<?> keyClass = key.getClass();
          java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> map =
              internalGetInput();
          if (!map.containsKey(key)) {
            throw new java.lang.IllegalArgumentException();
          }
          return map.get(key);
        }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
         *&#64;&#64;
         *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
         *&#64;&#64;         input without batching dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
         */
        private java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
        getMutableInputMap() {
          return internalGetMutableInput();
        }

        public static final int GRAPH_LOWER_BOUND_FIELD_NUMBER = 3;
        private inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graphLowerBound_;
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
         *&#64;&#64;
         *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
         *&#64;&#64;         If specified, the graph can be used for input shapes and
         *&#64;&#64;         batch sizes that are in closed interval between the lower
         *&#64;&#64;         bound specification and graph specification. For dynamic
         *&#64;&#64;         shape model, this allows CUDA graphs to be launched
         *&#64;&#64;         frequently without capturing all possible shape combinations.
         *&#64;&#64;         However, using graph for shape combinations different from
         *&#64;&#64;         the one used for capturing introduces uninitialized data for
         *&#64;&#64;         execution and it may distort the inference result if
         *&#64;&#64;         the model is sensitive to uninitialized data.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
         */
        @java.lang.Override
        public boolean hasGraphLowerBound() {
          return graphLowerBound_ != null;
        }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
         *&#64;&#64;
         *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
         *&#64;&#64;         If specified, the graph can be used for input shapes and
         *&#64;&#64;         batch sizes that are in closed interval between the lower
         *&#64;&#64;         bound specification and graph specification. For dynamic
         *&#64;&#64;         shape model, this allows CUDA graphs to be launched
         *&#64;&#64;         frequently without capturing all possible shape combinations.
         *&#64;&#64;         However, using graph for shape combinations different from
         *&#64;&#64;         the one used for capturing introduces uninitialized data for
         *&#64;&#64;         execution and it may distort the inference result if
         *&#64;&#64;         the model is sensitive to uninitialized data.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
         */
        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound getGraphLowerBound() {
          return graphLowerBound_ == null ? inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.getDefaultInstance() : graphLowerBound_;
        }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
         *&#64;&#64;
         *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
         *&#64;&#64;         If specified, the graph can be used for input shapes and
         *&#64;&#64;         batch sizes that are in closed interval between the lower
         *&#64;&#64;         bound specification and graph specification. For dynamic
         *&#64;&#64;         shape model, this allows CUDA graphs to be launched
         *&#64;&#64;         frequently without capturing all possible shape combinations.
         *&#64;&#64;         However, using graph for shape combinations different from
         *&#64;&#64;         the one used for capturing introduces uninitialized data for
         *&#64;&#64;         execution and it may distort the inference result if
         *&#64;&#64;         the model is sensitive to uninitialized data.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
         */
        private void setGraphLowerBound(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound value) {
          value.getClass();
  graphLowerBound_ = value;
          
          }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
         *&#64;&#64;
         *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
         *&#64;&#64;         If specified, the graph can be used for input shapes and
         *&#64;&#64;         batch sizes that are in closed interval between the lower
         *&#64;&#64;         bound specification and graph specification. For dynamic
         *&#64;&#64;         shape model, this allows CUDA graphs to be launched
         *&#64;&#64;         frequently without capturing all possible shape combinations.
         *&#64;&#64;         However, using graph for shape combinations different from
         *&#64;&#64;         the one used for capturing introduces uninitialized data for
         *&#64;&#64;         execution and it may distort the inference result if
         *&#64;&#64;         the model is sensitive to uninitialized data.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
         */
        @java.lang.SuppressWarnings({"ReferenceEquality"})
        private void mergeGraphLowerBound(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound value) {
          value.getClass();
  if (graphLowerBound_ != null &&
              graphLowerBound_ != inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.getDefaultInstance()) {
            graphLowerBound_ =
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.newBuilder(graphLowerBound_).mergeFrom(value).buildPartial();
          } else {
            graphLowerBound_ = value;
          }
          
        }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
         *&#64;&#64;
         *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
         *&#64;&#64;         If specified, the graph can be used for input shapes and
         *&#64;&#64;         batch sizes that are in closed interval between the lower
         *&#64;&#64;         bound specification and graph specification. For dynamic
         *&#64;&#64;         shape model, this allows CUDA graphs to be launched
         *&#64;&#64;         frequently without capturing all possible shape combinations.
         *&#64;&#64;         However, using graph for shape combinations different from
         *&#64;&#64;         the one used for capturing introduces uninitialized data for
         *&#64;&#64;         execution and it may distort the inference result if
         *&#64;&#64;         the model is sensitive to uninitialized data.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
         */
        private void clearGraphLowerBound() {  graphLowerBound_ = null;
          
        }

        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec parseFrom(
            java.nio.ByteBuffer data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return com.google.protobuf.GeneratedMessageLite.parseFrom(
              DEFAULT_INSTANCE, data);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec parseFrom(
            java.nio.ByteBuffer data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return com.google.protobuf.GeneratedMessageLite.parseFrom(
              DEFAULT_INSTANCE, data, extensionRegistry);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec parseFrom(
            com.google.protobuf.ByteString data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return com.google.protobuf.GeneratedMessageLite.parseFrom(
              DEFAULT_INSTANCE, data);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec parseFrom(
            com.google.protobuf.ByteString data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return com.google.protobuf.GeneratedMessageLite.parseFrom(
              DEFAULT_INSTANCE, data, extensionRegistry);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec parseFrom(byte[] data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return com.google.protobuf.GeneratedMessageLite.parseFrom(
              DEFAULT_INSTANCE, data);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec parseFrom(
            byte[] data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return com.google.protobuf.GeneratedMessageLite.parseFrom(
              DEFAULT_INSTANCE, data, extensionRegistry);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec parseFrom(java.io.InputStream input)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageLite.parseFrom(
              DEFAULT_INSTANCE, input);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec parseFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageLite.parseFrom(
              DEFAULT_INSTANCE, input, extensionRegistry);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec parseDelimitedFrom(java.io.InputStream input)
            throws java.io.IOException {
          return parseDelimitedFrom(DEFAULT_INSTANCE, input);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec parseDelimitedFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec parseFrom(
            com.google.protobuf.CodedInputStream input)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageLite.parseFrom(
              DEFAULT_INSTANCE, input);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec parseFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageLite.parseFrom(
              DEFAULT_INSTANCE, input, extensionRegistry);
        }

        public static Builder newBuilder() {
          return (Builder) DEFAULT_INSTANCE.createBuilder();
        }
        public static Builder newBuilder(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec prototype) {
          return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
        }

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: message GraphSpec
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured.
         *&#64;&#64;
         * </pre>
         *
         * Protobuf type {@code inference.ModelOptimizationPolicy.Cuda.GraphSpec}
         */
        public static final class Builder extends
            com.google.protobuf.GeneratedMessageLite.Builder<
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec, Builder> implements
            // @@protoc_insertion_point(builder_implements:inference.ModelOptimizationPolicy.Cuda.GraphSpec)
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpecOrBuilder {
          // Construct using inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.newBuilder()
          private Builder() {
            super(DEFAULT_INSTANCE);
          }


          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: int32 batch_size
           *&#64;&#64;
           *&#64;&#64;         The batch size of the CUDA graph. If 'max_batch_size' is 0,
           *&#64;&#64;         'batch_size' must be set to 0. Otherwise, 'batch_size' must
           *&#64;&#64;         be set to value between 1 and 'max_batch_size'.
           *&#64;&#64;
           * </pre>
           *
           * <code>int32 batch_size = 1;</code>
           * @return The batchSize.
           */
          @java.lang.Override
          public int getBatchSize() {
            return instance.getBatchSize();
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: int32 batch_size
           *&#64;&#64;
           *&#64;&#64;         The batch size of the CUDA graph. If 'max_batch_size' is 0,
           *&#64;&#64;         'batch_size' must be set to 0. Otherwise, 'batch_size' must
           *&#64;&#64;         be set to value between 1 and 'max_batch_size'.
           *&#64;&#64;
           * </pre>
           *
           * <code>int32 batch_size = 1;</code>
           * @param value The batchSize to set.
           * @return This builder for chaining.
           */
          public Builder setBatchSize(int value) {
            copyOnWrite();
            instance.setBatchSize(value);
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: int32 batch_size
           *&#64;&#64;
           *&#64;&#64;         The batch size of the CUDA graph. If 'max_batch_size' is 0,
           *&#64;&#64;         'batch_size' must be set to 0. Otherwise, 'batch_size' must
           *&#64;&#64;         be set to value between 1 and 'max_batch_size'.
           *&#64;&#64;
           * </pre>
           *
           * <code>int32 batch_size = 1;</code>
           * @return This builder for chaining.
           */
          public Builder clearBatchSize() {
            copyOnWrite();
            instance.clearBatchSize();
            return this;
          }

          @java.lang.Override

          public int getInputCount() {
            return instance.getInputMap().size();
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
           *&#64;&#64;         input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */
          @java.lang.Override

          public boolean containsInput(
              java.lang.String key) {
            java.lang.Class<?> keyClass = key.getClass();
            return instance.getInputMap().containsKey(key);
          }

          public Builder clearInput() {
            copyOnWrite();
            instance.getMutableInputMap().clear();
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
           *&#64;&#64;         input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */

          public Builder removeInput(
              java.lang.String key) {
            java.lang.Class<?> keyClass = key.getClass();
            copyOnWrite();
            instance.getMutableInputMap().remove(key);
            return this;
          }
          /**
           * Use {@link #getInputMap()} instead.
           */
          @java.lang.Override
          @java.lang.Deprecated
          public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> getInput() {
            return getInputMap();
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
           *&#64;&#64;         input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */
          @java.lang.Override
          public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> getInputMap() {
            return java.util.Collections.unmodifiableMap(
                instance.getInputMap());
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
           *&#64;&#64;         input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */
          @java.lang.Override

          public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getInputOrDefault(
              java.lang.String key,
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape defaultValue) {
            java.lang.Class<?> keyClass = key.getClass();
            java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> map =
                instance.getInputMap();
            return map.containsKey(key) ? map.get(key) : defaultValue;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
           *&#64;&#64;         input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */
          @java.lang.Override

          public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getInputOrThrow(
              java.lang.String key) {
            java.lang.Class<?> keyClass = key.getClass();
            java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> map =
                instance.getInputMap();
            if (!map.containsKey(key)) {
              throw new java.lang.IllegalArgumentException();
            }
            return map.get(key);
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
           *&#64;&#64;         input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */
          public Builder putInput(
              java.lang.String key,
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape value) {
            java.lang.Class<?> keyClass = key.getClass();
            java.lang.Class<?> valueClass = value.getClass();
            copyOnWrite();
            instance.getMutableInputMap().put(key, value);
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
           *&#64;&#64;         input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */
          public Builder putAllInput(
              java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> values) {
            copyOnWrite();
            instance.getMutableInputMap().putAll(values);
            return this;
          }

          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
           *&#64;&#64;
           *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
           *&#64;&#64;         If specified, the graph can be used for input shapes and
           *&#64;&#64;         batch sizes that are in closed interval between the lower
           *&#64;&#64;         bound specification and graph specification. For dynamic
           *&#64;&#64;         shape model, this allows CUDA graphs to be launched
           *&#64;&#64;         frequently without capturing all possible shape combinations.
           *&#64;&#64;         However, using graph for shape combinations different from
           *&#64;&#64;         the one used for capturing introduces uninitialized data for
           *&#64;&#64;         execution and it may distort the inference result if
           *&#64;&#64;         the model is sensitive to uninitialized data.
           *&#64;&#64;
           * </pre>
           *
           * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
           */
          @java.lang.Override
          public boolean hasGraphLowerBound() {
            return instance.hasGraphLowerBound();
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
           *&#64;&#64;
           *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
           *&#64;&#64;         If specified, the graph can be used for input shapes and
           *&#64;&#64;         batch sizes that are in closed interval between the lower
           *&#64;&#64;         bound specification and graph specification. For dynamic
           *&#64;&#64;         shape model, this allows CUDA graphs to be launched
           *&#64;&#64;         frequently without capturing all possible shape combinations.
           *&#64;&#64;         However, using graph for shape combinations different from
           *&#64;&#64;         the one used for capturing introduces uninitialized data for
           *&#64;&#64;         execution and it may distort the inference result if
           *&#64;&#64;         the model is sensitive to uninitialized data.
           *&#64;&#64;
           * </pre>
           *
           * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
           */
          @java.lang.Override
          public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound getGraphLowerBound() {
            return instance.getGraphLowerBound();
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
           *&#64;&#64;
           *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
           *&#64;&#64;         If specified, the graph can be used for input shapes and
           *&#64;&#64;         batch sizes that are in closed interval between the lower
           *&#64;&#64;         bound specification and graph specification. For dynamic
           *&#64;&#64;         shape model, this allows CUDA graphs to be launched
           *&#64;&#64;         frequently without capturing all possible shape combinations.
           *&#64;&#64;         However, using graph for shape combinations different from
           *&#64;&#64;         the one used for capturing introduces uninitialized data for
           *&#64;&#64;         execution and it may distort the inference result if
           *&#64;&#64;         the model is sensitive to uninitialized data.
           *&#64;&#64;
           * </pre>
           *
           * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
           */
          public Builder setGraphLowerBound(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound value) {
            copyOnWrite();
            instance.setGraphLowerBound(value);
            return this;
            }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
           *&#64;&#64;
           *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
           *&#64;&#64;         If specified, the graph can be used for input shapes and
           *&#64;&#64;         batch sizes that are in closed interval between the lower
           *&#64;&#64;         bound specification and graph specification. For dynamic
           *&#64;&#64;         shape model, this allows CUDA graphs to be launched
           *&#64;&#64;         frequently without capturing all possible shape combinations.
           *&#64;&#64;         However, using graph for shape combinations different from
           *&#64;&#64;         the one used for capturing introduces uninitialized data for
           *&#64;&#64;         execution and it may distort the inference result if
           *&#64;&#64;         the model is sensitive to uninitialized data.
           *&#64;&#64;
           * </pre>
           *
           * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
           */
          public Builder setGraphLowerBound(
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.Builder builderForValue) {
            copyOnWrite();
            instance.setGraphLowerBound(builderForValue.build());
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
           *&#64;&#64;
           *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
           *&#64;&#64;         If specified, the graph can be used for input shapes and
           *&#64;&#64;         batch sizes that are in closed interval between the lower
           *&#64;&#64;         bound specification and graph specification. For dynamic
           *&#64;&#64;         shape model, this allows CUDA graphs to be launched
           *&#64;&#64;         frequently without capturing all possible shape combinations.
           *&#64;&#64;         However, using graph for shape combinations different from
           *&#64;&#64;         the one used for capturing introduces uninitialized data for
           *&#64;&#64;         execution and it may distort the inference result if
           *&#64;&#64;         the model is sensitive to uninitialized data.
           *&#64;&#64;
           * </pre>
           *
           * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
           */
          public Builder mergeGraphLowerBound(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound value) {
            copyOnWrite();
            instance.mergeGraphLowerBound(value);
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
           *&#64;&#64;
           *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
           *&#64;&#64;         If specified, the graph can be used for input shapes and
           *&#64;&#64;         batch sizes that are in closed interval between the lower
           *&#64;&#64;         bound specification and graph specification. For dynamic
           *&#64;&#64;         shape model, this allows CUDA graphs to be launched
           *&#64;&#64;         frequently without capturing all possible shape combinations.
           *&#64;&#64;         However, using graph for shape combinations different from
           *&#64;&#64;         the one used for capturing introduces uninitialized data for
           *&#64;&#64;         execution and it may distort the inference result if
           *&#64;&#64;         the model is sensitive to uninitialized data.
           *&#64;&#64;
           * </pre>
           *
           * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
           */
          public Builder clearGraphLowerBound() {  copyOnWrite();
            instance.clearGraphLowerBound();
            return this;
          }

          // @@protoc_insertion_point(builder_scope:inference.ModelOptimizationPolicy.Cuda.GraphSpec)
        }
        @java.lang.Override
        @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
        protected final java.lang.Object dynamicMethod(
            com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
            java.lang.Object arg0, java.lang.Object arg1) {
          switch (method) {
            case NEW_MUTABLE_INSTANCE: {
              return new inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec();
            }
            case NEW_BUILDER: {
              return new Builder();
            }
            case BUILD_MESSAGE_INFO: {
                java.lang.Object[] objects = new java.lang.Object[] {
                  "batchSize_",
                  "input_",
                  InputDefaultEntryHolder.defaultEntry,
                  "graphLowerBound_",
                };
                java.lang.String info =
                    "\u0000\u0003\u0000\u0000\u0001\u0003\u0003\u0001\u0000\u0000\u0001\u0004\u00022\u0003" +
                    "\t";
                return newMessageInfo(DEFAULT_INSTANCE, info, objects);
            }
            // fall through
            case GET_DEFAULT_INSTANCE: {
              return DEFAULT_INSTANCE;
            }
            case GET_PARSER: {
              com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec> parser = PARSER;
              if (parser == null) {
                synchronized (inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.class) {
                  parser = PARSER;
                  if (parser == null) {
                    parser =
                        new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec>(
                            DEFAULT_INSTANCE);
                    PARSER = parser;
                  }
                }
              }
              return parser;
          }
          case GET_MEMOIZED_IS_INITIALIZED: {
            return (byte) 1;
          }
          case SET_MEMOIZED_IS_INITIALIZED: {
            return null;
          }
          }
          throw new UnsupportedOperationException();
        }


        // @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.Cuda.GraphSpec)
        private static final inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec DEFAULT_INSTANCE;
        static {
          GraphSpec defaultInstance = new GraphSpec();
          // New instances are implicitly immutable so no need to make
          // immutable.
          DEFAULT_INSTANCE = defaultInstance;
          com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
            GraphSpec.class, defaultInstance);
        }

        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec getDefaultInstance() {
          return DEFAULT_INSTANCE;
        }

        private static volatile com.google.protobuf.Parser<GraphSpec> PARSER;

        public static com.google.protobuf.Parser<GraphSpec> parser() {
          return DEFAULT_INSTANCE.getParserForType();
        }
      }

      public static final int GRAPHS_FIELD_NUMBER = 1;
      private boolean graphs_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool graphs
       *&#64;&#64;
       *&#64;&#64;       Use CUDA graphs API to capture model operations and execute
       *&#64;&#64;       them more efficiently. Default value is false.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool graphs = 1;</code>
       * @return The graphs.
       */
      @java.lang.Override
      public boolean getGraphs() {
        return graphs_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool graphs
       *&#64;&#64;
       *&#64;&#64;       Use CUDA graphs API to capture model operations and execute
       *&#64;&#64;       them more efficiently. Default value is false.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool graphs = 1;</code>
       * @param value The graphs to set.
       */
      private void setGraphs(boolean value) {
        
        graphs_ = value;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool graphs
       *&#64;&#64;
       *&#64;&#64;       Use CUDA graphs API to capture model operations and execute
       *&#64;&#64;       them more efficiently. Default value is false.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool graphs = 1;</code>
       */
      private void clearGraphs() {
        
        graphs_ = false;
      }

      public static final int BUSY_WAIT_EVENTS_FIELD_NUMBER = 2;
      private boolean busyWaitEvents_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool busy_wait_events
       *&#64;&#64;
       *&#64;&#64;       Use busy-waiting to synchronize CUDA events to achieve minimum
       *&#64;&#64;       latency from event complete to host thread to be notified, with
       *&#64;&#64;       the cost of high CPU load. Default value is false.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool busy_wait_events = 2;</code>
       * @return The busyWaitEvents.
       */
      @java.lang.Override
      public boolean getBusyWaitEvents() {
        return busyWaitEvents_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool busy_wait_events
       *&#64;&#64;
       *&#64;&#64;       Use busy-waiting to synchronize CUDA events to achieve minimum
       *&#64;&#64;       latency from event complete to host thread to be notified, with
       *&#64;&#64;       the cost of high CPU load. Default value is false.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool busy_wait_events = 2;</code>
       * @param value The busyWaitEvents to set.
       */
      private void setBusyWaitEvents(boolean value) {
        
        busyWaitEvents_ = value;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool busy_wait_events
       *&#64;&#64;
       *&#64;&#64;       Use busy-waiting to synchronize CUDA events to achieve minimum
       *&#64;&#64;       latency from event complete to host thread to be notified, with
       *&#64;&#64;       the cost of high CPU load. Default value is false.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool busy_wait_events = 2;</code>
       */
      private void clearBusyWaitEvents() {
        
        busyWaitEvents_ = false;
      }

      public static final int GRAPH_SPEC_FIELD_NUMBER = 3;
      private com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec> graphSpec_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
       *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
       *&#64;&#64;       based on model settings.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
       */
      @java.lang.Override
      public java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec> getGraphSpecList() {
        return graphSpec_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
       *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
       *&#64;&#64;       based on model settings.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
       */
      public java.util.List<? extends inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpecOrBuilder> 
          getGraphSpecOrBuilderList() {
        return graphSpec_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
       *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
       *&#64;&#64;       based on model settings.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
       */
      @java.lang.Override
      public int getGraphSpecCount() {
        return graphSpec_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
       *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
       *&#64;&#64;       based on model settings.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec getGraphSpec(int index) {
        return graphSpec_.get(index);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
       *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
       *&#64;&#64;       based on model settings.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
       */
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpecOrBuilder getGraphSpecOrBuilder(
          int index) {
        return graphSpec_.get(index);
      }
      private void ensureGraphSpecIsMutable() {
        com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec> tmp = graphSpec_;
        if (!tmp.isModifiable()) {
          graphSpec_ =
              com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
         }
      }

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
       *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
       *&#64;&#64;       based on model settings.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
       */
      private void setGraphSpec(
          int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec value) {
        value.getClass();
  ensureGraphSpecIsMutable();
        graphSpec_.set(index, value);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
       *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
       *&#64;&#64;       based on model settings.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
       */
      private void addGraphSpec(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec value) {
        value.getClass();
  ensureGraphSpecIsMutable();
        graphSpec_.add(value);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
       *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
       *&#64;&#64;       based on model settings.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
       */
      private void addGraphSpec(
          int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec value) {
        value.getClass();
  ensureGraphSpecIsMutable();
        graphSpec_.add(index, value);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
       *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
       *&#64;&#64;       based on model settings.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
       */
      private void addAllGraphSpec(
          java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec> values) {
        ensureGraphSpecIsMutable();
        com.google.protobuf.AbstractMessageLite.addAll(
            values, graphSpec_);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
       *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
       *&#64;&#64;       based on model settings.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
       */
      private void clearGraphSpec() {
        graphSpec_ = emptyProtobufList();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
       *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
       *&#64;&#64;       based on model settings.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
       */
      private void removeGraphSpec(int index) {
        ensureGraphSpecIsMutable();
        graphSpec_.remove(index);
      }

      public static final int OUTPUT_COPY_STREAM_FIELD_NUMBER = 4;
      private boolean outputCopyStream_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool output_copy_stream
       *&#64;&#64;
       *&#64;&#64;       Uses a CUDA stream separate from the inference stream to copy the
       *&#64;&#64;       output to host. Default value is false.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool output_copy_stream = 4;</code>
       * @return The outputCopyStream.
       */
      @java.lang.Override
      public boolean getOutputCopyStream() {
        return outputCopyStream_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool output_copy_stream
       *&#64;&#64;
       *&#64;&#64;       Uses a CUDA stream separate from the inference stream to copy the
       *&#64;&#64;       output to host. Default value is false.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool output_copy_stream = 4;</code>
       * @param value The outputCopyStream to set.
       */
      private void setOutputCopyStream(boolean value) {
        
        outputCopyStream_ = value;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool output_copy_stream
       *&#64;&#64;
       *&#64;&#64;       Uses a CUDA stream separate from the inference stream to copy the
       *&#64;&#64;       output to host. Default value is false.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool output_copy_stream = 4;</code>
       */
      private void clearOutputCopyStream() {
        
        outputCopyStream_ = false;
      }

      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }

      public static Builder newBuilder() {
        return (Builder) DEFAULT_INSTANCE.createBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda prototype) {
        return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
      }

      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: message Cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelOptimizationPolicy.Cuda}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageLite.Builder<
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda, Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelOptimizationPolicy.Cuda)
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.CudaOrBuilder {
        // Construct using inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.newBuilder()
        private Builder() {
          super(DEFAULT_INSTANCE);
        }


        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool graphs
         *&#64;&#64;
         *&#64;&#64;       Use CUDA graphs API to capture model operations and execute
         *&#64;&#64;       them more efficiently. Default value is false.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool graphs = 1;</code>
         * @return The graphs.
         */
        @java.lang.Override
        public boolean getGraphs() {
          return instance.getGraphs();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool graphs
         *&#64;&#64;
         *&#64;&#64;       Use CUDA graphs API to capture model operations and execute
         *&#64;&#64;       them more efficiently. Default value is false.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool graphs = 1;</code>
         * @param value The graphs to set.
         * @return This builder for chaining.
         */
        public Builder setGraphs(boolean value) {
          copyOnWrite();
          instance.setGraphs(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool graphs
         *&#64;&#64;
         *&#64;&#64;       Use CUDA graphs API to capture model operations and execute
         *&#64;&#64;       them more efficiently. Default value is false.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool graphs = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearGraphs() {
          copyOnWrite();
          instance.clearGraphs();
          return this;
        }

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool busy_wait_events
         *&#64;&#64;
         *&#64;&#64;       Use busy-waiting to synchronize CUDA events to achieve minimum
         *&#64;&#64;       latency from event complete to host thread to be notified, with
         *&#64;&#64;       the cost of high CPU load. Default value is false.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool busy_wait_events = 2;</code>
         * @return The busyWaitEvents.
         */
        @java.lang.Override
        public boolean getBusyWaitEvents() {
          return instance.getBusyWaitEvents();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool busy_wait_events
         *&#64;&#64;
         *&#64;&#64;       Use busy-waiting to synchronize CUDA events to achieve minimum
         *&#64;&#64;       latency from event complete to host thread to be notified, with
         *&#64;&#64;       the cost of high CPU load. Default value is false.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool busy_wait_events = 2;</code>
         * @param value The busyWaitEvents to set.
         * @return This builder for chaining.
         */
        public Builder setBusyWaitEvents(boolean value) {
          copyOnWrite();
          instance.setBusyWaitEvents(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool busy_wait_events
         *&#64;&#64;
         *&#64;&#64;       Use busy-waiting to synchronize CUDA events to achieve minimum
         *&#64;&#64;       latency from event complete to host thread to be notified, with
         *&#64;&#64;       the cost of high CPU load. Default value is false.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool busy_wait_events = 2;</code>
         * @return This builder for chaining.
         */
        public Builder clearBusyWaitEvents() {
          copyOnWrite();
          instance.clearBusyWaitEvents();
          return this;
        }

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        @java.lang.Override
        public java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec> getGraphSpecList() {
          return java.util.Collections.unmodifiableList(
              instance.getGraphSpecList());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        @java.lang.Override
        public int getGraphSpecCount() {
          return instance.getGraphSpecCount();
        }/**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec getGraphSpec(int index) {
          return instance.getGraphSpec(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public Builder setGraphSpec(
            int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec value) {
          copyOnWrite();
          instance.setGraphSpec(index, value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public Builder setGraphSpec(
            int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Builder builderForValue) {
          copyOnWrite();
          instance.setGraphSpec(index,
              builderForValue.build());
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public Builder addGraphSpec(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec value) {
          copyOnWrite();
          instance.addGraphSpec(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public Builder addGraphSpec(
            int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec value) {
          copyOnWrite();
          instance.addGraphSpec(index, value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public Builder addGraphSpec(
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Builder builderForValue) {
          copyOnWrite();
          instance.addGraphSpec(builderForValue.build());
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public Builder addGraphSpec(
            int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Builder builderForValue) {
          copyOnWrite();
          instance.addGraphSpec(index,
              builderForValue.build());
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public Builder addAllGraphSpec(
            java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec> values) {
          copyOnWrite();
          instance.addAllGraphSpec(values);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public Builder clearGraphSpec() {
          copyOnWrite();
          instance.clearGraphSpec();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public Builder removeGraphSpec(int index) {
          copyOnWrite();
          instance.removeGraphSpec(index);
          return this;
        }

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool output_copy_stream
         *&#64;&#64;
         *&#64;&#64;       Uses a CUDA stream separate from the inference stream to copy the
         *&#64;&#64;       output to host. Default value is false.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool output_copy_stream = 4;</code>
         * @return The outputCopyStream.
         */
        @java.lang.Override
        public boolean getOutputCopyStream() {
          return instance.getOutputCopyStream();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool output_copy_stream
         *&#64;&#64;
         *&#64;&#64;       Uses a CUDA stream separate from the inference stream to copy the
         *&#64;&#64;       output to host. Default value is false.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool output_copy_stream = 4;</code>
         * @param value The outputCopyStream to set.
         * @return This builder for chaining.
         */
        public Builder setOutputCopyStream(boolean value) {
          copyOnWrite();
          instance.setOutputCopyStream(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool output_copy_stream
         *&#64;&#64;
         *&#64;&#64;       Uses a CUDA stream separate from the inference stream to copy the
         *&#64;&#64;       output to host. Default value is false.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool output_copy_stream = 4;</code>
         * @return This builder for chaining.
         */
        public Builder clearOutputCopyStream() {
          copyOnWrite();
          instance.clearOutputCopyStream();
          return this;
        }

        // @@protoc_insertion_point(builder_scope:inference.ModelOptimizationPolicy.Cuda)
      }
      @java.lang.Override
      @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
      protected final java.lang.Object dynamicMethod(
          com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
          java.lang.Object arg0, java.lang.Object arg1) {
        switch (method) {
          case NEW_MUTABLE_INSTANCE: {
            return new inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda();
          }
          case NEW_BUILDER: {
            return new Builder();
          }
          case BUILD_MESSAGE_INFO: {
              java.lang.Object[] objects = new java.lang.Object[] {
                "graphs_",
                "busyWaitEvents_",
                "graphSpec_",
                inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.class,
                "outputCopyStream_",
              };
              java.lang.String info =
                  "\u0000\u0004\u0000\u0000\u0001\u0004\u0004\u0000\u0001\u0000\u0001\u0007\u0002\u0007" +
                  "\u0003\u001b\u0004\u0007";
              return newMessageInfo(DEFAULT_INSTANCE, info, objects);
          }
          // fall through
          case GET_DEFAULT_INSTANCE: {
            return DEFAULT_INSTANCE;
          }
          case GET_PARSER: {
            com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda> parser = PARSER;
            if (parser == null) {
              synchronized (inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.class) {
                parser = PARSER;
                if (parser == null) {
                  parser =
                      new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda>(
                          DEFAULT_INSTANCE);
                  PARSER = parser;
                }
              }
            }
            return parser;
        }
        case GET_MEMOIZED_IS_INITIALIZED: {
          return (byte) 1;
        }
        case SET_MEMOIZED_IS_INITIALIZED: {
          return null;
        }
        }
        throw new UnsupportedOperationException();
      }


      // @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.Cuda)
      private static final inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda DEFAULT_INSTANCE;
      static {
        Cuda defaultInstance = new Cuda();
        // New instances are implicitly immutable so no need to make
        // immutable.
        DEFAULT_INSTANCE = defaultInstance;
        com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
          Cuda.class, defaultInstance);
      }

      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static volatile com.google.protobuf.Parser<Cuda> PARSER;

      public static com.google.protobuf.Parser<Cuda> parser() {
        return DEFAULT_INSTANCE.getParserForType();
      }
    }

    public interface ExecutionAcceleratorsOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelOptimizationPolicy.ExecutionAccelerators)
        com.google.protobuf.MessageLiteOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
       *&#64;&#64;       "auto_mixed_precision", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
       *&#64;&#64;       the model will try to use FP16 for better performance.
       *&#64;&#64;       This optimization can not be set with "tensorrt".
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> 
          getGpuExecutionAcceleratorList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
       *&#64;&#64;       "auto_mixed_precision", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
       *&#64;&#64;       the model will try to use FP16 for better performance.
       *&#64;&#64;       This optimization can not be set with "tensorrt".
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator getGpuExecutionAccelerator(int index);
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
       *&#64;&#64;       "auto_mixed_precision", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
       *&#64;&#64;       the model will try to use FP16 for better performance.
       *&#64;&#64;       This optimization can not be set with "tensorrt".
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      int getGpuExecutionAcceleratorCount();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> 
          getCpuExecutionAcceleratorList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator getCpuExecutionAccelerator(int index);
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      int getCpuExecutionAcceleratorCount();
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: message ExecutionAccelerators
     *&#64;&#64;
     *&#64;&#64;     Specify the preferred execution accelerators to be used to execute
     *&#64;&#64;     the model. Currently only recognized by ONNX Runtime backend and
     *&#64;&#64;     TensorFlow backend.
     *&#64;&#64;
     *&#64;&#64;     For ONNX Runtime backend, it will deploy the model with the execution
     *&#64;&#64;     accelerators by priority, the priority is determined based on the
     *&#64;&#64;     order that they are set, i.e. the provider at the front has highest
     *&#64;&#64;     priority. Overall, the priority will be in the following order:
     *&#64;&#64;         &lt;gpu_execution_accelerator&gt; (if instance is on GPU)
     *&#64;&#64;         CUDA Execution Provider     (if instance is on GPU)
     *&#64;&#64;         &lt;cpu_execution_accelerator&gt;
     *&#64;&#64;         Default CPU Execution Provider
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelOptimizationPolicy.ExecutionAccelerators}
     */
    public  static final class ExecutionAccelerators extends
        com.google.protobuf.GeneratedMessageLite<
            ExecutionAccelerators, ExecutionAccelerators.Builder> implements
        // @@protoc_insertion_point(message_implements:inference.ModelOptimizationPolicy.ExecutionAccelerators)
        ExecutionAcceleratorsOrBuilder {
      private ExecutionAccelerators() {
        gpuExecutionAccelerator_ = emptyProtobufList();
        cpuExecutionAccelerator_ = emptyProtobufList();
      }
      public interface AcceleratorOrBuilder extends
          // @@protoc_insertion_point(interface_extends:inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator)
          com.google.protobuf.MessageLiteOrBuilder {

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the execution accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @return The name.
         */
        java.lang.String getName();
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the execution accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @return The bytes for name.
         */
        com.google.protobuf.ByteString
            getNameBytes();

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */
        int getParametersCount();
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */
        boolean containsParameters(
            java.lang.String key);
        /**
         * Use {@link #getParametersMap()} instead.
         */
        @java.lang.Deprecated
        java.util.Map<java.lang.String, java.lang.String>
        getParameters();
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */
        java.util.Map<java.lang.String, java.lang.String>
        getParametersMap();
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */

        java.lang.String getParametersOrDefault(
            java.lang.String key,
            java.lang.String defaultValue);
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */

        java.lang.String getParametersOrThrow(
            java.lang.String key);
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: message Accelerator
       *&#64;&#64;
       *&#64;&#64;     Specify the accelerator to be used to execute the model.
       *&#64;&#64;     Accelerator with the same name may accept different parameters
       *&#64;&#64;     depending on the backends.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator}
       */
      public  static final class Accelerator extends
          com.google.protobuf.GeneratedMessageLite<
              Accelerator, Accelerator.Builder> implements
          // @@protoc_insertion_point(message_implements:inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator)
          AcceleratorOrBuilder {
        private Accelerator() {
          name_ = "";
        }
        public static final int NAME_FIELD_NUMBER = 1;
        private java.lang.String name_;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the execution accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @return The name.
         */
        @java.lang.Override
        public java.lang.String getName() {
          return name_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the execution accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @return The bytes for name.
         */
        @java.lang.Override
        public com.google.protobuf.ByteString
            getNameBytes() {
          return com.google.protobuf.ByteString.copyFromUtf8(name_);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the execution accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @param value The name to set.
         */
        private void setName(
            java.lang.String value) {
          java.lang.Class<?> valueClass = value.getClass();
  
          name_ = value;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the execution accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        private void clearName() {
          
          name_ = getDefaultInstance().getName();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the execution accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @param value The bytes for name to set.
         */
        private void setNameBytes(
            com.google.protobuf.ByteString value) {
          checkByteStringIsUtf8(value);
          name_ = value.toStringUtf8();
          
        }

        public static final int PARAMETERS_FIELD_NUMBER = 2;
        private static final class ParametersDefaultEntryHolder {
          static final com.google.protobuf.MapEntryLite<
              java.lang.String, java.lang.String> defaultEntry =
                  com.google.protobuf.MapEntryLite
                  .<java.lang.String, java.lang.String>newDefaultInstance(
                      com.google.protobuf.WireFormat.FieldType.STRING,
                      "",
                      com.google.protobuf.WireFormat.FieldType.STRING,
                      "");
        }
        private com.google.protobuf.MapFieldLite<
            java.lang.String, java.lang.String> parameters_ =
                com.google.protobuf.MapFieldLite.emptyMapField();
        private com.google.protobuf.MapFieldLite<java.lang.String, java.lang.String>
        internalGetParameters() {
          return parameters_;
        }
        private com.google.protobuf.MapFieldLite<java.lang.String, java.lang.String>
        internalGetMutableParameters() {
          if (!parameters_.isMutable()) {
            parameters_ = parameters_.mutableCopy();
          }
          return parameters_;
        }
        @java.lang.Override

        public int getParametersCount() {
          return internalGetParameters().size();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */
        @java.lang.Override

        public boolean containsParameters(
            java.lang.String key) {
          java.lang.Class<?> keyClass = key.getClass();
          return internalGetParameters().containsKey(key);
        }
        /**
         * Use {@link #getParametersMap()} instead.
         */
        @java.lang.Override
        @java.lang.Deprecated
        public java.util.Map<java.lang.String, java.lang.String> getParameters() {
          return getParametersMap();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */
        @java.lang.Override

        public java.util.Map<java.lang.String, java.lang.String> getParametersMap() {
          return java.util.Collections.unmodifiableMap(
              internalGetParameters());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */
        @java.lang.Override

        public java.lang.String getParametersOrDefault(
            java.lang.String key,
            java.lang.String defaultValue) {
          java.lang.Class<?> keyClass = key.getClass();
          java.util.Map<java.lang.String, java.lang.String> map =
              internalGetParameters();
          return map.containsKey(key) ? map.get(key) : defaultValue;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */
        @java.lang.Override

        public java.lang.String getParametersOrThrow(
            java.lang.String key) {
          java.lang.Class<?> keyClass = key.getClass();
          java.util.Map<java.lang.String, java.lang.String> map =
              internalGetParameters();
          if (!map.containsKey(key)) {
            throw new java.lang.IllegalArgumentException();
          }
          return map.get(key);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */
        private java.util.Map<java.lang.String, java.lang.String>
        getMutableParametersMap() {
          return internalGetMutableParameters();
        }

        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(
            java.nio.ByteBuffer data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return com.google.protobuf.GeneratedMessageLite.parseFrom(
              DEFAULT_INSTANCE, data);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(
            java.nio.ByteBuffer data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return com.google.protobuf.GeneratedMessageLite.parseFrom(
              DEFAULT_INSTANCE, data, extensionRegistry);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(
            com.google.protobuf.ByteString data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return com.google.protobuf.GeneratedMessageLite.parseFrom(
              DEFAULT_INSTANCE, data);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(
            com.google.protobuf.ByteString data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return com.google.protobuf.GeneratedMessageLite.parseFrom(
              DEFAULT_INSTANCE, data, extensionRegistry);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(byte[] data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return com.google.protobuf.GeneratedMessageLite.parseFrom(
              DEFAULT_INSTANCE, data);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(
            byte[] data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return com.google.protobuf.GeneratedMessageLite.parseFrom(
              DEFAULT_INSTANCE, data, extensionRegistry);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(java.io.InputStream input)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageLite.parseFrom(
              DEFAULT_INSTANCE, input);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageLite.parseFrom(
              DEFAULT_INSTANCE, input, extensionRegistry);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseDelimitedFrom(java.io.InputStream input)
            throws java.io.IOException {
          return parseDelimitedFrom(DEFAULT_INSTANCE, input);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseDelimitedFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(
            com.google.protobuf.CodedInputStream input)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageLite.parseFrom(
              DEFAULT_INSTANCE, input);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageLite.parseFrom(
              DEFAULT_INSTANCE, input, extensionRegistry);
        }

        public static Builder newBuilder() {
          return (Builder) DEFAULT_INSTANCE.createBuilder();
        }
        public static Builder newBuilder(inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator prototype) {
          return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
        }

        /**
         * <pre>
         *&#64;&#64;
         *&#64;&#64;  .. cpp:var:: message Accelerator
         *&#64;&#64;
         *&#64;&#64;     Specify the accelerator to be used to execute the model.
         *&#64;&#64;     Accelerator with the same name may accept different parameters
         *&#64;&#64;     depending on the backends.
         *&#64;&#64;
         * </pre>
         *
         * Protobuf type {@code inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator}
         */
        public static final class Builder extends
            com.google.protobuf.GeneratedMessageLite.Builder<
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator, Builder> implements
            // @@protoc_insertion_point(builder_implements:inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator)
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder {
          // Construct using inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.newBuilder()
          private Builder() {
            super(DEFAULT_INSTANCE);
          }


          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: string name
           *&#64;&#64;
           *&#64;&#64;       The name of the execution accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>string name = 1;</code>
           * @return The name.
           */
          @java.lang.Override
          public java.lang.String getName() {
            return instance.getName();
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: string name
           *&#64;&#64;
           *&#64;&#64;       The name of the execution accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>string name = 1;</code>
           * @return The bytes for name.
           */
          @java.lang.Override
          public com.google.protobuf.ByteString
              getNameBytes() {
            return instance.getNameBytes();
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: string name
           *&#64;&#64;
           *&#64;&#64;       The name of the execution accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>string name = 1;</code>
           * @param value The name to set.
           * @return This builder for chaining.
           */
          public Builder setName(
              java.lang.String value) {
            copyOnWrite();
            instance.setName(value);
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: string name
           *&#64;&#64;
           *&#64;&#64;       The name of the execution accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>string name = 1;</code>
           * @return This builder for chaining.
           */
          public Builder clearName() {
            copyOnWrite();
            instance.clearName();
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: string name
           *&#64;&#64;
           *&#64;&#64;       The name of the execution accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>string name = 1;</code>
           * @param value The bytes for name to set.
           * @return This builder for chaining.
           */
          public Builder setNameBytes(
              com.google.protobuf.ByteString value) {
            copyOnWrite();
            instance.setNameBytes(value);
            return this;
          }

          @java.lang.Override

          public int getParametersCount() {
            return instance.getParametersMap().size();
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
           *&#64;&#64;
           *&#64;&#64;       Additional paremeters used to configure the accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, string&gt; parameters = 2;</code>
           */
          @java.lang.Override

          public boolean containsParameters(
              java.lang.String key) {
            java.lang.Class<?> keyClass = key.getClass();
            return instance.getParametersMap().containsKey(key);
          }

          public Builder clearParameters() {
            copyOnWrite();
            instance.getMutableParametersMap().clear();
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
           *&#64;&#64;
           *&#64;&#64;       Additional paremeters used to configure the accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, string&gt; parameters = 2;</code>
           */

          public Builder removeParameters(
              java.lang.String key) {
            java.lang.Class<?> keyClass = key.getClass();
            copyOnWrite();
            instance.getMutableParametersMap().remove(key);
            return this;
          }
          /**
           * Use {@link #getParametersMap()} instead.
           */
          @java.lang.Override
          @java.lang.Deprecated
          public java.util.Map<java.lang.String, java.lang.String> getParameters() {
            return getParametersMap();
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
           *&#64;&#64;
           *&#64;&#64;       Additional paremeters used to configure the accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, string&gt; parameters = 2;</code>
           */
          @java.lang.Override
          public java.util.Map<java.lang.String, java.lang.String> getParametersMap() {
            return java.util.Collections.unmodifiableMap(
                instance.getParametersMap());
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
           *&#64;&#64;
           *&#64;&#64;       Additional paremeters used to configure the accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, string&gt; parameters = 2;</code>
           */
          @java.lang.Override

          public java.lang.String getParametersOrDefault(
              java.lang.String key,
              java.lang.String defaultValue) {
            java.lang.Class<?> keyClass = key.getClass();
            java.util.Map<java.lang.String, java.lang.String> map =
                instance.getParametersMap();
            return map.containsKey(key) ? map.get(key) : defaultValue;
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
           *&#64;&#64;
           *&#64;&#64;       Additional paremeters used to configure the accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, string&gt; parameters = 2;</code>
           */
          @java.lang.Override

          public java.lang.String getParametersOrThrow(
              java.lang.String key) {
            java.lang.Class<?> keyClass = key.getClass();
            java.util.Map<java.lang.String, java.lang.String> map =
                instance.getParametersMap();
            if (!map.containsKey(key)) {
              throw new java.lang.IllegalArgumentException();
            }
            return map.get(key);
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
           *&#64;&#64;
           *&#64;&#64;       Additional paremeters used to configure the accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, string&gt; parameters = 2;</code>
           */
          public Builder putParameters(
              java.lang.String key,
              java.lang.String value) {
            java.lang.Class<?> keyClass = key.getClass();
            java.lang.Class<?> valueClass = value.getClass();
            copyOnWrite();
            instance.getMutableParametersMap().put(key, value);
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
           *&#64;&#64;
           *&#64;&#64;       Additional paremeters used to configure the accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, string&gt; parameters = 2;</code>
           */
          public Builder putAllParameters(
              java.util.Map<java.lang.String, java.lang.String> values) {
            copyOnWrite();
            instance.getMutableParametersMap().putAll(values);
            return this;
          }

          // @@protoc_insertion_point(builder_scope:inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator)
        }
        @java.lang.Override
        @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
        protected final java.lang.Object dynamicMethod(
            com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
            java.lang.Object arg0, java.lang.Object arg1) {
          switch (method) {
            case NEW_MUTABLE_INSTANCE: {
              return new inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator();
            }
            case NEW_BUILDER: {
              return new Builder();
            }
            case BUILD_MESSAGE_INFO: {
                java.lang.Object[] objects = new java.lang.Object[] {
                  "name_",
                  "parameters_",
                  ParametersDefaultEntryHolder.defaultEntry,
                };
                java.lang.String info =
                    "\u0000\u0002\u0000\u0000\u0001\u0002\u0002\u0001\u0000\u0000\u0001\u0208\u00022";
                return newMessageInfo(DEFAULT_INSTANCE, info, objects);
            }
            // fall through
            case GET_DEFAULT_INSTANCE: {
              return DEFAULT_INSTANCE;
            }
            case GET_PARSER: {
              com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> parser = PARSER;
              if (parser == null) {
                synchronized (inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.class) {
                  parser = PARSER;
                  if (parser == null) {
                    parser =
                        new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator>(
                            DEFAULT_INSTANCE);
                    PARSER = parser;
                  }
                }
              }
              return parser;
          }
          case GET_MEMOIZED_IS_INITIALIZED: {
            return (byte) 1;
          }
          case SET_MEMOIZED_IS_INITIALIZED: {
            return null;
          }
          }
          throw new UnsupportedOperationException();
        }


        // @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator)
        private static final inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator DEFAULT_INSTANCE;
        static {
          Accelerator defaultInstance = new Accelerator();
          // New instances are implicitly immutable so no need to make
          // immutable.
          DEFAULT_INSTANCE = defaultInstance;
          com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
            Accelerator.class, defaultInstance);
        }

        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator getDefaultInstance() {
          return DEFAULT_INSTANCE;
        }

        private static volatile com.google.protobuf.Parser<Accelerator> PARSER;

        public static com.google.protobuf.Parser<Accelerator> parser() {
          return DEFAULT_INSTANCE.getParserForType();
        }
      }

      public static final int GPU_EXECUTION_ACCELERATOR_FIELD_NUMBER = 1;
      private com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> gpuExecutionAccelerator_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
       *&#64;&#64;       "auto_mixed_precision", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
       *&#64;&#64;       the model will try to use FP16 for better performance.
       *&#64;&#64;       This optimization can not be set with "tensorrt".
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      @java.lang.Override
      public java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> getGpuExecutionAcceleratorList() {
        return gpuExecutionAccelerator_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
       *&#64;&#64;       "auto_mixed_precision", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
       *&#64;&#64;       the model will try to use FP16 for better performance.
       *&#64;&#64;       This optimization can not be set with "tensorrt".
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      public java.util.List<? extends inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder> 
          getGpuExecutionAcceleratorOrBuilderList() {
        return gpuExecutionAccelerator_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
       *&#64;&#64;       "auto_mixed_precision", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
       *&#64;&#64;       the model will try to use FP16 for better performance.
       *&#64;&#64;       This optimization can not be set with "tensorrt".
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      @java.lang.Override
      public int getGpuExecutionAcceleratorCount() {
        return gpuExecutionAccelerator_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
       *&#64;&#64;       "auto_mixed_precision", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
       *&#64;&#64;       the model will try to use FP16 for better performance.
       *&#64;&#64;       This optimization can not be set with "tensorrt".
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator getGpuExecutionAccelerator(int index) {
        return gpuExecutionAccelerator_.get(index);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
       *&#64;&#64;       "auto_mixed_precision", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
       *&#64;&#64;       the model will try to use FP16 for better performance.
       *&#64;&#64;       This optimization can not be set with "tensorrt".
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder getGpuExecutionAcceleratorOrBuilder(
          int index) {
        return gpuExecutionAccelerator_.get(index);
      }
      private void ensureGpuExecutionAcceleratorIsMutable() {
        com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> tmp = gpuExecutionAccelerator_;
        if (!tmp.isModifiable()) {
          gpuExecutionAccelerator_ =
              com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
         }
      }

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
       *&#64;&#64;       "auto_mixed_precision", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
       *&#64;&#64;       the model will try to use FP16 for better performance.
       *&#64;&#64;       This optimization can not be set with "tensorrt".
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      private void setGpuExecutionAccelerator(
          int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator value) {
        value.getClass();
  ensureGpuExecutionAcceleratorIsMutable();
        gpuExecutionAccelerator_.set(index, value);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
       *&#64;&#64;       "auto_mixed_precision", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
       *&#64;&#64;       the model will try to use FP16 for better performance.
       *&#64;&#64;       This optimization can not be set with "tensorrt".
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      private void addGpuExecutionAccelerator(inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator value) {
        value.getClass();
  ensureGpuExecutionAcceleratorIsMutable();
        gpuExecutionAccelerator_.add(value);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
       *&#64;&#64;       "auto_mixed_precision", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
       *&#64;&#64;       the model will try to use FP16 for better performance.
       *&#64;&#64;       This optimization can not be set with "tensorrt".
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      private void addGpuExecutionAccelerator(
          int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator value) {
        value.getClass();
  ensureGpuExecutionAcceleratorIsMutable();
        gpuExecutionAccelerator_.add(index, value);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
       *&#64;&#64;       "auto_mixed_precision", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
       *&#64;&#64;       the model will try to use FP16 for better performance.
       *&#64;&#64;       This optimization can not be set with "tensorrt".
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      private void addAllGpuExecutionAccelerator(
          java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> values) {
        ensureGpuExecutionAcceleratorIsMutable();
        com.google.protobuf.AbstractMessageLite.addAll(
            values, gpuExecutionAccelerator_);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
       *&#64;&#64;       "auto_mixed_precision", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
       *&#64;&#64;       the model will try to use FP16 for better performance.
       *&#64;&#64;       This optimization can not be set with "tensorrt".
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      private void clearGpuExecutionAccelerator() {
        gpuExecutionAccelerator_ = emptyProtobufList();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
       *&#64;&#64;       "auto_mixed_precision", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
       *&#64;&#64;       the model will try to use FP16 for better performance.
       *&#64;&#64;       This optimization can not be set with "tensorrt".
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      private void removeGpuExecutionAccelerator(int index) {
        ensureGpuExecutionAcceleratorIsMutable();
        gpuExecutionAccelerator_.remove(index);
      }

      public static final int CPU_EXECUTION_ACCELERATOR_FIELD_NUMBER = 2;
      private com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> cpuExecutionAccelerator_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      @java.lang.Override
      public java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> getCpuExecutionAcceleratorList() {
        return cpuExecutionAccelerator_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      public java.util.List<? extends inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder> 
          getCpuExecutionAcceleratorOrBuilderList() {
        return cpuExecutionAccelerator_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      @java.lang.Override
      public int getCpuExecutionAcceleratorCount() {
        return cpuExecutionAccelerator_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator getCpuExecutionAccelerator(int index) {
        return cpuExecutionAccelerator_.get(index);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder getCpuExecutionAcceleratorOrBuilder(
          int index) {
        return cpuExecutionAccelerator_.get(index);
      }
      private void ensureCpuExecutionAcceleratorIsMutable() {
        com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> tmp = cpuExecutionAccelerator_;
        if (!tmp.isModifiable()) {
          cpuExecutionAccelerator_ =
              com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
         }
      }

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      private void setCpuExecutionAccelerator(
          int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator value) {
        value.getClass();
  ensureCpuExecutionAcceleratorIsMutable();
        cpuExecutionAccelerator_.set(index, value);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      private void addCpuExecutionAccelerator(inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator value) {
        value.getClass();
  ensureCpuExecutionAcceleratorIsMutable();
        cpuExecutionAccelerator_.add(value);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      private void addCpuExecutionAccelerator(
          int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator value) {
        value.getClass();
  ensureCpuExecutionAcceleratorIsMutable();
        cpuExecutionAccelerator_.add(index, value);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      private void addAllCpuExecutionAccelerator(
          java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> values) {
        ensureCpuExecutionAcceleratorIsMutable();
        com.google.protobuf.AbstractMessageLite.addAll(
            values, cpuExecutionAccelerator_);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      private void clearCpuExecutionAccelerator() {
        cpuExecutionAccelerator_ = emptyProtobufList();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      private void removeCpuExecutionAccelerator(int index) {
        ensureCpuExecutionAcceleratorIsMutable();
        cpuExecutionAccelerator_.remove(index);
      }

      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }

      public static Builder newBuilder() {
        return (Builder) DEFAULT_INSTANCE.createBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators prototype) {
        return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
      }

      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: message ExecutionAccelerators
       *&#64;&#64;
       *&#64;&#64;     Specify the preferred execution accelerators to be used to execute
       *&#64;&#64;     the model. Currently only recognized by ONNX Runtime backend and
       *&#64;&#64;     TensorFlow backend.
       *&#64;&#64;
       *&#64;&#64;     For ONNX Runtime backend, it will deploy the model with the execution
       *&#64;&#64;     accelerators by priority, the priority is determined based on the
       *&#64;&#64;     order that they are set, i.e. the provider at the front has highest
       *&#64;&#64;     priority. Overall, the priority will be in the following order:
       *&#64;&#64;         &lt;gpu_execution_accelerator&gt; (if instance is on GPU)
       *&#64;&#64;         CUDA Execution Provider     (if instance is on GPU)
       *&#64;&#64;         &lt;cpu_execution_accelerator&gt;
       *&#64;&#64;         Default CPU Execution Provider
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelOptimizationPolicy.ExecutionAccelerators}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageLite.Builder<
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators, Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelOptimizationPolicy.ExecutionAccelerators)
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAcceleratorsOrBuilder {
        // Construct using inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.newBuilder()
        private Builder() {
          super(DEFAULT_INSTANCE);
        }


        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        @java.lang.Override
        public java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> getGpuExecutionAcceleratorList() {
          return java.util.Collections.unmodifiableList(
              instance.getGpuExecutionAcceleratorList());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        @java.lang.Override
        public int getGpuExecutionAcceleratorCount() {
          return instance.getGpuExecutionAcceleratorCount();
        }/**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator getGpuExecutionAccelerator(int index) {
          return instance.getGpuExecutionAccelerator(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder setGpuExecutionAccelerator(
            int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator value) {
          copyOnWrite();
          instance.setGpuExecutionAccelerator(index, value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder setGpuExecutionAccelerator(
            int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder builderForValue) {
          copyOnWrite();
          instance.setGpuExecutionAccelerator(index,
              builderForValue.build());
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder addGpuExecutionAccelerator(inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator value) {
          copyOnWrite();
          instance.addGpuExecutionAccelerator(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder addGpuExecutionAccelerator(
            int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator value) {
          copyOnWrite();
          instance.addGpuExecutionAccelerator(index, value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder addGpuExecutionAccelerator(
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder builderForValue) {
          copyOnWrite();
          instance.addGpuExecutionAccelerator(builderForValue.build());
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder addGpuExecutionAccelerator(
            int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder builderForValue) {
          copyOnWrite();
          instance.addGpuExecutionAccelerator(index,
              builderForValue.build());
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder addAllGpuExecutionAccelerator(
            java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> values) {
          copyOnWrite();
          instance.addAllGpuExecutionAccelerator(values);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder clearGpuExecutionAccelerator() {
          copyOnWrite();
          instance.clearGpuExecutionAccelerator();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder removeGpuExecutionAccelerator(int index) {
          copyOnWrite();
          instance.removeGpuExecutionAccelerator(index);
          return this;
        }

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        @java.lang.Override
        public java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> getCpuExecutionAcceleratorList() {
          return java.util.Collections.unmodifiableList(
              instance.getCpuExecutionAcceleratorList());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        @java.lang.Override
        public int getCpuExecutionAcceleratorCount() {
          return instance.getCpuExecutionAcceleratorCount();
        }/**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator getCpuExecutionAccelerator(int index) {
          return instance.getCpuExecutionAccelerator(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder setCpuExecutionAccelerator(
            int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator value) {
          copyOnWrite();
          instance.setCpuExecutionAccelerator(index, value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder setCpuExecutionAccelerator(
            int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder builderForValue) {
          copyOnWrite();
          instance.setCpuExecutionAccelerator(index,
              builderForValue.build());
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder addCpuExecutionAccelerator(inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator value) {
          copyOnWrite();
          instance.addCpuExecutionAccelerator(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder addCpuExecutionAccelerator(
            int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator value) {
          copyOnWrite();
          instance.addCpuExecutionAccelerator(index, value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder addCpuExecutionAccelerator(
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder builderForValue) {
          copyOnWrite();
          instance.addCpuExecutionAccelerator(builderForValue.build());
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder addCpuExecutionAccelerator(
            int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder builderForValue) {
          copyOnWrite();
          instance.addCpuExecutionAccelerator(index,
              builderForValue.build());
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder addAllCpuExecutionAccelerator(
            java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> values) {
          copyOnWrite();
          instance.addAllCpuExecutionAccelerator(values);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder clearCpuExecutionAccelerator() {
          copyOnWrite();
          instance.clearCpuExecutionAccelerator();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder removeCpuExecutionAccelerator(int index) {
          copyOnWrite();
          instance.removeCpuExecutionAccelerator(index);
          return this;
        }

        // @@protoc_insertion_point(builder_scope:inference.ModelOptimizationPolicy.ExecutionAccelerators)
      }
      @java.lang.Override
      @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
      protected final java.lang.Object dynamicMethod(
          com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
          java.lang.Object arg0, java.lang.Object arg1) {
        switch (method) {
          case NEW_MUTABLE_INSTANCE: {
            return new inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators();
          }
          case NEW_BUILDER: {
            return new Builder();
          }
          case BUILD_MESSAGE_INFO: {
              java.lang.Object[] objects = new java.lang.Object[] {
                "gpuExecutionAccelerator_",
                inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.class,
                "cpuExecutionAccelerator_",
                inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.class,
              };
              java.lang.String info =
                  "\u0000\u0002\u0000\u0000\u0001\u0002\u0002\u0000\u0002\u0000\u0001\u001b\u0002\u001b" +
                  "";
              return newMessageInfo(DEFAULT_INSTANCE, info, objects);
          }
          // fall through
          case GET_DEFAULT_INSTANCE: {
            return DEFAULT_INSTANCE;
          }
          case GET_PARSER: {
            com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators> parser = PARSER;
            if (parser == null) {
              synchronized (inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.class) {
                parser = PARSER;
                if (parser == null) {
                  parser =
                      new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators>(
                          DEFAULT_INSTANCE);
                  PARSER = parser;
                }
              }
            }
            return parser;
        }
        case GET_MEMOIZED_IS_INITIALIZED: {
          return (byte) 1;
        }
        case SET_MEMOIZED_IS_INITIALIZED: {
          return null;
        }
        }
        throw new UnsupportedOperationException();
      }


      // @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.ExecutionAccelerators)
      private static final inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators DEFAULT_INSTANCE;
      static {
        ExecutionAccelerators defaultInstance = new ExecutionAccelerators();
        // New instances are implicitly immutable so no need to make
        // immutable.
        DEFAULT_INSTANCE = defaultInstance;
        com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
          ExecutionAccelerators.class, defaultInstance);
      }

      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static volatile com.google.protobuf.Parser<ExecutionAccelerators> PARSER;

      public static com.google.protobuf.Parser<ExecutionAccelerators> parser() {
        return DEFAULT_INSTANCE.getParserForType();
      }
    }

    public interface PinnedMemoryBufferOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelOptimizationPolicy.PinnedMemoryBuffer)
        com.google.protobuf.MessageLiteOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool enable
       *&#64;&#64;
       *&#64;&#64;       Use pinned memory buffer. Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool enable = 1;</code>
       * @return The enable.
       */
      boolean getEnable();
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: message PinnedMemoryBuffer
     *&#64;&#64;
     *&#64;&#64;     Specify whether to use a pinned memory buffer when transferring data
     *&#64;&#64;     between non-pinned system memory and GPU memory. Using a pinned
     *&#64;&#64;     memory buffer for system from/to GPU transfers will typically provide
     *&#64;&#64;     increased performance. For example, in the common use case where the
     *&#64;&#64;     request provides inputs and delivers outputs via non-pinned system
     *&#64;&#64;     memory, if the model instance accepts GPU IOs, the inputs will be
     *&#64;&#64;     processed by two copies: from non-pinned system memory to pinned
     *&#64;&#64;     memory, and from pinned memory to GPU memory. Similarly, pinned
     *&#64;&#64;     memory will be used for delivering the outputs.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelOptimizationPolicy.PinnedMemoryBuffer}
     */
    public  static final class PinnedMemoryBuffer extends
        com.google.protobuf.GeneratedMessageLite<
            PinnedMemoryBuffer, PinnedMemoryBuffer.Builder> implements
        // @@protoc_insertion_point(message_implements:inference.ModelOptimizationPolicy.PinnedMemoryBuffer)
        PinnedMemoryBufferOrBuilder {
      private PinnedMemoryBuffer() {
      }
      public static final int ENABLE_FIELD_NUMBER = 1;
      private boolean enable_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool enable
       *&#64;&#64;
       *&#64;&#64;       Use pinned memory buffer. Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool enable = 1;</code>
       * @return The enable.
       */
      @java.lang.Override
      public boolean getEnable() {
        return enable_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool enable
       *&#64;&#64;
       *&#64;&#64;       Use pinned memory buffer. Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool enable = 1;</code>
       * @param value The enable to set.
       */
      private void setEnable(boolean value) {
        
        enable_ = value;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool enable
       *&#64;&#64;
       *&#64;&#64;       Use pinned memory buffer. Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool enable = 1;</code>
       */
      private void clearEnable() {
        
        enable_ = false;
      }

      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }

      public static Builder newBuilder() {
        return (Builder) DEFAULT_INSTANCE.createBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer prototype) {
        return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
      }

      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: message PinnedMemoryBuffer
       *&#64;&#64;
       *&#64;&#64;     Specify whether to use a pinned memory buffer when transferring data
       *&#64;&#64;     between non-pinned system memory and GPU memory. Using a pinned
       *&#64;&#64;     memory buffer for system from/to GPU transfers will typically provide
       *&#64;&#64;     increased performance. For example, in the common use case where the
       *&#64;&#64;     request provides inputs and delivers outputs via non-pinned system
       *&#64;&#64;     memory, if the model instance accepts GPU IOs, the inputs will be
       *&#64;&#64;     processed by two copies: from non-pinned system memory to pinned
       *&#64;&#64;     memory, and from pinned memory to GPU memory. Similarly, pinned
       *&#64;&#64;     memory will be used for delivering the outputs.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelOptimizationPolicy.PinnedMemoryBuffer}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageLite.Builder<
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer, Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelOptimizationPolicy.PinnedMemoryBuffer)
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBufferOrBuilder {
        // Construct using inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.newBuilder()
        private Builder() {
          super(DEFAULT_INSTANCE);
        }


        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool enable
         *&#64;&#64;
         *&#64;&#64;       Use pinned memory buffer. Default is true.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool enable = 1;</code>
         * @return The enable.
         */
        @java.lang.Override
        public boolean getEnable() {
          return instance.getEnable();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool enable
         *&#64;&#64;
         *&#64;&#64;       Use pinned memory buffer. Default is true.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool enable = 1;</code>
         * @param value The enable to set.
         * @return This builder for chaining.
         */
        public Builder setEnable(boolean value) {
          copyOnWrite();
          instance.setEnable(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool enable
         *&#64;&#64;
         *&#64;&#64;       Use pinned memory buffer. Default is true.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool enable = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearEnable() {
          copyOnWrite();
          instance.clearEnable();
          return this;
        }

        // @@protoc_insertion_point(builder_scope:inference.ModelOptimizationPolicy.PinnedMemoryBuffer)
      }
      @java.lang.Override
      @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
      protected final java.lang.Object dynamicMethod(
          com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
          java.lang.Object arg0, java.lang.Object arg1) {
        switch (method) {
          case NEW_MUTABLE_INSTANCE: {
            return new inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer();
          }
          case NEW_BUILDER: {
            return new Builder();
          }
          case BUILD_MESSAGE_INFO: {
              java.lang.Object[] objects = new java.lang.Object[] {
                "enable_",
              };
              java.lang.String info =
                  "\u0000\u0001\u0000\u0000\u0001\u0001\u0001\u0000\u0000\u0000\u0001\u0007";
              return newMessageInfo(DEFAULT_INSTANCE, info, objects);
          }
          // fall through
          case GET_DEFAULT_INSTANCE: {
            return DEFAULT_INSTANCE;
          }
          case GET_PARSER: {
            com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer> parser = PARSER;
            if (parser == null) {
              synchronized (inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.class) {
                parser = PARSER;
                if (parser == null) {
                  parser =
                      new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer>(
                          DEFAULT_INSTANCE);
                  PARSER = parser;
                }
              }
            }
            return parser;
        }
        case GET_MEMOIZED_IS_INITIALIZED: {
          return (byte) 1;
        }
        case SET_MEMOIZED_IS_INITIALIZED: {
          return null;
        }
        }
        throw new UnsupportedOperationException();
      }


      // @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.PinnedMemoryBuffer)
      private static final inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer DEFAULT_INSTANCE;
      static {
        PinnedMemoryBuffer defaultInstance = new PinnedMemoryBuffer();
        // New instances are implicitly immutable so no need to make
        // immutable.
        DEFAULT_INSTANCE = defaultInstance;
        com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
          PinnedMemoryBuffer.class, defaultInstance);
      }

      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static volatile com.google.protobuf.Parser<PinnedMemoryBuffer> PARSER;

      public static com.google.protobuf.Parser<PinnedMemoryBuffer> parser() {
        return DEFAULT_INSTANCE.getParserForType();
      }
    }

    public static final int GRAPH_FIELD_NUMBER = 1;
    private inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph graph_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
     */
    @java.lang.Override
    public boolean hasGraph() {
      return graph_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph getGraph() {
      return graph_ == null ? inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.getDefaultInstance() : graph_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
     */
    private void setGraph(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph value) {
      value.getClass();
  graph_ = value;
      
      }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
     */
    @java.lang.SuppressWarnings({"ReferenceEquality"})
    private void mergeGraph(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph value) {
      value.getClass();
  if (graph_ != null &&
          graph_ != inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.getDefaultInstance()) {
        graph_ =
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.newBuilder(graph_).mergeFrom(value).buildPartial();
      } else {
        graph_ = value;
      }
      
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
     */
    private void clearGraph() {  graph_ = null;
      
    }

    public static final int PRIORITY_FIELD_NUMBER = 2;
    private int priority_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelPriority priority
     *&#64;&#64;
     *&#64;&#64;     The priority setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
     * @return The enum numeric value on the wire for priority.
     */
    @java.lang.Override
    public int getPriorityValue() {
      return priority_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelPriority priority
     *&#64;&#64;
     *&#64;&#64;     The priority setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
     * @return The priority.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority getPriority() {
      inference.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority result = inference.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority.forNumber(priority_);
      return result == null ? inference.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority.UNRECOGNIZED : result;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelPriority priority
     *&#64;&#64;
     *&#64;&#64;     The priority setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
     * @param value The enum numeric value on the wire for priority to set.
     */
    private void setPriorityValue(int value) {
        priority_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelPriority priority
     *&#64;&#64;
     *&#64;&#64;     The priority setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
     * @param value The priority to set.
     */
    private void setPriority(inference.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority value) {
      priority_ = value.getNumber();
      
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelPriority priority
     *&#64;&#64;
     *&#64;&#64;     The priority setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
     */
    private void clearPriority() {
      
      priority_ = 0;
    }

    public static final int CUDA_FIELD_NUMBER = 3;
    private inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda cuda_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     */
    @java.lang.Override
    public boolean hasCuda() {
      return cuda_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda getCuda() {
      return cuda_ == null ? inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.getDefaultInstance() : cuda_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     */
    private void setCuda(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda value) {
      value.getClass();
  cuda_ = value;
      
      }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     */
    @java.lang.SuppressWarnings({"ReferenceEquality"})
    private void mergeCuda(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda value) {
      value.getClass();
  if (cuda_ != null &&
          cuda_ != inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.getDefaultInstance()) {
        cuda_ =
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.newBuilder(cuda_).mergeFrom(value).buildPartial();
      } else {
        cuda_ = value;
      }
      
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     */
    private void clearCuda() {  cuda_ = null;
      
    }

    public static final int EXECUTION_ACCELERATORS_FIELD_NUMBER = 4;
    private inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators executionAccelerators_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *&#64;&#64;
     *&#64;&#64;     The accelerators used for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
     */
    @java.lang.Override
    public boolean hasExecutionAccelerators() {
      return executionAccelerators_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *&#64;&#64;
     *&#64;&#64;     The accelerators used for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators getExecutionAccelerators() {
      return executionAccelerators_ == null ? inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.getDefaultInstance() : executionAccelerators_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *&#64;&#64;
     *&#64;&#64;     The accelerators used for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
     */
    private void setExecutionAccelerators(inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators value) {
      value.getClass();
  executionAccelerators_ = value;
      
      }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *&#64;&#64;
     *&#64;&#64;     The accelerators used for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
     */
    @java.lang.SuppressWarnings({"ReferenceEquality"})
    private void mergeExecutionAccelerators(inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators value) {
      value.getClass();
  if (executionAccelerators_ != null &&
          executionAccelerators_ != inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.getDefaultInstance()) {
        executionAccelerators_ =
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.newBuilder(executionAccelerators_).mergeFrom(value).buildPartial();
      } else {
        executionAccelerators_ = value;
      }
      
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *&#64;&#64;
     *&#64;&#64;     The accelerators used for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
     */
    private void clearExecutionAccelerators() {  executionAccelerators_ = null;
      
    }

    public static final int INPUT_PINNED_MEMORY_FIELD_NUMBER = 5;
    private inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer inputPinnedMemory_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
     */
    @java.lang.Override
    public boolean hasInputPinnedMemory() {
      return inputPinnedMemory_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer getInputPinnedMemory() {
      return inputPinnedMemory_ == null ? inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.getDefaultInstance() : inputPinnedMemory_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
     */
    private void setInputPinnedMemory(inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer value) {
      value.getClass();
  inputPinnedMemory_ = value;
      
      }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
     */
    @java.lang.SuppressWarnings({"ReferenceEquality"})
    private void mergeInputPinnedMemory(inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer value) {
      value.getClass();
  if (inputPinnedMemory_ != null &&
          inputPinnedMemory_ != inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.getDefaultInstance()) {
        inputPinnedMemory_ =
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.newBuilder(inputPinnedMemory_).mergeFrom(value).buildPartial();
      } else {
        inputPinnedMemory_ = value;
      }
      
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
     */
    private void clearInputPinnedMemory() {  inputPinnedMemory_ = null;
      
    }

    public static final int OUTPUT_PINNED_MEMORY_FIELD_NUMBER = 6;
    private inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer outputPinnedMemory_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
     */
    @java.lang.Override
    public boolean hasOutputPinnedMemory() {
      return outputPinnedMemory_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer getOutputPinnedMemory() {
      return outputPinnedMemory_ == null ? inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.getDefaultInstance() : outputPinnedMemory_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
     */
    private void setOutputPinnedMemory(inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer value) {
      value.getClass();
  outputPinnedMemory_ = value;
      
      }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
     */
    @java.lang.SuppressWarnings({"ReferenceEquality"})
    private void mergeOutputPinnedMemory(inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer value) {
      value.getClass();
  if (outputPinnedMemory_ != null &&
          outputPinnedMemory_ != inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.getDefaultInstance()) {
        outputPinnedMemory_ =
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.newBuilder(outputPinnedMemory_).mergeFrom(value).buildPartial();
      } else {
        outputPinnedMemory_ = value;
      }
      
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
     */
    private void clearOutputPinnedMemory() {  outputPinnedMemory_ = null;
      
    }

    public static final int GATHER_KERNEL_BUFFER_THRESHOLD_FIELD_NUMBER = 7;
    private int gatherKernelBufferThreshold_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 gather_kernel_buffer_threshold
     *&#64;&#64;
     *&#64;&#64;     The backend may use a gather kernel to gather input data if the
     *&#64;&#64;     device has direct access to the source buffer and the destination
     *&#64;&#64;     buffer. In such case, the gather kernel will be used only if the
     *&#64;&#64;     number of buffers to be gathered is greater or equal to
     *&#64;&#64;     the specifed value. If 0, the gather kernel will be disabled.
     *&#64;&#64;     Default value is 0.
     *&#64;&#64;     Currently only recognized by TensorRT backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 gather_kernel_buffer_threshold = 7;</code>
     * @return The gatherKernelBufferThreshold.
     */
    @java.lang.Override
    public int getGatherKernelBufferThreshold() {
      return gatherKernelBufferThreshold_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 gather_kernel_buffer_threshold
     *&#64;&#64;
     *&#64;&#64;     The backend may use a gather kernel to gather input data if the
     *&#64;&#64;     device has direct access to the source buffer and the destination
     *&#64;&#64;     buffer. In such case, the gather kernel will be used only if the
     *&#64;&#64;     number of buffers to be gathered is greater or equal to
     *&#64;&#64;     the specifed value. If 0, the gather kernel will be disabled.
     *&#64;&#64;     Default value is 0.
     *&#64;&#64;     Currently only recognized by TensorRT backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 gather_kernel_buffer_threshold = 7;</code>
     * @param value The gatherKernelBufferThreshold to set.
     */
    private void setGatherKernelBufferThreshold(int value) {
      
      gatherKernelBufferThreshold_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 gather_kernel_buffer_threshold
     *&#64;&#64;
     *&#64;&#64;     The backend may use a gather kernel to gather input data if the
     *&#64;&#64;     device has direct access to the source buffer and the destination
     *&#64;&#64;     buffer. In such case, the gather kernel will be used only if the
     *&#64;&#64;     number of buffers to be gathered is greater or equal to
     *&#64;&#64;     the specifed value. If 0, the gather kernel will be disabled.
     *&#64;&#64;     Default value is 0.
     *&#64;&#64;     Currently only recognized by TensorRT backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 gather_kernel_buffer_threshold = 7;</code>
     */
    private void clearGatherKernelBufferThreshold() {
      
      gatherKernelBufferThreshold_ = 0;
    }

    public static final int EAGER_BATCHING_FIELD_NUMBER = 8;
    private boolean eagerBatching_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool eager_batching
     *&#64;&#64;
     *&#64;&#64;     Start preparing the next batch before the model instance is ready
     *&#64;&#64;     for the next inference. This option can be used to overlap the
     *&#64;&#64;     batch preparation with model execution, with the trade-off that
     *&#64;&#64;     the next batch might be smaller than what it could have been.
     *&#64;&#64;     Default value is false.
     *&#64;&#64;     Currently only recognized by TensorRT backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool eager_batching = 8;</code>
     * @return The eagerBatching.
     */
    @java.lang.Override
    public boolean getEagerBatching() {
      return eagerBatching_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool eager_batching
     *&#64;&#64;
     *&#64;&#64;     Start preparing the next batch before the model instance is ready
     *&#64;&#64;     for the next inference. This option can be used to overlap the
     *&#64;&#64;     batch preparation with model execution, with the trade-off that
     *&#64;&#64;     the next batch might be smaller than what it could have been.
     *&#64;&#64;     Default value is false.
     *&#64;&#64;     Currently only recognized by TensorRT backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool eager_batching = 8;</code>
     * @param value The eagerBatching to set.
     */
    private void setEagerBatching(boolean value) {
      
      eagerBatching_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool eager_batching
     *&#64;&#64;
     *&#64;&#64;     Start preparing the next batch before the model instance is ready
     *&#64;&#64;     for the next inference. This option can be used to overlap the
     *&#64;&#64;     batch preparation with model execution, with the trade-off that
     *&#64;&#64;     the next batch might be smaller than what it could have been.
     *&#64;&#64;     Default value is false.
     *&#64;&#64;     Currently only recognized by TensorRT backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool eager_batching = 8;</code>
     */
    private void clearEagerBatching() {
      
      eagerBatching_ = false;
    }

    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }

    public static Builder newBuilder() {
      return (Builder) DEFAULT_INSTANCE.createBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelOptimizationPolicy prototype) {
      return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelOptimizationPolicy
     *&#64;&#64;
     *&#64;&#64;   Optimization settings for a model. These settings control if/how a
     *&#64;&#64;   model is optimized and prioritized by the backend framework when
     *&#64;&#64;   it is loaded.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelOptimizationPolicy}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageLite.Builder<
          inference.ModelConfigOuterClass.ModelOptimizationPolicy, Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelOptimizationPolicy)
        inference.ModelConfigOuterClass.ModelOptimizationPolicyOrBuilder {
      // Construct using inference.ModelConfigOuterClass.ModelOptimizationPolicy.newBuilder()
      private Builder() {
        super(DEFAULT_INSTANCE);
      }


      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      @java.lang.Override
      public boolean hasGraph() {
        return instance.hasGraph();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph getGraph() {
        return instance.getGraph();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public Builder setGraph(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph value) {
        copyOnWrite();
        instance.setGraph(value);
        return this;
        }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public Builder setGraph(
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.Builder builderForValue) {
        copyOnWrite();
        instance.setGraph(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public Builder mergeGraph(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph value) {
        copyOnWrite();
        instance.mergeGraph(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public Builder clearGraph() {  copyOnWrite();
        instance.clearGraph();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelPriority priority
       *&#64;&#64;
       *&#64;&#64;     The priority setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
       * @return The enum numeric value on the wire for priority.
       */
      @java.lang.Override
      public int getPriorityValue() {
        return instance.getPriorityValue();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelPriority priority
       *&#64;&#64;
       *&#64;&#64;     The priority setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
       * @param value The priority to set.
       * @return This builder for chaining.
       */
      public Builder setPriorityValue(int value) {
        copyOnWrite();
        instance.setPriorityValue(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelPriority priority
       *&#64;&#64;
       *&#64;&#64;     The priority setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
       * @return The priority.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority getPriority() {
        return instance.getPriority();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelPriority priority
       *&#64;&#64;
       *&#64;&#64;     The priority setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
       * @param value The enum numeric value on the wire for priority to set.
       * @return This builder for chaining.
       */
      public Builder setPriority(inference.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority value) {
        copyOnWrite();
        instance.setPriority(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelPriority priority
       *&#64;&#64;
       *&#64;&#64;     The priority setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearPriority() {
        copyOnWrite();
        instance.clearPriority();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      @java.lang.Override
      public boolean hasCuda() {
        return instance.hasCuda();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda getCuda() {
        return instance.getCuda();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public Builder setCuda(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda value) {
        copyOnWrite();
        instance.setCuda(value);
        return this;
        }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public Builder setCuda(
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.Builder builderForValue) {
        copyOnWrite();
        instance.setCuda(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public Builder mergeCuda(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda value) {
        copyOnWrite();
        instance.mergeCuda(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public Builder clearCuda() {  copyOnWrite();
        instance.clearCuda();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
       *&#64;&#64;
       *&#64;&#64;     The accelerators used for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
       */
      @java.lang.Override
      public boolean hasExecutionAccelerators() {
        return instance.hasExecutionAccelerators();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
       *&#64;&#64;
       *&#64;&#64;     The accelerators used for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators getExecutionAccelerators() {
        return instance.getExecutionAccelerators();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
       *&#64;&#64;
       *&#64;&#64;     The accelerators used for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
       */
      public Builder setExecutionAccelerators(inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators value) {
        copyOnWrite();
        instance.setExecutionAccelerators(value);
        return this;
        }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
       *&#64;&#64;
       *&#64;&#64;     The accelerators used for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
       */
      public Builder setExecutionAccelerators(
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Builder builderForValue) {
        copyOnWrite();
        instance.setExecutionAccelerators(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
       *&#64;&#64;
       *&#64;&#64;     The accelerators used for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
       */
      public Builder mergeExecutionAccelerators(inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators value) {
        copyOnWrite();
        instance.mergeExecutionAccelerators(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
       *&#64;&#64;
       *&#64;&#64;     The accelerators used for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
       */
      public Builder clearExecutionAccelerators() {  copyOnWrite();
        instance.clearExecutionAccelerators();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
       */
      @java.lang.Override
      public boolean hasInputPinnedMemory() {
        return instance.hasInputPinnedMemory();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer getInputPinnedMemory() {
        return instance.getInputPinnedMemory();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
       */
      public Builder setInputPinnedMemory(inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer value) {
        copyOnWrite();
        instance.setInputPinnedMemory(value);
        return this;
        }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
       */
      public Builder setInputPinnedMemory(
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.Builder builderForValue) {
        copyOnWrite();
        instance.setInputPinnedMemory(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
       */
      public Builder mergeInputPinnedMemory(inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer value) {
        copyOnWrite();
        instance.mergeInputPinnedMemory(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
       */
      public Builder clearInputPinnedMemory() {  copyOnWrite();
        instance.clearInputPinnedMemory();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
       */
      @java.lang.Override
      public boolean hasOutputPinnedMemory() {
        return instance.hasOutputPinnedMemory();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer getOutputPinnedMemory() {
        return instance.getOutputPinnedMemory();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
       */
      public Builder setOutputPinnedMemory(inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer value) {
        copyOnWrite();
        instance.setOutputPinnedMemory(value);
        return this;
        }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
       */
      public Builder setOutputPinnedMemory(
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.Builder builderForValue) {
        copyOnWrite();
        instance.setOutputPinnedMemory(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
       */
      public Builder mergeOutputPinnedMemory(inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer value) {
        copyOnWrite();
        instance.mergeOutputPinnedMemory(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
       */
      public Builder clearOutputPinnedMemory() {  copyOnWrite();
        instance.clearOutputPinnedMemory();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 gather_kernel_buffer_threshold
       *&#64;&#64;
       *&#64;&#64;     The backend may use a gather kernel to gather input data if the
       *&#64;&#64;     device has direct access to the source buffer and the destination
       *&#64;&#64;     buffer. In such case, the gather kernel will be used only if the
       *&#64;&#64;     number of buffers to be gathered is greater or equal to
       *&#64;&#64;     the specifed value. If 0, the gather kernel will be disabled.
       *&#64;&#64;     Default value is 0.
       *&#64;&#64;     Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 gather_kernel_buffer_threshold = 7;</code>
       * @return The gatherKernelBufferThreshold.
       */
      @java.lang.Override
      public int getGatherKernelBufferThreshold() {
        return instance.getGatherKernelBufferThreshold();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 gather_kernel_buffer_threshold
       *&#64;&#64;
       *&#64;&#64;     The backend may use a gather kernel to gather input data if the
       *&#64;&#64;     device has direct access to the source buffer and the destination
       *&#64;&#64;     buffer. In such case, the gather kernel will be used only if the
       *&#64;&#64;     number of buffers to be gathered is greater or equal to
       *&#64;&#64;     the specifed value. If 0, the gather kernel will be disabled.
       *&#64;&#64;     Default value is 0.
       *&#64;&#64;     Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 gather_kernel_buffer_threshold = 7;</code>
       * @param value The gatherKernelBufferThreshold to set.
       * @return This builder for chaining.
       */
      public Builder setGatherKernelBufferThreshold(int value) {
        copyOnWrite();
        instance.setGatherKernelBufferThreshold(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 gather_kernel_buffer_threshold
       *&#64;&#64;
       *&#64;&#64;     The backend may use a gather kernel to gather input data if the
       *&#64;&#64;     device has direct access to the source buffer and the destination
       *&#64;&#64;     buffer. In such case, the gather kernel will be used only if the
       *&#64;&#64;     number of buffers to be gathered is greater or equal to
       *&#64;&#64;     the specifed value. If 0, the gather kernel will be disabled.
       *&#64;&#64;     Default value is 0.
       *&#64;&#64;     Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 gather_kernel_buffer_threshold = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearGatherKernelBufferThreshold() {
        copyOnWrite();
        instance.clearGatherKernelBufferThreshold();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool eager_batching
       *&#64;&#64;
       *&#64;&#64;     Start preparing the next batch before the model instance is ready
       *&#64;&#64;     for the next inference. This option can be used to overlap the
       *&#64;&#64;     batch preparation with model execution, with the trade-off that
       *&#64;&#64;     the next batch might be smaller than what it could have been.
       *&#64;&#64;     Default value is false.
       *&#64;&#64;     Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool eager_batching = 8;</code>
       * @return The eagerBatching.
       */
      @java.lang.Override
      public boolean getEagerBatching() {
        return instance.getEagerBatching();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool eager_batching
       *&#64;&#64;
       *&#64;&#64;     Start preparing the next batch before the model instance is ready
       *&#64;&#64;     for the next inference. This option can be used to overlap the
       *&#64;&#64;     batch preparation with model execution, with the trade-off that
       *&#64;&#64;     the next batch might be smaller than what it could have been.
       *&#64;&#64;     Default value is false.
       *&#64;&#64;     Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool eager_batching = 8;</code>
       * @param value The eagerBatching to set.
       * @return This builder for chaining.
       */
      public Builder setEagerBatching(boolean value) {
        copyOnWrite();
        instance.setEagerBatching(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool eager_batching
       *&#64;&#64;
       *&#64;&#64;     Start preparing the next batch before the model instance is ready
       *&#64;&#64;     for the next inference. This option can be used to overlap the
       *&#64;&#64;     batch preparation with model execution, with the trade-off that
       *&#64;&#64;     the next batch might be smaller than what it could have been.
       *&#64;&#64;     Default value is false.
       *&#64;&#64;     Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool eager_batching = 8;</code>
       * @return This builder for chaining.
       */
      public Builder clearEagerBatching() {
        copyOnWrite();
        instance.clearEagerBatching();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:inference.ModelOptimizationPolicy)
    }
    @java.lang.Override
    @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
    protected final java.lang.Object dynamicMethod(
        com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
        java.lang.Object arg0, java.lang.Object arg1) {
      switch (method) {
        case NEW_MUTABLE_INSTANCE: {
          return new inference.ModelConfigOuterClass.ModelOptimizationPolicy();
        }
        case NEW_BUILDER: {
          return new Builder();
        }
        case BUILD_MESSAGE_INFO: {
            java.lang.Object[] objects = new java.lang.Object[] {
              "graph_",
              "priority_",
              "cuda_",
              "executionAccelerators_",
              "inputPinnedMemory_",
              "outputPinnedMemory_",
              "gatherKernelBufferThreshold_",
              "eagerBatching_",
            };
            java.lang.String info =
                "\u0000\b\u0000\u0000\u0001\b\b\u0000\u0000\u0000\u0001\t\u0002\f\u0003\t\u0004\t" +
                "\u0005\t\u0006\t\u0007\u000b\b\u0007";
            return newMessageInfo(DEFAULT_INSTANCE, info, objects);
        }
        // fall through
        case GET_DEFAULT_INSTANCE: {
          return DEFAULT_INSTANCE;
        }
        case GET_PARSER: {
          com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelOptimizationPolicy> parser = PARSER;
          if (parser == null) {
            synchronized (inference.ModelConfigOuterClass.ModelOptimizationPolicy.class) {
              parser = PARSER;
              if (parser == null) {
                parser =
                    new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelOptimizationPolicy>(
                        DEFAULT_INSTANCE);
                PARSER = parser;
              }
            }
          }
          return parser;
      }
      case GET_MEMOIZED_IS_INITIALIZED: {
        return (byte) 1;
      }
      case SET_MEMOIZED_IS_INITIALIZED: {
        return null;
      }
      }
      throw new UnsupportedOperationException();
    }


    // @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy)
    private static final inference.ModelConfigOuterClass.ModelOptimizationPolicy DEFAULT_INSTANCE;
    static {
      ModelOptimizationPolicy defaultInstance = new ModelOptimizationPolicy();
      // New instances are implicitly immutable so no need to make
      // immutable.
      DEFAULT_INSTANCE = defaultInstance;
      com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
        ModelOptimizationPolicy.class, defaultInstance);
    }

    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static volatile com.google.protobuf.Parser<ModelOptimizationPolicy> PARSER;

    public static com.google.protobuf.Parser<ModelOptimizationPolicy> parser() {
      return DEFAULT_INSTANCE.getParserForType();
    }
  }

  public interface ModelQueuePolicyOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelQueuePolicy)
      com.google.protobuf.MessageLiteOrBuilder {

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: TimeoutAction timeout_action
     *&#64;&#64;
     *&#64;&#64;     The action applied to timed-out request.
     *&#64;&#64;     The default action is REJECT.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelQueuePolicy.TimeoutAction timeout_action = 1;</code>
     * @return The enum numeric value on the wire for timeoutAction.
     */
    int getTimeoutActionValue();
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: TimeoutAction timeout_action
     *&#64;&#64;
     *&#64;&#64;     The action applied to timed-out request.
     *&#64;&#64;     The default action is REJECT.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelQueuePolicy.TimeoutAction timeout_action = 1;</code>
     * @return The timeoutAction.
     */
    inference.ModelConfigOuterClass.ModelQueuePolicy.TimeoutAction getTimeoutAction();

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: uint64 default_timeout_microseconds
     *&#64;&#64;
     *&#64;&#64;     The default timeout for every request, in microseconds.
     *&#64;&#64;     The default value is 0 which indicates that no timeout is set.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 default_timeout_microseconds = 2;</code>
     * @return The defaultTimeoutMicroseconds.
     */
    long getDefaultTimeoutMicroseconds();

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: bool allow_timeout_override
     *&#64;&#64;
     *&#64;&#64;     Whether individual request can override the default timeout value.
     *&#64;&#64;     When true, individual requests can set a timeout that is less than
     *&#64;&#64;     the default timeout value but may not increase the timeout.
     *&#64;&#64;     The default value is false.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool allow_timeout_override = 3;</code>
     * @return The allowTimeoutOverride.
     */
    boolean getAllowTimeoutOverride();

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: uint32 max_queue_size
     *&#64;&#64;
     *&#64;&#64;     The maximum queue size for holding requests. A request will be
     *&#64;&#64;     rejected immediately if it can't be enqueued because the queue is
     *&#64;&#64;     full. The default value is 0 which indicates that no maximum
     *&#64;&#64;     queue size is enforced.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 max_queue_size = 4;</code>
     * @return The maxQueueSize.
     */
    int getMaxQueueSize();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelQueuePolicy
   *&#64;&#64;
   *&#64;&#64;   Queue policy for inference requests.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelQueuePolicy}
   */
  public  static final class ModelQueuePolicy extends
      com.google.protobuf.GeneratedMessageLite<
          ModelQueuePolicy, ModelQueuePolicy.Builder> implements
      // @@protoc_insertion_point(message_implements:inference.ModelQueuePolicy)
      ModelQueuePolicyOrBuilder {
    private ModelQueuePolicy() {
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:enum:: TimeoutAction
     *&#64;&#64;
     *&#64;&#64;     The action applied to timed-out requests.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf enum {@code inference.ModelQueuePolicy.TimeoutAction}
     */
    public enum TimeoutAction
        implements com.google.protobuf.Internal.EnumLite {
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Action::REJECT = 0
       *&#64;&#64;
       *&#64;&#64;       Reject the request and return error message accordingly.
       *&#64;&#64;
       * </pre>
       *
       * <code>REJECT = 0;</code>
       */
      REJECT(0),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Action::DELAY = 1
       *&#64;&#64;
       *&#64;&#64;       Delay the request until all other requests at the same
       *&#64;&#64;       (or higher) priority levels that have not reached their timeouts
       *&#64;&#64;       are processed. A delayed request will eventually be processed,
       *&#64;&#64;       but may be delayed indefinitely due to newly arriving requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>DELAY = 1;</code>
       */
      DELAY(1),
      UNRECOGNIZED(-1),
      ;

      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Action::REJECT = 0
       *&#64;&#64;
       *&#64;&#64;       Reject the request and return error message accordingly.
       *&#64;&#64;
       * </pre>
       *
       * <code>REJECT = 0;</code>
       */
      public static final int REJECT_VALUE = 0;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Action::DELAY = 1
       *&#64;&#64;
       *&#64;&#64;       Delay the request until all other requests at the same
       *&#64;&#64;       (or higher) priority levels that have not reached their timeouts
       *&#64;&#64;       are processed. A delayed request will eventually be processed,
       *&#64;&#64;       but may be delayed indefinitely due to newly arriving requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>DELAY = 1;</code>
       */
      public static final int DELAY_VALUE = 1;


      @java.lang.Override
      public final int getNumber() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalArgumentException(
              "Can't get the number of an unknown enum value.");
        }
        return value;
      }

      /**
       * @param value The number of the enum to look for.
       * @return The enum associated with the given number.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static TimeoutAction valueOf(int value) {
        return forNumber(value);
      }

      public static TimeoutAction forNumber(int value) {
        switch (value) {
          case 0: return REJECT;
          case 1: return DELAY;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<TimeoutAction>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          TimeoutAction> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<TimeoutAction>() {
              @java.lang.Override
              public TimeoutAction findValueByNumber(int number) {
                return TimeoutAction.forNumber(number);
              }
            };

      public static com.google.protobuf.Internal.EnumVerifier 
          internalGetVerifier() {
        return TimeoutActionVerifier.INSTANCE;
      }

      private static final class TimeoutActionVerifier implements 
           com.google.protobuf.Internal.EnumVerifier { 
              static final com.google.protobuf.Internal.EnumVerifier           INSTANCE = new TimeoutActionVerifier();
              @java.lang.Override
              public boolean isInRange(int number) {
                return TimeoutAction.forNumber(number) != null;
              }
            };

      private final int value;

      private TimeoutAction(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:inference.ModelQueuePolicy.TimeoutAction)
    }

    public static final int TIMEOUT_ACTION_FIELD_NUMBER = 1;
    private int timeoutAction_;
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: TimeoutAction timeout_action
     *&#64;&#64;
     *&#64;&#64;     The action applied to timed-out request.
     *&#64;&#64;     The default action is REJECT.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelQueuePolicy.TimeoutAction timeout_action = 1;</code>
     * @return The enum numeric value on the wire for timeoutAction.
     */
    @java.lang.Override
    public int getTimeoutActionValue() {
      return timeoutAction_;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: TimeoutAction timeout_action
     *&#64;&#64;
     *&#64;&#64;     The action applied to timed-out request.
     *&#64;&#64;     The default action is REJECT.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelQueuePolicy.TimeoutAction timeout_action = 1;</code>
     * @return The timeoutAction.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelQueuePolicy.TimeoutAction getTimeoutAction() {
      inference.ModelConfigOuterClass.ModelQueuePolicy.TimeoutAction result = inference.ModelConfigOuterClass.ModelQueuePolicy.TimeoutAction.forNumber(timeoutAction_);
      return result == null ? inference.ModelConfigOuterClass.ModelQueuePolicy.TimeoutAction.UNRECOGNIZED : result;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: TimeoutAction timeout_action
     *&#64;&#64;
     *&#64;&#64;     The action applied to timed-out request.
     *&#64;&#64;     The default action is REJECT.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelQueuePolicy.TimeoutAction timeout_action = 1;</code>
     * @param value The enum numeric value on the wire for timeoutAction to set.
     */
    private void setTimeoutActionValue(int value) {
        timeoutAction_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: TimeoutAction timeout_action
     *&#64;&#64;
     *&#64;&#64;     The action applied to timed-out request.
     *&#64;&#64;     The default action is REJECT.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelQueuePolicy.TimeoutAction timeout_action = 1;</code>
     * @param value The timeoutAction to set.
     */
    private void setTimeoutAction(inference.ModelConfigOuterClass.ModelQueuePolicy.TimeoutAction value) {
      timeoutAction_ = value.getNumber();
      
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: TimeoutAction timeout_action
     *&#64;&#64;
     *&#64;&#64;     The action applied to timed-out request.
     *&#64;&#64;     The default action is REJECT.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelQueuePolicy.TimeoutAction timeout_action = 1;</code>
     */
    private void clearTimeoutAction() {
      
      timeoutAction_ = 0;
    }

    public static final int DEFAULT_TIMEOUT_MICROSECONDS_FIELD_NUMBER = 2;
    private long defaultTimeoutMicroseconds_;
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: uint64 default_timeout_microseconds
     *&#64;&#64;
     *&#64;&#64;     The default timeout for every request, in microseconds.
     *&#64;&#64;     The default value is 0 which indicates that no timeout is set.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 default_timeout_microseconds = 2;</code>
     * @return The defaultTimeoutMicroseconds.
     */
    @java.lang.Override
    public long getDefaultTimeoutMicroseconds() {
      return defaultTimeoutMicroseconds_;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: uint64 default_timeout_microseconds
     *&#64;&#64;
     *&#64;&#64;     The default timeout for every request, in microseconds.
     *&#64;&#64;     The default value is 0 which indicates that no timeout is set.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 default_timeout_microseconds = 2;</code>
     * @param value The defaultTimeoutMicroseconds to set.
     */
    private void setDefaultTimeoutMicroseconds(long value) {
      
      defaultTimeoutMicroseconds_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: uint64 default_timeout_microseconds
     *&#64;&#64;
     *&#64;&#64;     The default timeout for every request, in microseconds.
     *&#64;&#64;     The default value is 0 which indicates that no timeout is set.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 default_timeout_microseconds = 2;</code>
     */
    private void clearDefaultTimeoutMicroseconds() {
      
      defaultTimeoutMicroseconds_ = 0L;
    }

    public static final int ALLOW_TIMEOUT_OVERRIDE_FIELD_NUMBER = 3;
    private boolean allowTimeoutOverride_;
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: bool allow_timeout_override
     *&#64;&#64;
     *&#64;&#64;     Whether individual request can override the default timeout value.
     *&#64;&#64;     When true, individual requests can set a timeout that is less than
     *&#64;&#64;     the default timeout value but may not increase the timeout.
     *&#64;&#64;     The default value is false.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool allow_timeout_override = 3;</code>
     * @return The allowTimeoutOverride.
     */
    @java.lang.Override
    public boolean getAllowTimeoutOverride() {
      return allowTimeoutOverride_;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: bool allow_timeout_override
     *&#64;&#64;
     *&#64;&#64;     Whether individual request can override the default timeout value.
     *&#64;&#64;     When true, individual requests can set a timeout that is less than
     *&#64;&#64;     the default timeout value but may not increase the timeout.
     *&#64;&#64;     The default value is false.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool allow_timeout_override = 3;</code>
     * @param value The allowTimeoutOverride to set.
     */
    private void setAllowTimeoutOverride(boolean value) {
      
      allowTimeoutOverride_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: bool allow_timeout_override
     *&#64;&#64;
     *&#64;&#64;     Whether individual request can override the default timeout value.
     *&#64;&#64;     When true, individual requests can set a timeout that is less than
     *&#64;&#64;     the default timeout value but may not increase the timeout.
     *&#64;&#64;     The default value is false.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool allow_timeout_override = 3;</code>
     */
    private void clearAllowTimeoutOverride() {
      
      allowTimeoutOverride_ = false;
    }

    public static final int MAX_QUEUE_SIZE_FIELD_NUMBER = 4;
    private int maxQueueSize_;
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: uint32 max_queue_size
     *&#64;&#64;
     *&#64;&#64;     The maximum queue size for holding requests. A request will be
     *&#64;&#64;     rejected immediately if it can't be enqueued because the queue is
     *&#64;&#64;     full. The default value is 0 which indicates that no maximum
     *&#64;&#64;     queue size is enforced.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 max_queue_size = 4;</code>
     * @return The maxQueueSize.
     */
    @java.lang.Override
    public int getMaxQueueSize() {
      return maxQueueSize_;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: uint32 max_queue_size
     *&#64;&#64;
     *&#64;&#64;     The maximum queue size for holding requests. A request will be
     *&#64;&#64;     rejected immediately if it can't be enqueued because the queue is
     *&#64;&#64;     full. The default value is 0 which indicates that no maximum
     *&#64;&#64;     queue size is enforced.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 max_queue_size = 4;</code>
     * @param value The maxQueueSize to set.
     */
    private void setMaxQueueSize(int value) {
      
      maxQueueSize_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: uint32 max_queue_size
     *&#64;&#64;
     *&#64;&#64;     The maximum queue size for holding requests. A request will be
     *&#64;&#64;     rejected immediately if it can't be enqueued because the queue is
     *&#64;&#64;     full. The default value is 0 which indicates that no maximum
     *&#64;&#64;     queue size is enforced.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 max_queue_size = 4;</code>
     */
    private void clearMaxQueueSize() {
      
      maxQueueSize_ = 0;
    }

    public static inference.ModelConfigOuterClass.ModelQueuePolicy parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelQueuePolicy parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelQueuePolicy parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelQueuePolicy parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelQueuePolicy parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelQueuePolicy parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelQueuePolicy parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelQueuePolicy parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelQueuePolicy parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelQueuePolicy parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelQueuePolicy parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelQueuePolicy parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }

    public static Builder newBuilder() {
      return (Builder) DEFAULT_INSTANCE.createBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelQueuePolicy prototype) {
      return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelQueuePolicy
     *&#64;&#64;
     *&#64;&#64;   Queue policy for inference requests.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelQueuePolicy}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageLite.Builder<
          inference.ModelConfigOuterClass.ModelQueuePolicy, Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelQueuePolicy)
        inference.ModelConfigOuterClass.ModelQueuePolicyOrBuilder {
      // Construct using inference.ModelConfigOuterClass.ModelQueuePolicy.newBuilder()
      private Builder() {
        super(DEFAULT_INSTANCE);
      }


      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: TimeoutAction timeout_action
       *&#64;&#64;
       *&#64;&#64;     The action applied to timed-out request.
       *&#64;&#64;     The default action is REJECT.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelQueuePolicy.TimeoutAction timeout_action = 1;</code>
       * @return The enum numeric value on the wire for timeoutAction.
       */
      @java.lang.Override
      public int getTimeoutActionValue() {
        return instance.getTimeoutActionValue();
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: TimeoutAction timeout_action
       *&#64;&#64;
       *&#64;&#64;     The action applied to timed-out request.
       *&#64;&#64;     The default action is REJECT.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelQueuePolicy.TimeoutAction timeout_action = 1;</code>
       * @param value The timeoutAction to set.
       * @return This builder for chaining.
       */
      public Builder setTimeoutActionValue(int value) {
        copyOnWrite();
        instance.setTimeoutActionValue(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: TimeoutAction timeout_action
       *&#64;&#64;
       *&#64;&#64;     The action applied to timed-out request.
       *&#64;&#64;     The default action is REJECT.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelQueuePolicy.TimeoutAction timeout_action = 1;</code>
       * @return The timeoutAction.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelQueuePolicy.TimeoutAction getTimeoutAction() {
        return instance.getTimeoutAction();
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: TimeoutAction timeout_action
       *&#64;&#64;
       *&#64;&#64;     The action applied to timed-out request.
       *&#64;&#64;     The default action is REJECT.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelQueuePolicy.TimeoutAction timeout_action = 1;</code>
       * @param value The enum numeric value on the wire for timeoutAction to set.
       * @return This builder for chaining.
       */
      public Builder setTimeoutAction(inference.ModelConfigOuterClass.ModelQueuePolicy.TimeoutAction value) {
        copyOnWrite();
        instance.setTimeoutAction(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: TimeoutAction timeout_action
       *&#64;&#64;
       *&#64;&#64;     The action applied to timed-out request.
       *&#64;&#64;     The default action is REJECT.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelQueuePolicy.TimeoutAction timeout_action = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearTimeoutAction() {
        copyOnWrite();
        instance.clearTimeoutAction();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: uint64 default_timeout_microseconds
       *&#64;&#64;
       *&#64;&#64;     The default timeout for every request, in microseconds.
       *&#64;&#64;     The default value is 0 which indicates that no timeout is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 default_timeout_microseconds = 2;</code>
       * @return The defaultTimeoutMicroseconds.
       */
      @java.lang.Override
      public long getDefaultTimeoutMicroseconds() {
        return instance.getDefaultTimeoutMicroseconds();
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: uint64 default_timeout_microseconds
       *&#64;&#64;
       *&#64;&#64;     The default timeout for every request, in microseconds.
       *&#64;&#64;     The default value is 0 which indicates that no timeout is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 default_timeout_microseconds = 2;</code>
       * @param value The defaultTimeoutMicroseconds to set.
       * @return This builder for chaining.
       */
      public Builder setDefaultTimeoutMicroseconds(long value) {
        copyOnWrite();
        instance.setDefaultTimeoutMicroseconds(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: uint64 default_timeout_microseconds
       *&#64;&#64;
       *&#64;&#64;     The default timeout for every request, in microseconds.
       *&#64;&#64;     The default value is 0 which indicates that no timeout is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 default_timeout_microseconds = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearDefaultTimeoutMicroseconds() {
        copyOnWrite();
        instance.clearDefaultTimeoutMicroseconds();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: bool allow_timeout_override
       *&#64;&#64;
       *&#64;&#64;     Whether individual request can override the default timeout value.
       *&#64;&#64;     When true, individual requests can set a timeout that is less than
       *&#64;&#64;     the default timeout value but may not increase the timeout.
       *&#64;&#64;     The default value is false.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool allow_timeout_override = 3;</code>
       * @return The allowTimeoutOverride.
       */
      @java.lang.Override
      public boolean getAllowTimeoutOverride() {
        return instance.getAllowTimeoutOverride();
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: bool allow_timeout_override
       *&#64;&#64;
       *&#64;&#64;     Whether individual request can override the default timeout value.
       *&#64;&#64;     When true, individual requests can set a timeout that is less than
       *&#64;&#64;     the default timeout value but may not increase the timeout.
       *&#64;&#64;     The default value is false.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool allow_timeout_override = 3;</code>
       * @param value The allowTimeoutOverride to set.
       * @return This builder for chaining.
       */
      public Builder setAllowTimeoutOverride(boolean value) {
        copyOnWrite();
        instance.setAllowTimeoutOverride(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: bool allow_timeout_override
       *&#64;&#64;
       *&#64;&#64;     Whether individual request can override the default timeout value.
       *&#64;&#64;     When true, individual requests can set a timeout that is less than
       *&#64;&#64;     the default timeout value but may not increase the timeout.
       *&#64;&#64;     The default value is false.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool allow_timeout_override = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearAllowTimeoutOverride() {
        copyOnWrite();
        instance.clearAllowTimeoutOverride();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: uint32 max_queue_size
       *&#64;&#64;
       *&#64;&#64;     The maximum queue size for holding requests. A request will be
       *&#64;&#64;     rejected immediately if it can't be enqueued because the queue is
       *&#64;&#64;     full. The default value is 0 which indicates that no maximum
       *&#64;&#64;     queue size is enforced.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 max_queue_size = 4;</code>
       * @return The maxQueueSize.
       */
      @java.lang.Override
      public int getMaxQueueSize() {
        return instance.getMaxQueueSize();
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: uint32 max_queue_size
       *&#64;&#64;
       *&#64;&#64;     The maximum queue size for holding requests. A request will be
       *&#64;&#64;     rejected immediately if it can't be enqueued because the queue is
       *&#64;&#64;     full. The default value is 0 which indicates that no maximum
       *&#64;&#64;     queue size is enforced.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 max_queue_size = 4;</code>
       * @param value The maxQueueSize to set.
       * @return This builder for chaining.
       */
      public Builder setMaxQueueSize(int value) {
        copyOnWrite();
        instance.setMaxQueueSize(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: uint32 max_queue_size
       *&#64;&#64;
       *&#64;&#64;     The maximum queue size for holding requests. A request will be
       *&#64;&#64;     rejected immediately if it can't be enqueued because the queue is
       *&#64;&#64;     full. The default value is 0 which indicates that no maximum
       *&#64;&#64;     queue size is enforced.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 max_queue_size = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearMaxQueueSize() {
        copyOnWrite();
        instance.clearMaxQueueSize();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:inference.ModelQueuePolicy)
    }
    @java.lang.Override
    @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
    protected final java.lang.Object dynamicMethod(
        com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
        java.lang.Object arg0, java.lang.Object arg1) {
      switch (method) {
        case NEW_MUTABLE_INSTANCE: {
          return new inference.ModelConfigOuterClass.ModelQueuePolicy();
        }
        case NEW_BUILDER: {
          return new Builder();
        }
        case BUILD_MESSAGE_INFO: {
            java.lang.Object[] objects = new java.lang.Object[] {
              "timeoutAction_",
              "defaultTimeoutMicroseconds_",
              "allowTimeoutOverride_",
              "maxQueueSize_",
            };
            java.lang.String info =
                "\u0000\u0004\u0000\u0000\u0001\u0004\u0004\u0000\u0000\u0000\u0001\f\u0002\u0003" +
                "\u0003\u0007\u0004\u000b";
            return newMessageInfo(DEFAULT_INSTANCE, info, objects);
        }
        // fall through
        case GET_DEFAULT_INSTANCE: {
          return DEFAULT_INSTANCE;
        }
        case GET_PARSER: {
          com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelQueuePolicy> parser = PARSER;
          if (parser == null) {
            synchronized (inference.ModelConfigOuterClass.ModelQueuePolicy.class) {
              parser = PARSER;
              if (parser == null) {
                parser =
                    new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelQueuePolicy>(
                        DEFAULT_INSTANCE);
                PARSER = parser;
              }
            }
          }
          return parser;
      }
      case GET_MEMOIZED_IS_INITIALIZED: {
        return (byte) 1;
      }
      case SET_MEMOIZED_IS_INITIALIZED: {
        return null;
      }
      }
      throw new UnsupportedOperationException();
    }


    // @@protoc_insertion_point(class_scope:inference.ModelQueuePolicy)
    private static final inference.ModelConfigOuterClass.ModelQueuePolicy DEFAULT_INSTANCE;
    static {
      ModelQueuePolicy defaultInstance = new ModelQueuePolicy();
      // New instances are implicitly immutable so no need to make
      // immutable.
      DEFAULT_INSTANCE = defaultInstance;
      com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
        ModelQueuePolicy.class, defaultInstance);
    }

    public static inference.ModelConfigOuterClass.ModelQueuePolicy getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static volatile com.google.protobuf.Parser<ModelQueuePolicy> PARSER;

    public static com.google.protobuf.Parser<ModelQueuePolicy> parser() {
      return DEFAULT_INSTANCE.getParserForType();
    }
  }

  public interface ModelDynamicBatchingOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelDynamicBatching)
      com.google.protobuf.MessageLiteOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     * @return A list containing the preferredBatchSize.
     */
    java.util.List<java.lang.Integer> getPreferredBatchSizeList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     * @return The count of preferredBatchSize.
     */
    int getPreferredBatchSizeCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     * @param index The index of the element to return.
     * @return The preferredBatchSize at the given index.
     */
    int getPreferredBatchSize(int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 max_queue_delay_microseconds
     *&#64;&#64;
     *&#64;&#64;     The maximum time, in microseconds, a request will be delayed in
     *&#64;&#64;     the scheduling queue to wait for additional requests for
     *&#64;&#64;     batching. Default is 0.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 max_queue_delay_microseconds = 2;</code>
     * @return The maxQueueDelayMicroseconds.
     */
    long getMaxQueueDelayMicroseconds();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool preserve_ordering
     *&#64;&#64;
     *&#64;&#64;     Should the dynamic batcher preserve the ordering of responses to
     *&#64;&#64;     match the order of requests received by the scheduler. Default is
     *&#64;&#64;     false. If true, the responses will be returned in the same order as
     *&#64;&#64;     the order of requests sent to the scheduler. If false, the responses
     *&#64;&#64;     may be returned in arbitrary order. This option is specifically
     *&#64;&#64;     needed when a sequence of related inference requests (i.e. inference
     *&#64;&#64;     requests with the same correlation ID) are sent to the dynamic
     *&#64;&#64;     batcher to ensure that the sequence responses are in the correct
     *&#64;&#64;     order.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool preserve_ordering = 3;</code>
     * @return The preserveOrdering.
     */
    boolean getPreserveOrdering();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 priority_levels
     *&#64;&#64;
     *&#64;&#64;     The number of priority levels to be enabled for the model,
     *&#64;&#64;     the priority level starts from 1 and 1 is the highest priority.
     *&#64;&#64;     Requests are handled in priority order with all priority 1 requests
     *&#64;&#64;     processed before priority 2, all priority 2 requests processed before
     *&#64;&#64;     priority 3, etc. Requests with the same priority level will be
     *&#64;&#64;     handled in the order that they are received.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 priority_levels = 4;</code>
     * @return The priorityLevels.
     */
    int getPriorityLevels();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 default_priority_level
     *&#64;&#64;
     *&#64;&#64;     The priority level used for requests that don't specify their
     *&#64;&#64;     priority. The value must be in the range [ 1, 'priority_levels' ].
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 default_priority_level = 5;</code>
     * @return The defaultPriorityLevel.
     */
    int getDefaultPriorityLevel();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
     *&#64;&#64;
     *&#64;&#64;     The default queue policy used for requests that don't require
     *&#64;&#64;     priority handling and requests that specify priority levels where
     *&#64;&#64;     there is no specific policy given. If not specified, a policy with
     *&#64;&#64;     default field values will be used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
     * @return Whether the defaultQueuePolicy field is set.
     */
    boolean hasDefaultQueuePolicy();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
     *&#64;&#64;
     *&#64;&#64;     The default queue policy used for requests that don't require
     *&#64;&#64;     priority handling and requests that specify priority levels where
     *&#64;&#64;     there is no specific policy given. If not specified, a policy with
     *&#64;&#64;     default field values will be used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
     * @return The defaultQueuePolicy.
     */
    inference.ModelConfigOuterClass.ModelQueuePolicy getDefaultQueuePolicy();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
     *&#64;&#64;
     *&#64;&#64;     Specify the queue policy for the priority level. The default queue
     *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
     *&#64;&#64;     policy.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
     */
    int getPriorityQueuePolicyCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
     *&#64;&#64;
     *&#64;&#64;     Specify the queue policy for the priority level. The default queue
     *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
     *&#64;&#64;     policy.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
     */
    boolean containsPriorityQueuePolicy(
        int key);
    /**
     * Use {@link #getPriorityQueuePolicyMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy>
    getPriorityQueuePolicy();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
     *&#64;&#64;
     *&#64;&#64;     Specify the queue policy for the priority level. The default queue
     *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
     *&#64;&#64;     policy.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
     */
    java.util.Map<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy>
    getPriorityQueuePolicyMap();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
     *&#64;&#64;
     *&#64;&#64;     Specify the queue policy for the priority level. The default queue
     *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
     *&#64;&#64;     policy.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
     */

    inference.ModelConfigOuterClass.ModelQueuePolicy getPriorityQueuePolicyOrDefault(
        int key,
        inference.ModelConfigOuterClass.ModelQueuePolicy defaultValue);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
     *&#64;&#64;
     *&#64;&#64;     Specify the queue policy for the priority level. The default queue
     *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
     *&#64;&#64;     policy.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
     */

    inference.ModelConfigOuterClass.ModelQueuePolicy getPriorityQueuePolicyOrThrow(
        int key);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelDynamicBatching
   *&#64;&#64;
   *&#64;&#64;   Dynamic batching configuration. These settings control how dynamic
   *&#64;&#64;   batching operates for the model.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelDynamicBatching}
   */
  public  static final class ModelDynamicBatching extends
      com.google.protobuf.GeneratedMessageLite<
          ModelDynamicBatching, ModelDynamicBatching.Builder> implements
      // @@protoc_insertion_point(message_implements:inference.ModelDynamicBatching)
      ModelDynamicBatchingOrBuilder {
    private ModelDynamicBatching() {
      preferredBatchSize_ = emptyIntList();
    }
    public static final int PREFERRED_BATCH_SIZE_FIELD_NUMBER = 1;
    private com.google.protobuf.Internal.IntList preferredBatchSize_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     * @return A list containing the preferredBatchSize.
     */
    @java.lang.Override
    public java.util.List<java.lang.Integer>
        getPreferredBatchSizeList() {
      return preferredBatchSize_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     * @return The count of preferredBatchSize.
     */
    @java.lang.Override
    public int getPreferredBatchSizeCount() {
      return preferredBatchSize_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     * @param index The index of the element to return.
     * @return The preferredBatchSize at the given index.
     */
    @java.lang.Override
    public int getPreferredBatchSize(int index) {
      return preferredBatchSize_.getInt(index);
    }
    private int preferredBatchSizeMemoizedSerializedSize = -1;
    private void ensurePreferredBatchSizeIsMutable() {
      com.google.protobuf.Internal.IntList tmp = preferredBatchSize_;
      if (!tmp.isModifiable()) {
        preferredBatchSize_ =
            com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
       }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     * @param index The index to set the value at.
     * @param value The preferredBatchSize to set.
     */
    private void setPreferredBatchSize(
        int index, int value) {
      ensurePreferredBatchSizeIsMutable();
      preferredBatchSize_.setInt(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     * @param value The preferredBatchSize to add.
     */
    private void addPreferredBatchSize(int value) {
      ensurePreferredBatchSizeIsMutable();
      preferredBatchSize_.addInt(value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     * @param values The preferredBatchSize to add.
     */
    private void addAllPreferredBatchSize(
        java.lang.Iterable<? extends java.lang.Integer> values) {
      ensurePreferredBatchSizeIsMutable();
      com.google.protobuf.AbstractMessageLite.addAll(
          values, preferredBatchSize_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     */
    private void clearPreferredBatchSize() {
      preferredBatchSize_ = emptyIntList();
    }

    public static final int MAX_QUEUE_DELAY_MICROSECONDS_FIELD_NUMBER = 2;
    private long maxQueueDelayMicroseconds_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 max_queue_delay_microseconds
     *&#64;&#64;
     *&#64;&#64;     The maximum time, in microseconds, a request will be delayed in
     *&#64;&#64;     the scheduling queue to wait for additional requests for
     *&#64;&#64;     batching. Default is 0.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 max_queue_delay_microseconds = 2;</code>
     * @return The maxQueueDelayMicroseconds.
     */
    @java.lang.Override
    public long getMaxQueueDelayMicroseconds() {
      return maxQueueDelayMicroseconds_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 max_queue_delay_microseconds
     *&#64;&#64;
     *&#64;&#64;     The maximum time, in microseconds, a request will be delayed in
     *&#64;&#64;     the scheduling queue to wait for additional requests for
     *&#64;&#64;     batching. Default is 0.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 max_queue_delay_microseconds = 2;</code>
     * @param value The maxQueueDelayMicroseconds to set.
     */
    private void setMaxQueueDelayMicroseconds(long value) {
      
      maxQueueDelayMicroseconds_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 max_queue_delay_microseconds
     *&#64;&#64;
     *&#64;&#64;     The maximum time, in microseconds, a request will be delayed in
     *&#64;&#64;     the scheduling queue to wait for additional requests for
     *&#64;&#64;     batching. Default is 0.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 max_queue_delay_microseconds = 2;</code>
     */
    private void clearMaxQueueDelayMicroseconds() {
      
      maxQueueDelayMicroseconds_ = 0L;
    }

    public static final int PRESERVE_ORDERING_FIELD_NUMBER = 3;
    private boolean preserveOrdering_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool preserve_ordering
     *&#64;&#64;
     *&#64;&#64;     Should the dynamic batcher preserve the ordering of responses to
     *&#64;&#64;     match the order of requests received by the scheduler. Default is
     *&#64;&#64;     false. If true, the responses will be returned in the same order as
     *&#64;&#64;     the order of requests sent to the scheduler. If false, the responses
     *&#64;&#64;     may be returned in arbitrary order. This option is specifically
     *&#64;&#64;     needed when a sequence of related inference requests (i.e. inference
     *&#64;&#64;     requests with the same correlation ID) are sent to the dynamic
     *&#64;&#64;     batcher to ensure that the sequence responses are in the correct
     *&#64;&#64;     order.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool preserve_ordering = 3;</code>
     * @return The preserveOrdering.
     */
    @java.lang.Override
    public boolean getPreserveOrdering() {
      return preserveOrdering_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool preserve_ordering
     *&#64;&#64;
     *&#64;&#64;     Should the dynamic batcher preserve the ordering of responses to
     *&#64;&#64;     match the order of requests received by the scheduler. Default is
     *&#64;&#64;     false. If true, the responses will be returned in the same order as
     *&#64;&#64;     the order of requests sent to the scheduler. If false, the responses
     *&#64;&#64;     may be returned in arbitrary order. This option is specifically
     *&#64;&#64;     needed when a sequence of related inference requests (i.e. inference
     *&#64;&#64;     requests with the same correlation ID) are sent to the dynamic
     *&#64;&#64;     batcher to ensure that the sequence responses are in the correct
     *&#64;&#64;     order.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool preserve_ordering = 3;</code>
     * @param value The preserveOrdering to set.
     */
    private void setPreserveOrdering(boolean value) {
      
      preserveOrdering_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool preserve_ordering
     *&#64;&#64;
     *&#64;&#64;     Should the dynamic batcher preserve the ordering of responses to
     *&#64;&#64;     match the order of requests received by the scheduler. Default is
     *&#64;&#64;     false. If true, the responses will be returned in the same order as
     *&#64;&#64;     the order of requests sent to the scheduler. If false, the responses
     *&#64;&#64;     may be returned in arbitrary order. This option is specifically
     *&#64;&#64;     needed when a sequence of related inference requests (i.e. inference
     *&#64;&#64;     requests with the same correlation ID) are sent to the dynamic
     *&#64;&#64;     batcher to ensure that the sequence responses are in the correct
     *&#64;&#64;     order.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool preserve_ordering = 3;</code>
     */
    private void clearPreserveOrdering() {
      
      preserveOrdering_ = false;
    }

    public static final int PRIORITY_LEVELS_FIELD_NUMBER = 4;
    private int priorityLevels_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 priority_levels
     *&#64;&#64;
     *&#64;&#64;     The number of priority levels to be enabled for the model,
     *&#64;&#64;     the priority level starts from 1 and 1 is the highest priority.
     *&#64;&#64;     Requests are handled in priority order with all priority 1 requests
     *&#64;&#64;     processed before priority 2, all priority 2 requests processed before
     *&#64;&#64;     priority 3, etc. Requests with the same priority level will be
     *&#64;&#64;     handled in the order that they are received.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 priority_levels = 4;</code>
     * @return The priorityLevels.
     */
    @java.lang.Override
    public int getPriorityLevels() {
      return priorityLevels_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 priority_levels
     *&#64;&#64;
     *&#64;&#64;     The number of priority levels to be enabled for the model,
     *&#64;&#64;     the priority level starts from 1 and 1 is the highest priority.
     *&#64;&#64;     Requests are handled in priority order with all priority 1 requests
     *&#64;&#64;     processed before priority 2, all priority 2 requests processed before
     *&#64;&#64;     priority 3, etc. Requests with the same priority level will be
     *&#64;&#64;     handled in the order that they are received.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 priority_levels = 4;</code>
     * @param value The priorityLevels to set.
     */
    private void setPriorityLevels(int value) {
      
      priorityLevels_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 priority_levels
     *&#64;&#64;
     *&#64;&#64;     The number of priority levels to be enabled for the model,
     *&#64;&#64;     the priority level starts from 1 and 1 is the highest priority.
     *&#64;&#64;     Requests are handled in priority order with all priority 1 requests
     *&#64;&#64;     processed before priority 2, all priority 2 requests processed before
     *&#64;&#64;     priority 3, etc. Requests with the same priority level will be
     *&#64;&#64;     handled in the order that they are received.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 priority_levels = 4;</code>
     */
    private void clearPriorityLevels() {
      
      priorityLevels_ = 0;
    }

    public static final int DEFAULT_PRIORITY_LEVEL_FIELD_NUMBER = 5;
    private int defaultPriorityLevel_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 default_priority_level
     *&#64;&#64;
     *&#64;&#64;     The priority level used for requests that don't specify their
     *&#64;&#64;     priority. The value must be in the range [ 1, 'priority_levels' ].
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 default_priority_level = 5;</code>
     * @return The defaultPriorityLevel.
     */
    @java.lang.Override
    public int getDefaultPriorityLevel() {
      return defaultPriorityLevel_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 default_priority_level
     *&#64;&#64;
     *&#64;&#64;     The priority level used for requests that don't specify their
     *&#64;&#64;     priority. The value must be in the range [ 1, 'priority_levels' ].
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 default_priority_level = 5;</code>
     * @param value The defaultPriorityLevel to set.
     */
    private void setDefaultPriorityLevel(int value) {
      
      defaultPriorityLevel_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 default_priority_level
     *&#64;&#64;
     *&#64;&#64;     The priority level used for requests that don't specify their
     *&#64;&#64;     priority. The value must be in the range [ 1, 'priority_levels' ].
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 default_priority_level = 5;</code>
     */
    private void clearDefaultPriorityLevel() {
      
      defaultPriorityLevel_ = 0;
    }

    public static final int DEFAULT_QUEUE_POLICY_FIELD_NUMBER = 6;
    private inference.ModelConfigOuterClass.ModelQueuePolicy defaultQueuePolicy_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
     *&#64;&#64;
     *&#64;&#64;     The default queue policy used for requests that don't require
     *&#64;&#64;     priority handling and requests that specify priority levels where
     *&#64;&#64;     there is no specific policy given. If not specified, a policy with
     *&#64;&#64;     default field values will be used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
     */
    @java.lang.Override
    public boolean hasDefaultQueuePolicy() {
      return defaultQueuePolicy_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
     *&#64;&#64;
     *&#64;&#64;     The default queue policy used for requests that don't require
     *&#64;&#64;     priority handling and requests that specify priority levels where
     *&#64;&#64;     there is no specific policy given. If not specified, a policy with
     *&#64;&#64;     default field values will be used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelQueuePolicy getDefaultQueuePolicy() {
      return defaultQueuePolicy_ == null ? inference.ModelConfigOuterClass.ModelQueuePolicy.getDefaultInstance() : defaultQueuePolicy_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
     *&#64;&#64;
     *&#64;&#64;     The default queue policy used for requests that don't require
     *&#64;&#64;     priority handling and requests that specify priority levels where
     *&#64;&#64;     there is no specific policy given. If not specified, a policy with
     *&#64;&#64;     default field values will be used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
     */
    private void setDefaultQueuePolicy(inference.ModelConfigOuterClass.ModelQueuePolicy value) {
      value.getClass();
  defaultQueuePolicy_ = value;
      
      }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
     *&#64;&#64;
     *&#64;&#64;     The default queue policy used for requests that don't require
     *&#64;&#64;     priority handling and requests that specify priority levels where
     *&#64;&#64;     there is no specific policy given. If not specified, a policy with
     *&#64;&#64;     default field values will be used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
     */
    @java.lang.SuppressWarnings({"ReferenceEquality"})
    private void mergeDefaultQueuePolicy(inference.ModelConfigOuterClass.ModelQueuePolicy value) {
      value.getClass();
  if (defaultQueuePolicy_ != null &&
          defaultQueuePolicy_ != inference.ModelConfigOuterClass.ModelQueuePolicy.getDefaultInstance()) {
        defaultQueuePolicy_ =
          inference.ModelConfigOuterClass.ModelQueuePolicy.newBuilder(defaultQueuePolicy_).mergeFrom(value).buildPartial();
      } else {
        defaultQueuePolicy_ = value;
      }
      
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
     *&#64;&#64;
     *&#64;&#64;     The default queue policy used for requests that don't require
     *&#64;&#64;     priority handling and requests that specify priority levels where
     *&#64;&#64;     there is no specific policy given. If not specified, a policy with
     *&#64;&#64;     default field values will be used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
     */
    private void clearDefaultQueuePolicy() {  defaultQueuePolicy_ = null;
      
    }

    public static final int PRIORITY_QUEUE_POLICY_FIELD_NUMBER = 7;
    private static final class PriorityQueuePolicyDefaultEntryHolder {
      static final com.google.protobuf.MapEntryLite<
          java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy> defaultEntry =
              com.google.protobuf.MapEntryLite
              .<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy>newDefaultInstance(
                  com.google.protobuf.WireFormat.FieldType.UINT32,
                  0,
                  com.google.protobuf.WireFormat.FieldType.MESSAGE,
                  inference.ModelConfigOuterClass.ModelQueuePolicy.getDefaultInstance());
    }
    private com.google.protobuf.MapFieldLite<
        java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy> priorityQueuePolicy_ =
            com.google.protobuf.MapFieldLite.emptyMapField();
    private com.google.protobuf.MapFieldLite<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy>
    internalGetPriorityQueuePolicy() {
      return priorityQueuePolicy_;
    }
    private com.google.protobuf.MapFieldLite<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy>
    internalGetMutablePriorityQueuePolicy() {
      if (!priorityQueuePolicy_.isMutable()) {
        priorityQueuePolicy_ = priorityQueuePolicy_.mutableCopy();
      }
      return priorityQueuePolicy_;
    }
    @java.lang.Override

    public int getPriorityQueuePolicyCount() {
      return internalGetPriorityQueuePolicy().size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
     *&#64;&#64;
     *&#64;&#64;     Specify the queue policy for the priority level. The default queue
     *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
     *&#64;&#64;     policy.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
     */
    @java.lang.Override

    public boolean containsPriorityQueuePolicy(
        int key) {
      
      return internalGetPriorityQueuePolicy().containsKey(key);
    }
    /**
     * Use {@link #getPriorityQueuePolicyMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy> getPriorityQueuePolicy() {
      return getPriorityQueuePolicyMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
     *&#64;&#64;
     *&#64;&#64;     Specify the queue policy for the priority level. The default queue
     *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
     *&#64;&#64;     policy.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy> getPriorityQueuePolicyMap() {
      return java.util.Collections.unmodifiableMap(
          internalGetPriorityQueuePolicy());
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
     *&#64;&#64;
     *&#64;&#64;     Specify the queue policy for the priority level. The default queue
     *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
     *&#64;&#64;     policy.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
     */
    @java.lang.Override

    public inference.ModelConfigOuterClass.ModelQueuePolicy getPriorityQueuePolicyOrDefault(
        int key,
        inference.ModelConfigOuterClass.ModelQueuePolicy defaultValue) {
      
      java.util.Map<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy> map =
          internalGetPriorityQueuePolicy();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
     *&#64;&#64;
     *&#64;&#64;     Specify the queue policy for the priority level. The default queue
     *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
     *&#64;&#64;     policy.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
     */
    @java.lang.Override

    public inference.ModelConfigOuterClass.ModelQueuePolicy getPriorityQueuePolicyOrThrow(
        int key) {
      
      java.util.Map<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy> map =
          internalGetPriorityQueuePolicy();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
     *&#64;&#64;
     *&#64;&#64;     Specify the queue policy for the priority level. The default queue
     *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
     *&#64;&#64;     policy.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
     */
    private java.util.Map<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy>
    getMutablePriorityQueuePolicyMap() {
      return internalGetMutablePriorityQueuePolicy();
    }

    public static inference.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelDynamicBatching parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelDynamicBatching parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelDynamicBatching parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelDynamicBatching parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }

    public static Builder newBuilder() {
      return (Builder) DEFAULT_INSTANCE.createBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelDynamicBatching prototype) {
      return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelDynamicBatching
     *&#64;&#64;
     *&#64;&#64;   Dynamic batching configuration. These settings control how dynamic
     *&#64;&#64;   batching operates for the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelDynamicBatching}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageLite.Builder<
          inference.ModelConfigOuterClass.ModelDynamicBatching, Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelDynamicBatching)
        inference.ModelConfigOuterClass.ModelDynamicBatchingOrBuilder {
      // Construct using inference.ModelConfigOuterClass.ModelDynamicBatching.newBuilder()
      private Builder() {
        super(DEFAULT_INSTANCE);
      }


      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       * @return A list containing the preferredBatchSize.
       */
      @java.lang.Override
      public java.util.List<java.lang.Integer>
          getPreferredBatchSizeList() {
        return java.util.Collections.unmodifiableList(
            instance.getPreferredBatchSizeList());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       * @return The count of preferredBatchSize.
       */
      @java.lang.Override
      public int getPreferredBatchSizeCount() {
        return instance.getPreferredBatchSizeCount();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       * @param index The index of the element to return.
       * @return The preferredBatchSize at the given index.
       */
      @java.lang.Override
      public int getPreferredBatchSize(int index) {
        return instance.getPreferredBatchSize(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       * @param value The preferredBatchSize to set.
       * @return This builder for chaining.
       */
      public Builder setPreferredBatchSize(
          int index, int value) {
        copyOnWrite();
        instance.setPreferredBatchSize(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       * @param value The preferredBatchSize to add.
       * @return This builder for chaining.
       */
      public Builder addPreferredBatchSize(int value) {
        copyOnWrite();
        instance.addPreferredBatchSize(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       * @param values The preferredBatchSize to add.
       * @return This builder for chaining.
       */
      public Builder addAllPreferredBatchSize(
          java.lang.Iterable<? extends java.lang.Integer> values) {
        copyOnWrite();
        instance.addAllPreferredBatchSize(values);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearPreferredBatchSize() {
        copyOnWrite();
        instance.clearPreferredBatchSize();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;     The maximum time, in microseconds, a request will be delayed in
       *&#64;&#64;     the scheduling queue to wait for additional requests for
       *&#64;&#64;     batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 2;</code>
       * @return The maxQueueDelayMicroseconds.
       */
      @java.lang.Override
      public long getMaxQueueDelayMicroseconds() {
        return instance.getMaxQueueDelayMicroseconds();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;     The maximum time, in microseconds, a request will be delayed in
       *&#64;&#64;     the scheduling queue to wait for additional requests for
       *&#64;&#64;     batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 2;</code>
       * @param value The maxQueueDelayMicroseconds to set.
       * @return This builder for chaining.
       */
      public Builder setMaxQueueDelayMicroseconds(long value) {
        copyOnWrite();
        instance.setMaxQueueDelayMicroseconds(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;     The maximum time, in microseconds, a request will be delayed in
       *&#64;&#64;     the scheduling queue to wait for additional requests for
       *&#64;&#64;     batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearMaxQueueDelayMicroseconds() {
        copyOnWrite();
        instance.clearMaxQueueDelayMicroseconds();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool preserve_ordering
       *&#64;&#64;
       *&#64;&#64;     Should the dynamic batcher preserve the ordering of responses to
       *&#64;&#64;     match the order of requests received by the scheduler. Default is
       *&#64;&#64;     false. If true, the responses will be returned in the same order as
       *&#64;&#64;     the order of requests sent to the scheduler. If false, the responses
       *&#64;&#64;     may be returned in arbitrary order. This option is specifically
       *&#64;&#64;     needed when a sequence of related inference requests (i.e. inference
       *&#64;&#64;     requests with the same correlation ID) are sent to the dynamic
       *&#64;&#64;     batcher to ensure that the sequence responses are in the correct
       *&#64;&#64;     order.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool preserve_ordering = 3;</code>
       * @return The preserveOrdering.
       */
      @java.lang.Override
      public boolean getPreserveOrdering() {
        return instance.getPreserveOrdering();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool preserve_ordering
       *&#64;&#64;
       *&#64;&#64;     Should the dynamic batcher preserve the ordering of responses to
       *&#64;&#64;     match the order of requests received by the scheduler. Default is
       *&#64;&#64;     false. If true, the responses will be returned in the same order as
       *&#64;&#64;     the order of requests sent to the scheduler. If false, the responses
       *&#64;&#64;     may be returned in arbitrary order. This option is specifically
       *&#64;&#64;     needed when a sequence of related inference requests (i.e. inference
       *&#64;&#64;     requests with the same correlation ID) are sent to the dynamic
       *&#64;&#64;     batcher to ensure that the sequence responses are in the correct
       *&#64;&#64;     order.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool preserve_ordering = 3;</code>
       * @param value The preserveOrdering to set.
       * @return This builder for chaining.
       */
      public Builder setPreserveOrdering(boolean value) {
        copyOnWrite();
        instance.setPreserveOrdering(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool preserve_ordering
       *&#64;&#64;
       *&#64;&#64;     Should the dynamic batcher preserve the ordering of responses to
       *&#64;&#64;     match the order of requests received by the scheduler. Default is
       *&#64;&#64;     false. If true, the responses will be returned in the same order as
       *&#64;&#64;     the order of requests sent to the scheduler. If false, the responses
       *&#64;&#64;     may be returned in arbitrary order. This option is specifically
       *&#64;&#64;     needed when a sequence of related inference requests (i.e. inference
       *&#64;&#64;     requests with the same correlation ID) are sent to the dynamic
       *&#64;&#64;     batcher to ensure that the sequence responses are in the correct
       *&#64;&#64;     order.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool preserve_ordering = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearPreserveOrdering() {
        copyOnWrite();
        instance.clearPreserveOrdering();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 priority_levels
       *&#64;&#64;
       *&#64;&#64;     The number of priority levels to be enabled for the model,
       *&#64;&#64;     the priority level starts from 1 and 1 is the highest priority.
       *&#64;&#64;     Requests are handled in priority order with all priority 1 requests
       *&#64;&#64;     processed before priority 2, all priority 2 requests processed before
       *&#64;&#64;     priority 3, etc. Requests with the same priority level will be
       *&#64;&#64;     handled in the order that they are received.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 priority_levels = 4;</code>
       * @return The priorityLevels.
       */
      @java.lang.Override
      public int getPriorityLevels() {
        return instance.getPriorityLevels();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 priority_levels
       *&#64;&#64;
       *&#64;&#64;     The number of priority levels to be enabled for the model,
       *&#64;&#64;     the priority level starts from 1 and 1 is the highest priority.
       *&#64;&#64;     Requests are handled in priority order with all priority 1 requests
       *&#64;&#64;     processed before priority 2, all priority 2 requests processed before
       *&#64;&#64;     priority 3, etc. Requests with the same priority level will be
       *&#64;&#64;     handled in the order that they are received.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 priority_levels = 4;</code>
       * @param value The priorityLevels to set.
       * @return This builder for chaining.
       */
      public Builder setPriorityLevels(int value) {
        copyOnWrite();
        instance.setPriorityLevels(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 priority_levels
       *&#64;&#64;
       *&#64;&#64;     The number of priority levels to be enabled for the model,
       *&#64;&#64;     the priority level starts from 1 and 1 is the highest priority.
       *&#64;&#64;     Requests are handled in priority order with all priority 1 requests
       *&#64;&#64;     processed before priority 2, all priority 2 requests processed before
       *&#64;&#64;     priority 3, etc. Requests with the same priority level will be
       *&#64;&#64;     handled in the order that they are received.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 priority_levels = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearPriorityLevels() {
        copyOnWrite();
        instance.clearPriorityLevels();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 default_priority_level
       *&#64;&#64;
       *&#64;&#64;     The priority level used for requests that don't specify their
       *&#64;&#64;     priority. The value must be in the range [ 1, 'priority_levels' ].
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 default_priority_level = 5;</code>
       * @return The defaultPriorityLevel.
       */
      @java.lang.Override
      public int getDefaultPriorityLevel() {
        return instance.getDefaultPriorityLevel();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 default_priority_level
       *&#64;&#64;
       *&#64;&#64;     The priority level used for requests that don't specify their
       *&#64;&#64;     priority. The value must be in the range [ 1, 'priority_levels' ].
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 default_priority_level = 5;</code>
       * @param value The defaultPriorityLevel to set.
       * @return This builder for chaining.
       */
      public Builder setDefaultPriorityLevel(int value) {
        copyOnWrite();
        instance.setDefaultPriorityLevel(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 default_priority_level
       *&#64;&#64;
       *&#64;&#64;     The priority level used for requests that don't specify their
       *&#64;&#64;     priority. The value must be in the range [ 1, 'priority_levels' ].
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 default_priority_level = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearDefaultPriorityLevel() {
        copyOnWrite();
        instance.clearDefaultPriorityLevel();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
       *&#64;&#64;
       *&#64;&#64;     The default queue policy used for requests that don't require
       *&#64;&#64;     priority handling and requests that specify priority levels where
       *&#64;&#64;     there is no specific policy given. If not specified, a policy with
       *&#64;&#64;     default field values will be used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
       */
      @java.lang.Override
      public boolean hasDefaultQueuePolicy() {
        return instance.hasDefaultQueuePolicy();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
       *&#64;&#64;
       *&#64;&#64;     The default queue policy used for requests that don't require
       *&#64;&#64;     priority handling and requests that specify priority levels where
       *&#64;&#64;     there is no specific policy given. If not specified, a policy with
       *&#64;&#64;     default field values will be used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelQueuePolicy getDefaultQueuePolicy() {
        return instance.getDefaultQueuePolicy();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
       *&#64;&#64;
       *&#64;&#64;     The default queue policy used for requests that don't require
       *&#64;&#64;     priority handling and requests that specify priority levels where
       *&#64;&#64;     there is no specific policy given. If not specified, a policy with
       *&#64;&#64;     default field values will be used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
       */
      public Builder setDefaultQueuePolicy(inference.ModelConfigOuterClass.ModelQueuePolicy value) {
        copyOnWrite();
        instance.setDefaultQueuePolicy(value);
        return this;
        }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
       *&#64;&#64;
       *&#64;&#64;     The default queue policy used for requests that don't require
       *&#64;&#64;     priority handling and requests that specify priority levels where
       *&#64;&#64;     there is no specific policy given. If not specified, a policy with
       *&#64;&#64;     default field values will be used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
       */
      public Builder setDefaultQueuePolicy(
          inference.ModelConfigOuterClass.ModelQueuePolicy.Builder builderForValue) {
        copyOnWrite();
        instance.setDefaultQueuePolicy(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
       *&#64;&#64;
       *&#64;&#64;     The default queue policy used for requests that don't require
       *&#64;&#64;     priority handling and requests that specify priority levels where
       *&#64;&#64;     there is no specific policy given. If not specified, a policy with
       *&#64;&#64;     default field values will be used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
       */
      public Builder mergeDefaultQueuePolicy(inference.ModelConfigOuterClass.ModelQueuePolicy value) {
        copyOnWrite();
        instance.mergeDefaultQueuePolicy(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
       *&#64;&#64;
       *&#64;&#64;     The default queue policy used for requests that don't require
       *&#64;&#64;     priority handling and requests that specify priority levels where
       *&#64;&#64;     there is no specific policy given. If not specified, a policy with
       *&#64;&#64;     default field values will be used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
       */
      public Builder clearDefaultQueuePolicy() {  copyOnWrite();
        instance.clearDefaultQueuePolicy();
        return this;
      }

      @java.lang.Override

      public int getPriorityQueuePolicyCount() {
        return instance.getPriorityQueuePolicyMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
       *&#64;&#64;
       *&#64;&#64;     Specify the queue policy for the priority level. The default queue
       *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
       *&#64;&#64;     policy.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
       */
      @java.lang.Override

      public boolean containsPriorityQueuePolicy(
          int key) {
        
        return instance.getPriorityQueuePolicyMap().containsKey(key);
      }

      public Builder clearPriorityQueuePolicy() {
        copyOnWrite();
        instance.getMutablePriorityQueuePolicyMap().clear();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
       *&#64;&#64;
       *&#64;&#64;     Specify the queue policy for the priority level. The default queue
       *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
       *&#64;&#64;     policy.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
       */

      public Builder removePriorityQueuePolicy(
          int key) {
        
        copyOnWrite();
        instance.getMutablePriorityQueuePolicyMap().remove(key);
        return this;
      }
      /**
       * Use {@link #getPriorityQueuePolicyMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy> getPriorityQueuePolicy() {
        return getPriorityQueuePolicyMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
       *&#64;&#64;
       *&#64;&#64;     Specify the queue policy for the priority level. The default queue
       *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
       *&#64;&#64;     policy.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
       */
      @java.lang.Override
      public java.util.Map<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy> getPriorityQueuePolicyMap() {
        return java.util.Collections.unmodifiableMap(
            instance.getPriorityQueuePolicyMap());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
       *&#64;&#64;
       *&#64;&#64;     Specify the queue policy for the priority level. The default queue
       *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
       *&#64;&#64;     policy.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
       */
      @java.lang.Override

      public inference.ModelConfigOuterClass.ModelQueuePolicy getPriorityQueuePolicyOrDefault(
          int key,
          inference.ModelConfigOuterClass.ModelQueuePolicy defaultValue) {
        
        java.util.Map<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy> map =
            instance.getPriorityQueuePolicyMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
       *&#64;&#64;
       *&#64;&#64;     Specify the queue policy for the priority level. The default queue
       *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
       *&#64;&#64;     policy.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
       */
      @java.lang.Override

      public inference.ModelConfigOuterClass.ModelQueuePolicy getPriorityQueuePolicyOrThrow(
          int key) {
        
        java.util.Map<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy> map =
            instance.getPriorityQueuePolicyMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
       *&#64;&#64;
       *&#64;&#64;     Specify the queue policy for the priority level. The default queue
       *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
       *&#64;&#64;     policy.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
       */
      public Builder putPriorityQueuePolicy(
          int key,
          inference.ModelConfigOuterClass.ModelQueuePolicy value) {
        
        java.lang.Class<?> valueClass = value.getClass();
        copyOnWrite();
        instance.getMutablePriorityQueuePolicyMap().put(key, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
       *&#64;&#64;
       *&#64;&#64;     Specify the queue policy for the priority level. The default queue
       *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
       *&#64;&#64;     policy.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
       */
      public Builder putAllPriorityQueuePolicy(
          java.util.Map<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy> values) {
        copyOnWrite();
        instance.getMutablePriorityQueuePolicyMap().putAll(values);
        return this;
      }

      // @@protoc_insertion_point(builder_scope:inference.ModelDynamicBatching)
    }
    @java.lang.Override
    @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
    protected final java.lang.Object dynamicMethod(
        com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
        java.lang.Object arg0, java.lang.Object arg1) {
      switch (method) {
        case NEW_MUTABLE_INSTANCE: {
          return new inference.ModelConfigOuterClass.ModelDynamicBatching();
        }
        case NEW_BUILDER: {
          return new Builder();
        }
        case BUILD_MESSAGE_INFO: {
            java.lang.Object[] objects = new java.lang.Object[] {
              "preferredBatchSize_",
              "maxQueueDelayMicroseconds_",
              "preserveOrdering_",
              "priorityLevels_",
              "defaultPriorityLevel_",
              "defaultQueuePolicy_",
              "priorityQueuePolicy_",
              PriorityQueuePolicyDefaultEntryHolder.defaultEntry,
            };
            java.lang.String info =
                "\u0000\u0007\u0000\u0000\u0001\u0007\u0007\u0001\u0001\u0000\u0001\'\u0002\u0003" +
                "\u0003\u0007\u0004\u000b\u0005\u000b\u0006\t\u00072";
            return newMessageInfo(DEFAULT_INSTANCE, info, objects);
        }
        // fall through
        case GET_DEFAULT_INSTANCE: {
          return DEFAULT_INSTANCE;
        }
        case GET_PARSER: {
          com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelDynamicBatching> parser = PARSER;
          if (parser == null) {
            synchronized (inference.ModelConfigOuterClass.ModelDynamicBatching.class) {
              parser = PARSER;
              if (parser == null) {
                parser =
                    new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelDynamicBatching>(
                        DEFAULT_INSTANCE);
                PARSER = parser;
              }
            }
          }
          return parser;
      }
      case GET_MEMOIZED_IS_INITIALIZED: {
        return (byte) 1;
      }
      case SET_MEMOIZED_IS_INITIALIZED: {
        return null;
      }
      }
      throw new UnsupportedOperationException();
    }


    // @@protoc_insertion_point(class_scope:inference.ModelDynamicBatching)
    private static final inference.ModelConfigOuterClass.ModelDynamicBatching DEFAULT_INSTANCE;
    static {
      ModelDynamicBatching defaultInstance = new ModelDynamicBatching();
      // New instances are implicitly immutable so no need to make
      // immutable.
      DEFAULT_INSTANCE = defaultInstance;
      com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
        ModelDynamicBatching.class, defaultInstance);
    }

    public static inference.ModelConfigOuterClass.ModelDynamicBatching getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static volatile com.google.protobuf.Parser<ModelDynamicBatching> PARSER;

    public static com.google.protobuf.Parser<ModelDynamicBatching> parser() {
      return DEFAULT_INSTANCE.getParserForType();
    }
  }

  public interface ModelSequenceBatchingOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelSequenceBatching)
      com.google.protobuf.MessageLiteOrBuilder {

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyDirect direct
     *&#64;&#64;
     *&#64;&#64;       StrategyDirect scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
     * @return Whether the direct field is set.
     */
    boolean hasDirect();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyDirect direct
     *&#64;&#64;
     *&#64;&#64;       StrategyDirect scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
     * @return The direct.
     */
    inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect getDirect();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
     *&#64;&#64;
     *&#64;&#64;       StrategyOldest scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
     * @return Whether the oldest field is set.
     */
    boolean hasOldest();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
     *&#64;&#64;
     *&#64;&#64;       StrategyOldest scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
     * @return The oldest.
     */
    inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest getOldest();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 max_sequence_idle_microseconds
     *&#64;&#64;
     *&#64;&#64;     The maximum time, in microseconds, that a sequence is allowed to
     *&#64;&#64;     be idle before it is aborted. The inference server considers a
     *&#64;&#64;     sequence idle when it does not have any inference request queued
     *&#64;&#64;     for the sequence. If this limit is exceeded, the inference server
     *&#64;&#64;     will free the sequence slot allocated by the sequence and make it
     *&#64;&#64;     available for another sequence. If not specified (or specified as
     *&#64;&#64;     zero) a default value of 1000000 (1 second) is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 max_sequence_idle_microseconds = 1;</code>
     * @return The maxSequenceIdleMicroseconds.
     */
    long getMaxSequenceIdleMicroseconds();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    java.util.List<inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> 
        getControlInputList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput getControlInput(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    int getControlInputCount();

    public inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyChoiceCase getStrategyChoiceCase();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelSequenceBatching
   *&#64;&#64;
   *&#64;&#64;   Sequence batching configuration. These settings control how sequence
   *&#64;&#64;   batching operates for the model.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelSequenceBatching}
   */
  public  static final class ModelSequenceBatching extends
      com.google.protobuf.GeneratedMessageLite<
          ModelSequenceBatching, ModelSequenceBatching.Builder> implements
      // @@protoc_insertion_point(message_implements:inference.ModelSequenceBatching)
      ModelSequenceBatchingOrBuilder {
    private ModelSequenceBatching() {
      controlInput_ = emptyProtobufList();
    }
    public interface ControlOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelSequenceBatching.Control)
        com.google.protobuf.MessageLiteOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this control.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.Control.Kind kind = 1;</code>
       * @return The enum numeric value on the wire for kind.
       */
      int getKindValue();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this control.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.Control.Kind kind = 1;</code>
       * @return The kind.
       */
      inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind getKind();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       * @return A list containing the int32FalseTrue.
       */
      java.util.List<java.lang.Integer> getInt32FalseTrueList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       * @return The count of int32FalseTrue.
       */
      int getInt32FalseTrueCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       * @param index The index of the element to return.
       * @return The int32FalseTrue at the given index.
       */
      int getInt32FalseTrue(int index);

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       * @return A list containing the fp32FalseTrue.
       */
      java.util.List<java.lang.Float> getFp32FalseTrueList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       * @return The count of fp32FalseTrue.
       */
      int getFp32FalseTrueCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       * @param index The index of the element to return.
       * @return The fp32FalseTrue at the given index.
       */
      float getFp32FalseTrue(int index);

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The control's datatype.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 4;</code>
       * @return The enum numeric value on the wire for dataType.
       */
      int getDataTypeValue();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The control's datatype.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 4;</code>
       * @return The dataType.
       */
      inference.ModelConfigOuterClass.DataType getDataType();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message Control
     *&#64;&#64;
     *&#64;&#64;     A control is a signal that the sequence batcher uses to
     *&#64;&#64;     communicate with a backend.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelSequenceBatching.Control}
     */
    public  static final class Control extends
        com.google.protobuf.GeneratedMessageLite<
            Control, Control.Builder> implements
        // @@protoc_insertion_point(message_implements:inference.ModelSequenceBatching.Control)
        ControlOrBuilder {
      private Control() {
        int32FalseTrue_ = emptyIntList();
        fp32FalseTrue_ = emptyFloatList();
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;    .. cpp:enum:: Kind
       *&#64;&#64;
       *&#64;&#64;       The kind of the control.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf enum {@code inference.ModelSequenceBatching.Control.Kind}
       */
      public enum Kind
          implements com.google.protobuf.Internal.EnumLite {
        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_START = 0
         *&#64;&#64;
         *&#64;&#64;         A new sequence is/is-not starting. If true a sequence is
         *&#64;&#64;         starting, if false a sequence is continuing. Must
         *&#64;&#64;         specify either int32_false_true or fp32_false_true for
         *&#64;&#64;         this control. This control is optional.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_START = 0;</code>
         */
        CONTROL_SEQUENCE_START(0),
        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_READY = 1
         *&#64;&#64;
         *&#64;&#64;         A sequence is/is-not ready for inference. If true the
         *&#64;&#64;         input tensor data is valid and should be used. If false
         *&#64;&#64;         the input tensor data is invalid and inferencing should
         *&#64;&#64;         be "skipped".  Must specify either int32_false_true or
         *&#64;&#64;         fp32_false_true for this control. This control is optional.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_READY = 1;</code>
         */
        CONTROL_SEQUENCE_READY(1),
        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_END = 2
         *&#64;&#64;
         *&#64;&#64;         A sequence is/is-not ending. If true a sequence is
         *&#64;&#64;         ending, if false a sequence is continuing. Must
         *&#64;&#64;         specify either int32_false_true or fp32_false_true for
         *&#64;&#64;         this control. This control is optional.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_END = 2;</code>
         */
        CONTROL_SEQUENCE_END(2),
        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_CORRID = 3
         *&#64;&#64;
         *&#64;&#64;         The correlation ID of the sequence. The correlation ID
         *&#64;&#64;         is an uint64_t value that is communicated in whole or
         *&#64;&#64;         in part by the tensor. The tensor's datatype must be
         *&#64;&#64;         specified by data_type and must be TYPE_UINT64, TYPE_INT64,
         *&#64;&#64;         TYPE_UINT32 or TYPE_INT32. If a 32-bit datatype is specified
         *&#64;&#64;         the correlation ID will be truncated to the low-order 32
         *&#64;&#64;         bits. This control is optional.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_CORRID = 3;</code>
         */
        CONTROL_SEQUENCE_CORRID(3),
        UNRECOGNIZED(-1),
        ;

        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_START = 0
         *&#64;&#64;
         *&#64;&#64;         A new sequence is/is-not starting. If true a sequence is
         *&#64;&#64;         starting, if false a sequence is continuing. Must
         *&#64;&#64;         specify either int32_false_true or fp32_false_true for
         *&#64;&#64;         this control. This control is optional.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_START = 0;</code>
         */
        public static final int CONTROL_SEQUENCE_START_VALUE = 0;
        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_READY = 1
         *&#64;&#64;
         *&#64;&#64;         A sequence is/is-not ready for inference. If true the
         *&#64;&#64;         input tensor data is valid and should be used. If false
         *&#64;&#64;         the input tensor data is invalid and inferencing should
         *&#64;&#64;         be "skipped".  Must specify either int32_false_true or
         *&#64;&#64;         fp32_false_true for this control. This control is optional.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_READY = 1;</code>
         */
        public static final int CONTROL_SEQUENCE_READY_VALUE = 1;
        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_END = 2
         *&#64;&#64;
         *&#64;&#64;         A sequence is/is-not ending. If true a sequence is
         *&#64;&#64;         ending, if false a sequence is continuing. Must
         *&#64;&#64;         specify either int32_false_true or fp32_false_true for
         *&#64;&#64;         this control. This control is optional.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_END = 2;</code>
         */
        public static final int CONTROL_SEQUENCE_END_VALUE = 2;
        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_CORRID = 3
         *&#64;&#64;
         *&#64;&#64;         The correlation ID of the sequence. The correlation ID
         *&#64;&#64;         is an uint64_t value that is communicated in whole or
         *&#64;&#64;         in part by the tensor. The tensor's datatype must be
         *&#64;&#64;         specified by data_type and must be TYPE_UINT64, TYPE_INT64,
         *&#64;&#64;         TYPE_UINT32 or TYPE_INT32. If a 32-bit datatype is specified
         *&#64;&#64;         the correlation ID will be truncated to the low-order 32
         *&#64;&#64;         bits. This control is optional.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_CORRID = 3;</code>
         */
        public static final int CONTROL_SEQUENCE_CORRID_VALUE = 3;


        @java.lang.Override
        public final int getNumber() {
          if (this == UNRECOGNIZED) {
            throw new java.lang.IllegalArgumentException(
                "Can't get the number of an unknown enum value.");
          }
          return value;
        }

        /**
         * @param value The number of the enum to look for.
         * @return The enum associated with the given number.
         * @deprecated Use {@link #forNumber(int)} instead.
         */
        @java.lang.Deprecated
        public static Kind valueOf(int value) {
          return forNumber(value);
        }

        public static Kind forNumber(int value) {
          switch (value) {
            case 0: return CONTROL_SEQUENCE_START;
            case 1: return CONTROL_SEQUENCE_READY;
            case 2: return CONTROL_SEQUENCE_END;
            case 3: return CONTROL_SEQUENCE_CORRID;
            default: return null;
          }
        }

        public static com.google.protobuf.Internal.EnumLiteMap<Kind>
            internalGetValueMap() {
          return internalValueMap;
        }
        private static final com.google.protobuf.Internal.EnumLiteMap<
            Kind> internalValueMap =
              new com.google.protobuf.Internal.EnumLiteMap<Kind>() {
                @java.lang.Override
                public Kind findValueByNumber(int number) {
                  return Kind.forNumber(number);
                }
              };

        public static com.google.protobuf.Internal.EnumVerifier 
            internalGetVerifier() {
          return KindVerifier.INSTANCE;
        }

        private static final class KindVerifier implements 
             com.google.protobuf.Internal.EnumVerifier { 
                static final com.google.protobuf.Internal.EnumVerifier           INSTANCE = new KindVerifier();
                @java.lang.Override
                public boolean isInRange(int number) {
                  return Kind.forNumber(number) != null;
                }
              };

        private final int value;

        private Kind(int value) {
          this.value = value;
        }

        // @@protoc_insertion_point(enum_scope:inference.ModelSequenceBatching.Control.Kind)
      }

      public static final int KIND_FIELD_NUMBER = 1;
      private int kind_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this control.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.Control.Kind kind = 1;</code>
       * @return The enum numeric value on the wire for kind.
       */
      @java.lang.Override
      public int getKindValue() {
        return kind_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this control.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.Control.Kind kind = 1;</code>
       * @return The kind.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind getKind() {
        inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind result = inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind.forNumber(kind_);
        return result == null ? inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this control.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.Control.Kind kind = 1;</code>
       * @param value The enum numeric value on the wire for kind to set.
       */
      private void setKindValue(int value) {
          kind_ = value;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this control.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.Control.Kind kind = 1;</code>
       * @param value The kind to set.
       */
      private void setKind(inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind value) {
        kind_ = value.getNumber();
        
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this control.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.Control.Kind kind = 1;</code>
       */
      private void clearKind() {
        
        kind_ = 0;
      }

      public static final int INT32_FALSE_TRUE_FIELD_NUMBER = 2;
      private com.google.protobuf.Internal.IntList int32FalseTrue_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       * @return A list containing the int32FalseTrue.
       */
      @java.lang.Override
      public java.util.List<java.lang.Integer>
          getInt32FalseTrueList() {
        return int32FalseTrue_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       * @return The count of int32FalseTrue.
       */
      @java.lang.Override
      public int getInt32FalseTrueCount() {
        return int32FalseTrue_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       * @param index The index of the element to return.
       * @return The int32FalseTrue at the given index.
       */
      @java.lang.Override
      public int getInt32FalseTrue(int index) {
        return int32FalseTrue_.getInt(index);
      }
      private int int32FalseTrueMemoizedSerializedSize = -1;
      private void ensureInt32FalseTrueIsMutable() {
        com.google.protobuf.Internal.IntList tmp = int32FalseTrue_;
        if (!tmp.isModifiable()) {
          int32FalseTrue_ =
              com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
         }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       * @param index The index to set the value at.
       * @param value The int32FalseTrue to set.
       */
      private void setInt32FalseTrue(
          int index, int value) {
        ensureInt32FalseTrueIsMutable();
        int32FalseTrue_.setInt(index, value);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       * @param value The int32FalseTrue to add.
       */
      private void addInt32FalseTrue(int value) {
        ensureInt32FalseTrueIsMutable();
        int32FalseTrue_.addInt(value);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       * @param values The int32FalseTrue to add.
       */
      private void addAllInt32FalseTrue(
          java.lang.Iterable<? extends java.lang.Integer> values) {
        ensureInt32FalseTrueIsMutable();
        com.google.protobuf.AbstractMessageLite.addAll(
            values, int32FalseTrue_);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       */
      private void clearInt32FalseTrue() {
        int32FalseTrue_ = emptyIntList();
      }

      public static final int FP32_FALSE_TRUE_FIELD_NUMBER = 3;
      private com.google.protobuf.Internal.FloatList fp32FalseTrue_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       * @return A list containing the fp32FalseTrue.
       */
      @java.lang.Override
      public java.util.List<java.lang.Float>
          getFp32FalseTrueList() {
        return fp32FalseTrue_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       * @return The count of fp32FalseTrue.
       */
      @java.lang.Override
      public int getFp32FalseTrueCount() {
        return fp32FalseTrue_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       * @param index The index of the element to return.
       * @return The fp32FalseTrue at the given index.
       */
      @java.lang.Override
      public float getFp32FalseTrue(int index) {
        return fp32FalseTrue_.getFloat(index);
      }
      private int fp32FalseTrueMemoizedSerializedSize = -1;
      private void ensureFp32FalseTrueIsMutable() {
        com.google.protobuf.Internal.FloatList tmp = fp32FalseTrue_;
        if (!tmp.isModifiable()) {
          fp32FalseTrue_ =
              com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
         }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       * @param index The index to set the value at.
       * @param value The fp32FalseTrue to set.
       */
      private void setFp32FalseTrue(
          int index, float value) {
        ensureFp32FalseTrueIsMutable();
        fp32FalseTrue_.setFloat(index, value);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       * @param value The fp32FalseTrue to add.
       */
      private void addFp32FalseTrue(float value) {
        ensureFp32FalseTrueIsMutable();
        fp32FalseTrue_.addFloat(value);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       * @param values The fp32FalseTrue to add.
       */
      private void addAllFp32FalseTrue(
          java.lang.Iterable<? extends java.lang.Float> values) {
        ensureFp32FalseTrueIsMutable();
        com.google.protobuf.AbstractMessageLite.addAll(
            values, fp32FalseTrue_);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       */
      private void clearFp32FalseTrue() {
        fp32FalseTrue_ = emptyFloatList();
      }

      public static final int DATA_TYPE_FIELD_NUMBER = 4;
      private int dataType_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The control's datatype.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 4;</code>
       * @return The enum numeric value on the wire for dataType.
       */
      @java.lang.Override
      public int getDataTypeValue() {
        return dataType_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The control's datatype.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 4;</code>
       * @return The dataType.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.DataType getDataType() {
        inference.ModelConfigOuterClass.DataType result = inference.ModelConfigOuterClass.DataType.forNumber(dataType_);
        return result == null ? inference.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The control's datatype.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 4;</code>
       * @param value The enum numeric value on the wire for dataType to set.
       */
      private void setDataTypeValue(int value) {
          dataType_ = value;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The control's datatype.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 4;</code>
       * @param value The dataType to set.
       */
      private void setDataType(inference.ModelConfigOuterClass.DataType value) {
        dataType_ = value.getNumber();
        
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The control's datatype.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 4;</code>
       */
      private void clearDataType() {
        
        dataType_ = 0;
      }

      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }

      public static Builder newBuilder() {
        return (Builder) DEFAULT_INSTANCE.createBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelSequenceBatching.Control prototype) {
        return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message Control
       *&#64;&#64;
       *&#64;&#64;     A control is a signal that the sequence batcher uses to
       *&#64;&#64;     communicate with a backend.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelSequenceBatching.Control}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageLite.Builder<
            inference.ModelConfigOuterClass.ModelSequenceBatching.Control, Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelSequenceBatching.Control)
          inference.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder {
        // Construct using inference.ModelConfigOuterClass.ModelSequenceBatching.Control.newBuilder()
        private Builder() {
          super(DEFAULT_INSTANCE);
        }


        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Kind kind
         *&#64;&#64;
         *&#64;&#64;       The kind of this control.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.ModelSequenceBatching.Control.Kind kind = 1;</code>
         * @return The enum numeric value on the wire for kind.
         */
        @java.lang.Override
        public int getKindValue() {
          return instance.getKindValue();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Kind kind
         *&#64;&#64;
         *&#64;&#64;       The kind of this control.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.ModelSequenceBatching.Control.Kind kind = 1;</code>
         * @param value The kind to set.
         * @return This builder for chaining.
         */
        public Builder setKindValue(int value) {
          copyOnWrite();
          instance.setKindValue(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Kind kind
         *&#64;&#64;
         *&#64;&#64;       The kind of this control.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.ModelSequenceBatching.Control.Kind kind = 1;</code>
         * @return The kind.
         */
        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind getKind() {
          return instance.getKind();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Kind kind
         *&#64;&#64;
         *&#64;&#64;       The kind of this control.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.ModelSequenceBatching.Control.Kind kind = 1;</code>
         * @param value The enum numeric value on the wire for kind to set.
         * @return This builder for chaining.
         */
        public Builder setKind(inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind value) {
          copyOnWrite();
          instance.setKind(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Kind kind
         *&#64;&#64;
         *&#64;&#64;       The kind of this control.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.ModelSequenceBatching.Control.Kind kind = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearKind() {
          copyOnWrite();
          instance.clearKind();
          return this;
        }

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         * @return A list containing the int32FalseTrue.
         */
        @java.lang.Override
        public java.util.List<java.lang.Integer>
            getInt32FalseTrueList() {
          return java.util.Collections.unmodifiableList(
              instance.getInt32FalseTrueList());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         * @return The count of int32FalseTrue.
         */
        @java.lang.Override
        public int getInt32FalseTrueCount() {
          return instance.getInt32FalseTrueCount();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         * @param index The index of the element to return.
         * @return The int32FalseTrue at the given index.
         */
        @java.lang.Override
        public int getInt32FalseTrue(int index) {
          return instance.getInt32FalseTrue(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         * @param value The int32FalseTrue to set.
         * @return This builder for chaining.
         */
        public Builder setInt32FalseTrue(
            int index, int value) {
          copyOnWrite();
          instance.setInt32FalseTrue(index, value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         * @param value The int32FalseTrue to add.
         * @return This builder for chaining.
         */
        public Builder addInt32FalseTrue(int value) {
          copyOnWrite();
          instance.addInt32FalseTrue(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         * @param values The int32FalseTrue to add.
         * @return This builder for chaining.
         */
        public Builder addAllInt32FalseTrue(
            java.lang.Iterable<? extends java.lang.Integer> values) {
          copyOnWrite();
          instance.addAllInt32FalseTrue(values);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         * @return This builder for chaining.
         */
        public Builder clearInt32FalseTrue() {
          copyOnWrite();
          instance.clearInt32FalseTrue();
          return this;
        }

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         * @return A list containing the fp32FalseTrue.
         */
        @java.lang.Override
        public java.util.List<java.lang.Float>
            getFp32FalseTrueList() {
          return java.util.Collections.unmodifiableList(
              instance.getFp32FalseTrueList());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         * @return The count of fp32FalseTrue.
         */
        @java.lang.Override
        public int getFp32FalseTrueCount() {
          return instance.getFp32FalseTrueCount();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         * @param index The index of the element to return.
         * @return The fp32FalseTrue at the given index.
         */
        @java.lang.Override
        public float getFp32FalseTrue(int index) {
          return instance.getFp32FalseTrue(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         * @param value The fp32FalseTrue to set.
         * @return This builder for chaining.
         */
        public Builder setFp32FalseTrue(
            int index, float value) {
          copyOnWrite();
          instance.setFp32FalseTrue(index, value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         * @param value The fp32FalseTrue to add.
         * @return This builder for chaining.
         */
        public Builder addFp32FalseTrue(float value) {
          copyOnWrite();
          instance.addFp32FalseTrue(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         * @param values The fp32FalseTrue to add.
         * @return This builder for chaining.
         */
        public Builder addAllFp32FalseTrue(
            java.lang.Iterable<? extends java.lang.Float> values) {
          copyOnWrite();
          instance.addAllFp32FalseTrue(values);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         * @return This builder for chaining.
         */
        public Builder clearFp32FalseTrue() {
          copyOnWrite();
          instance.clearFp32FalseTrue();
          return this;
        }

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The control's datatype.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.DataType data_type = 4;</code>
         * @return The enum numeric value on the wire for dataType.
         */
        @java.lang.Override
        public int getDataTypeValue() {
          return instance.getDataTypeValue();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The control's datatype.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.DataType data_type = 4;</code>
         * @param value The dataType to set.
         * @return This builder for chaining.
         */
        public Builder setDataTypeValue(int value) {
          copyOnWrite();
          instance.setDataTypeValue(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The control's datatype.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.DataType data_type = 4;</code>
         * @return The dataType.
         */
        @java.lang.Override
        public inference.ModelConfigOuterClass.DataType getDataType() {
          return instance.getDataType();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The control's datatype.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.DataType data_type = 4;</code>
         * @param value The enum numeric value on the wire for dataType to set.
         * @return This builder for chaining.
         */
        public Builder setDataType(inference.ModelConfigOuterClass.DataType value) {
          copyOnWrite();
          instance.setDataType(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The control's datatype.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.DataType data_type = 4;</code>
         * @return This builder for chaining.
         */
        public Builder clearDataType() {
          copyOnWrite();
          instance.clearDataType();
          return this;
        }

        // @@protoc_insertion_point(builder_scope:inference.ModelSequenceBatching.Control)
      }
      @java.lang.Override
      @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
      protected final java.lang.Object dynamicMethod(
          com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
          java.lang.Object arg0, java.lang.Object arg1) {
        switch (method) {
          case NEW_MUTABLE_INSTANCE: {
            return new inference.ModelConfigOuterClass.ModelSequenceBatching.Control();
          }
          case NEW_BUILDER: {
            return new Builder();
          }
          case BUILD_MESSAGE_INFO: {
              java.lang.Object[] objects = new java.lang.Object[] {
                "kind_",
                "int32FalseTrue_",
                "fp32FalseTrue_",
                "dataType_",
              };
              java.lang.String info =
                  "\u0000\u0004\u0000\u0000\u0001\u0004\u0004\u0000\u0002\u0000\u0001\f\u0002\'\u0003" +
                  "$\u0004\f";
              return newMessageInfo(DEFAULT_INSTANCE, info, objects);
          }
          // fall through
          case GET_DEFAULT_INSTANCE: {
            return DEFAULT_INSTANCE;
          }
          case GET_PARSER: {
            com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelSequenceBatching.Control> parser = PARSER;
            if (parser == null) {
              synchronized (inference.ModelConfigOuterClass.ModelSequenceBatching.Control.class) {
                parser = PARSER;
                if (parser == null) {
                  parser =
                      new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelSequenceBatching.Control>(
                          DEFAULT_INSTANCE);
                  PARSER = parser;
                }
              }
            }
            return parser;
        }
        case GET_MEMOIZED_IS_INITIALIZED: {
          return (byte) 1;
        }
        case SET_MEMOIZED_IS_INITIALIZED: {
          return null;
        }
        }
        throw new UnsupportedOperationException();
      }


      // @@protoc_insertion_point(class_scope:inference.ModelSequenceBatching.Control)
      private static final inference.ModelConfigOuterClass.ModelSequenceBatching.Control DEFAULT_INSTANCE;
      static {
        Control defaultInstance = new Control();
        // New instances are implicitly immutable so no need to make
        // immutable.
        DEFAULT_INSTANCE = defaultInstance;
        com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
          Control.class, defaultInstance);
      }

      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static volatile com.google.protobuf.Parser<Control> PARSER;

      public static com.google.protobuf.Parser<Control> parser() {
        return DEFAULT_INSTANCE.getParserForType();
      }
    }

    public interface ControlInputOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelSequenceBatching.ControlInput)
        com.google.protobuf.MessageLiteOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The name.
       */
      java.lang.String getName();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      com.google.protobuf.ByteString
          getNameBytes();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
       */
      java.util.List<inference.ModelConfigOuterClass.ModelSequenceBatching.Control> 
          getControlList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
       */
      inference.ModelConfigOuterClass.ModelSequenceBatching.Control getControl(int index);
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
       */
      int getControlCount();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message ControlInput
     *&#64;&#64;
     *&#64;&#64;     The sequence control values to communicate by a model input.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelSequenceBatching.ControlInput}
     */
    public  static final class ControlInput extends
        com.google.protobuf.GeneratedMessageLite<
            ControlInput, ControlInput.Builder> implements
        // @@protoc_insertion_point(message_implements:inference.ModelSequenceBatching.ControlInput)
        ControlInputOrBuilder {
      private ControlInput() {
        name_ = "";
        control_ = emptyProtobufList();
      }
      public static final int NAME_FIELD_NUMBER = 1;
      private java.lang.String name_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The name.
       */
      @java.lang.Override
      public java.lang.String getName() {
        return name_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString
          getNameBytes() {
        return com.google.protobuf.ByteString.copyFromUtf8(name_);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The name to set.
       */
      private void setName(
          java.lang.String value) {
        java.lang.Class<?> valueClass = value.getClass();
  
        name_ = value;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      private void clearName() {
        
        name_ = getDefaultInstance().getName();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The bytes for name to set.
       */
      private void setNameBytes(
          com.google.protobuf.ByteString value) {
        checkByteStringIsUtf8(value);
        name_ = value.toStringUtf8();
        
      }

      public static final int CONTROL_FIELD_NUMBER = 2;
      private com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.ModelSequenceBatching.Control> control_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
       */
      @java.lang.Override
      public java.util.List<inference.ModelConfigOuterClass.ModelSequenceBatching.Control> getControlList() {
        return control_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
       */
      public java.util.List<? extends inference.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder> 
          getControlOrBuilderList() {
        return control_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
       */
      @java.lang.Override
      public int getControlCount() {
        return control_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelSequenceBatching.Control getControl(int index) {
        return control_.get(index);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
       */
      public inference.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder getControlOrBuilder(
          int index) {
        return control_.get(index);
      }
      private void ensureControlIsMutable() {
        com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.ModelSequenceBatching.Control> tmp = control_;
        if (!tmp.isModifiable()) {
          control_ =
              com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
         }
      }

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
       */
      private void setControl(
          int index, inference.ModelConfigOuterClass.ModelSequenceBatching.Control value) {
        value.getClass();
  ensureControlIsMutable();
        control_.set(index, value);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
       */
      private void addControl(inference.ModelConfigOuterClass.ModelSequenceBatching.Control value) {
        value.getClass();
  ensureControlIsMutable();
        control_.add(value);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
       */
      private void addControl(
          int index, inference.ModelConfigOuterClass.ModelSequenceBatching.Control value) {
        value.getClass();
  ensureControlIsMutable();
        control_.add(index, value);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
       */
      private void addAllControl(
          java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelSequenceBatching.Control> values) {
        ensureControlIsMutable();
        com.google.protobuf.AbstractMessageLite.addAll(
            values, control_);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
       */
      private void clearControl() {
        control_ = emptyProtobufList();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
       */
      private void removeControl(int index) {
        ensureControlIsMutable();
        control_.remove(index);
      }

      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }

      public static Builder newBuilder() {
        return (Builder) DEFAULT_INSTANCE.createBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput prototype) {
        return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message ControlInput
       *&#64;&#64;
       *&#64;&#64;     The sequence control values to communicate by a model input.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelSequenceBatching.ControlInput}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageLite.Builder<
            inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput, Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelSequenceBatching.ControlInput)
          inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder {
        // Construct using inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.newBuilder()
        private Builder() {
          super(DEFAULT_INSTANCE);
        }


        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @return The name.
         */
        @java.lang.Override
        public java.lang.String getName() {
          return instance.getName();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @return The bytes for name.
         */
        @java.lang.Override
        public com.google.protobuf.ByteString
            getNameBytes() {
          return instance.getNameBytes();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @param value The name to set.
         * @return This builder for chaining.
         */
        public Builder setName(
            java.lang.String value) {
          copyOnWrite();
          instance.setName(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearName() {
          copyOnWrite();
          instance.clearName();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @param value The bytes for name to set.
         * @return This builder for chaining.
         */
        public Builder setNameBytes(
            com.google.protobuf.ByteString value) {
          copyOnWrite();
          instance.setNameBytes(value);
          return this;
        }

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        @java.lang.Override
        public java.util.List<inference.ModelConfigOuterClass.ModelSequenceBatching.Control> getControlList() {
          return java.util.Collections.unmodifiableList(
              instance.getControlList());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        @java.lang.Override
        public int getControlCount() {
          return instance.getControlCount();
        }/**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelSequenceBatching.Control getControl(int index) {
          return instance.getControl(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder setControl(
            int index, inference.ModelConfigOuterClass.ModelSequenceBatching.Control value) {
          copyOnWrite();
          instance.setControl(index, value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder setControl(
            int index, inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder builderForValue) {
          copyOnWrite();
          instance.setControl(index,
              builderForValue.build());
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder addControl(inference.ModelConfigOuterClass.ModelSequenceBatching.Control value) {
          copyOnWrite();
          instance.addControl(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder addControl(
            int index, inference.ModelConfigOuterClass.ModelSequenceBatching.Control value) {
          copyOnWrite();
          instance.addControl(index, value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder addControl(
            inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder builderForValue) {
          copyOnWrite();
          instance.addControl(builderForValue.build());
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder addControl(
            int index, inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder builderForValue) {
          copyOnWrite();
          instance.addControl(index,
              builderForValue.build());
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder addAllControl(
            java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelSequenceBatching.Control> values) {
          copyOnWrite();
          instance.addAllControl(values);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder clearControl() {
          copyOnWrite();
          instance.clearControl();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder removeControl(int index) {
          copyOnWrite();
          instance.removeControl(index);
          return this;
        }

        // @@protoc_insertion_point(builder_scope:inference.ModelSequenceBatching.ControlInput)
      }
      @java.lang.Override
      @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
      protected final java.lang.Object dynamicMethod(
          com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
          java.lang.Object arg0, java.lang.Object arg1) {
        switch (method) {
          case NEW_MUTABLE_INSTANCE: {
            return new inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput();
          }
          case NEW_BUILDER: {
            return new Builder();
          }
          case BUILD_MESSAGE_INFO: {
              java.lang.Object[] objects = new java.lang.Object[] {
                "name_",
                "control_",
                inference.ModelConfigOuterClass.ModelSequenceBatching.Control.class,
              };
              java.lang.String info =
                  "\u0000\u0002\u0000\u0000\u0001\u0002\u0002\u0000\u0001\u0000\u0001\u0208\u0002\u001b" +
                  "";
              return newMessageInfo(DEFAULT_INSTANCE, info, objects);
          }
          // fall through
          case GET_DEFAULT_INSTANCE: {
            return DEFAULT_INSTANCE;
          }
          case GET_PARSER: {
            com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> parser = PARSER;
            if (parser == null) {
              synchronized (inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.class) {
                parser = PARSER;
                if (parser == null) {
                  parser =
                      new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput>(
                          DEFAULT_INSTANCE);
                  PARSER = parser;
                }
              }
            }
            return parser;
        }
        case GET_MEMOIZED_IS_INITIALIZED: {
          return (byte) 1;
        }
        case SET_MEMOIZED_IS_INITIALIZED: {
          return null;
        }
        }
        throw new UnsupportedOperationException();
      }


      // @@protoc_insertion_point(class_scope:inference.ModelSequenceBatching.ControlInput)
      private static final inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput DEFAULT_INSTANCE;
      static {
        ControlInput defaultInstance = new ControlInput();
        // New instances are implicitly immutable so no need to make
        // immutable.
        DEFAULT_INSTANCE = defaultInstance;
        com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
          ControlInput.class, defaultInstance);
      }

      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static volatile com.google.protobuf.Parser<ControlInput> PARSER;

      public static com.google.protobuf.Parser<ControlInput> parser() {
        return DEFAULT_INSTANCE.getParserForType();
      }
    }

    public interface StrategyDirectOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelSequenceBatching.StrategyDirect)
        com.google.protobuf.MessageLiteOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;       The maximum time, in microseconds, a candidate request
       *&#64;&#64;       will be delayed in the sequence batch scheduling queue to
       *&#64;&#64;       wait for additional requests for batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 1;</code>
       * @return The maxQueueDelayMicroseconds.
       */
      long getMaxQueueDelayMicroseconds();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float minimum_slot_utilization
       *&#64;&#64;
       *&#64;&#64;       The minimum slot utilization that must be satisfied to
       *&#64;&#64;       execute the batch before 'max_queue_delay_microseconds' expires.
       *&#64;&#64;       For example, a value of 0.5 indicates that the batch should be
       *&#64;&#64;       executed as soon as 50% or more of the slots are ready even if
       *&#64;&#64;       the 'max_queue_delay_microseconds' timeout has not expired.
       *&#64;&#64;       The default is 0.0, indicating that a batch will be executed
       *&#64;&#64;       before 'max_queue_delay_microseconds' timeout expires if at least
       *&#64;&#64;       one batch slot is ready. 'max_queue_delay_microseconds' will be
       *&#64;&#64;       ignored unless minimum_slot_utilization is set to a non-zero
       *&#64;&#64;       value.
       *&#64;&#64;
       * </pre>
       *
       * <code>float minimum_slot_utilization = 2;</code>
       * @return The minimumSlotUtilization.
       */
      float getMinimumSlotUtilization();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message StrategyDirect
     *&#64;&#64;
     *&#64;&#64;     The sequence batcher uses a specific, unique batch
     *&#64;&#64;     slot for each sequence. All inference requests in a
     *&#64;&#64;     sequence are directed to the same batch slot in the same
     *&#64;&#64;     model instance over the lifetime of the sequence. This
     *&#64;&#64;     is the default strategy.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelSequenceBatching.StrategyDirect}
     */
    public  static final class StrategyDirect extends
        com.google.protobuf.GeneratedMessageLite<
            StrategyDirect, StrategyDirect.Builder> implements
        // @@protoc_insertion_point(message_implements:inference.ModelSequenceBatching.StrategyDirect)
        StrategyDirectOrBuilder {
      private StrategyDirect() {
      }
      public static final int MAX_QUEUE_DELAY_MICROSECONDS_FIELD_NUMBER = 1;
      private long maxQueueDelayMicroseconds_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;       The maximum time, in microseconds, a candidate request
       *&#64;&#64;       will be delayed in the sequence batch scheduling queue to
       *&#64;&#64;       wait for additional requests for batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 1;</code>
       * @return The maxQueueDelayMicroseconds.
       */
      @java.lang.Override
      public long getMaxQueueDelayMicroseconds() {
        return maxQueueDelayMicroseconds_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;       The maximum time, in microseconds, a candidate request
       *&#64;&#64;       will be delayed in the sequence batch scheduling queue to
       *&#64;&#64;       wait for additional requests for batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 1;</code>
       * @param value The maxQueueDelayMicroseconds to set.
       */
      private void setMaxQueueDelayMicroseconds(long value) {
        
        maxQueueDelayMicroseconds_ = value;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;       The maximum time, in microseconds, a candidate request
       *&#64;&#64;       will be delayed in the sequence batch scheduling queue to
       *&#64;&#64;       wait for additional requests for batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 1;</code>
       */
      private void clearMaxQueueDelayMicroseconds() {
        
        maxQueueDelayMicroseconds_ = 0L;
      }

      public static final int MINIMUM_SLOT_UTILIZATION_FIELD_NUMBER = 2;
      private float minimumSlotUtilization_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float minimum_slot_utilization
       *&#64;&#64;
       *&#64;&#64;       The minimum slot utilization that must be satisfied to
       *&#64;&#64;       execute the batch before 'max_queue_delay_microseconds' expires.
       *&#64;&#64;       For example, a value of 0.5 indicates that the batch should be
       *&#64;&#64;       executed as soon as 50% or more of the slots are ready even if
       *&#64;&#64;       the 'max_queue_delay_microseconds' timeout has not expired.
       *&#64;&#64;       The default is 0.0, indicating that a batch will be executed
       *&#64;&#64;       before 'max_queue_delay_microseconds' timeout expires if at least
       *&#64;&#64;       one batch slot is ready. 'max_queue_delay_microseconds' will be
       *&#64;&#64;       ignored unless minimum_slot_utilization is set to a non-zero
       *&#64;&#64;       value.
       *&#64;&#64;
       * </pre>
       *
       * <code>float minimum_slot_utilization = 2;</code>
       * @return The minimumSlotUtilization.
       */
      @java.lang.Override
      public float getMinimumSlotUtilization() {
        return minimumSlotUtilization_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float minimum_slot_utilization
       *&#64;&#64;
       *&#64;&#64;       The minimum slot utilization that must be satisfied to
       *&#64;&#64;       execute the batch before 'max_queue_delay_microseconds' expires.
       *&#64;&#64;       For example, a value of 0.5 indicates that the batch should be
       *&#64;&#64;       executed as soon as 50% or more of the slots are ready even if
       *&#64;&#64;       the 'max_queue_delay_microseconds' timeout has not expired.
       *&#64;&#64;       The default is 0.0, indicating that a batch will be executed
       *&#64;&#64;       before 'max_queue_delay_microseconds' timeout expires if at least
       *&#64;&#64;       one batch slot is ready. 'max_queue_delay_microseconds' will be
       *&#64;&#64;       ignored unless minimum_slot_utilization is set to a non-zero
       *&#64;&#64;       value.
       *&#64;&#64;
       * </pre>
       *
       * <code>float minimum_slot_utilization = 2;</code>
       * @param value The minimumSlotUtilization to set.
       */
      private void setMinimumSlotUtilization(float value) {
        
        minimumSlotUtilization_ = value;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float minimum_slot_utilization
       *&#64;&#64;
       *&#64;&#64;       The minimum slot utilization that must be satisfied to
       *&#64;&#64;       execute the batch before 'max_queue_delay_microseconds' expires.
       *&#64;&#64;       For example, a value of 0.5 indicates that the batch should be
       *&#64;&#64;       executed as soon as 50% or more of the slots are ready even if
       *&#64;&#64;       the 'max_queue_delay_microseconds' timeout has not expired.
       *&#64;&#64;       The default is 0.0, indicating that a batch will be executed
       *&#64;&#64;       before 'max_queue_delay_microseconds' timeout expires if at least
       *&#64;&#64;       one batch slot is ready. 'max_queue_delay_microseconds' will be
       *&#64;&#64;       ignored unless minimum_slot_utilization is set to a non-zero
       *&#64;&#64;       value.
       *&#64;&#64;
       * </pre>
       *
       * <code>float minimum_slot_utilization = 2;</code>
       */
      private void clearMinimumSlotUtilization() {
        
        minimumSlotUtilization_ = 0F;
      }

      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }

      public static Builder newBuilder() {
        return (Builder) DEFAULT_INSTANCE.createBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect prototype) {
        return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message StrategyDirect
       *&#64;&#64;
       *&#64;&#64;     The sequence batcher uses a specific, unique batch
       *&#64;&#64;     slot for each sequence. All inference requests in a
       *&#64;&#64;     sequence are directed to the same batch slot in the same
       *&#64;&#64;     model instance over the lifetime of the sequence. This
       *&#64;&#64;     is the default strategy.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelSequenceBatching.StrategyDirect}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageLite.Builder<
            inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect, Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelSequenceBatching.StrategyDirect)
          inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirectOrBuilder {
        // Construct using inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.newBuilder()
        private Builder() {
          super(DEFAULT_INSTANCE);
        }


        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
         *&#64;&#64;
         *&#64;&#64;       The maximum time, in microseconds, a candidate request
         *&#64;&#64;       will be delayed in the sequence batch scheduling queue to
         *&#64;&#64;       wait for additional requests for batching. Default is 0.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint64 max_queue_delay_microseconds = 1;</code>
         * @return The maxQueueDelayMicroseconds.
         */
        @java.lang.Override
        public long getMaxQueueDelayMicroseconds() {
          return instance.getMaxQueueDelayMicroseconds();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
         *&#64;&#64;
         *&#64;&#64;       The maximum time, in microseconds, a candidate request
         *&#64;&#64;       will be delayed in the sequence batch scheduling queue to
         *&#64;&#64;       wait for additional requests for batching. Default is 0.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint64 max_queue_delay_microseconds = 1;</code>
         * @param value The maxQueueDelayMicroseconds to set.
         * @return This builder for chaining.
         */
        public Builder setMaxQueueDelayMicroseconds(long value) {
          copyOnWrite();
          instance.setMaxQueueDelayMicroseconds(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
         *&#64;&#64;
         *&#64;&#64;       The maximum time, in microseconds, a candidate request
         *&#64;&#64;       will be delayed in the sequence batch scheduling queue to
         *&#64;&#64;       wait for additional requests for batching. Default is 0.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint64 max_queue_delay_microseconds = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearMaxQueueDelayMicroseconds() {
          copyOnWrite();
          instance.clearMaxQueueDelayMicroseconds();
          return this;
        }

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float minimum_slot_utilization
         *&#64;&#64;
         *&#64;&#64;       The minimum slot utilization that must be satisfied to
         *&#64;&#64;       execute the batch before 'max_queue_delay_microseconds' expires.
         *&#64;&#64;       For example, a value of 0.5 indicates that the batch should be
         *&#64;&#64;       executed as soon as 50% or more of the slots are ready even if
         *&#64;&#64;       the 'max_queue_delay_microseconds' timeout has not expired.
         *&#64;&#64;       The default is 0.0, indicating that a batch will be executed
         *&#64;&#64;       before 'max_queue_delay_microseconds' timeout expires if at least
         *&#64;&#64;       one batch slot is ready. 'max_queue_delay_microseconds' will be
         *&#64;&#64;       ignored unless minimum_slot_utilization is set to a non-zero
         *&#64;&#64;       value.
         *&#64;&#64;
         * </pre>
         *
         * <code>float minimum_slot_utilization = 2;</code>
         * @return The minimumSlotUtilization.
         */
        @java.lang.Override
        public float getMinimumSlotUtilization() {
          return instance.getMinimumSlotUtilization();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float minimum_slot_utilization
         *&#64;&#64;
         *&#64;&#64;       The minimum slot utilization that must be satisfied to
         *&#64;&#64;       execute the batch before 'max_queue_delay_microseconds' expires.
         *&#64;&#64;       For example, a value of 0.5 indicates that the batch should be
         *&#64;&#64;       executed as soon as 50% or more of the slots are ready even if
         *&#64;&#64;       the 'max_queue_delay_microseconds' timeout has not expired.
         *&#64;&#64;       The default is 0.0, indicating that a batch will be executed
         *&#64;&#64;       before 'max_queue_delay_microseconds' timeout expires if at least
         *&#64;&#64;       one batch slot is ready. 'max_queue_delay_microseconds' will be
         *&#64;&#64;       ignored unless minimum_slot_utilization is set to a non-zero
         *&#64;&#64;       value.
         *&#64;&#64;
         * </pre>
         *
         * <code>float minimum_slot_utilization = 2;</code>
         * @param value The minimumSlotUtilization to set.
         * @return This builder for chaining.
         */
        public Builder setMinimumSlotUtilization(float value) {
          copyOnWrite();
          instance.setMinimumSlotUtilization(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float minimum_slot_utilization
         *&#64;&#64;
         *&#64;&#64;       The minimum slot utilization that must be satisfied to
         *&#64;&#64;       execute the batch before 'max_queue_delay_microseconds' expires.
         *&#64;&#64;       For example, a value of 0.5 indicates that the batch should be
         *&#64;&#64;       executed as soon as 50% or more of the slots are ready even if
         *&#64;&#64;       the 'max_queue_delay_microseconds' timeout has not expired.
         *&#64;&#64;       The default is 0.0, indicating that a batch will be executed
         *&#64;&#64;       before 'max_queue_delay_microseconds' timeout expires if at least
         *&#64;&#64;       one batch slot is ready. 'max_queue_delay_microseconds' will be
         *&#64;&#64;       ignored unless minimum_slot_utilization is set to a non-zero
         *&#64;&#64;       value.
         *&#64;&#64;
         * </pre>
         *
         * <code>float minimum_slot_utilization = 2;</code>
         * @return This builder for chaining.
         */
        public Builder clearMinimumSlotUtilization() {
          copyOnWrite();
          instance.clearMinimumSlotUtilization();
          return this;
        }

        // @@protoc_insertion_point(builder_scope:inference.ModelSequenceBatching.StrategyDirect)
      }
      @java.lang.Override
      @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
      protected final java.lang.Object dynamicMethod(
          com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
          java.lang.Object arg0, java.lang.Object arg1) {
        switch (method) {
          case NEW_MUTABLE_INSTANCE: {
            return new inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect();
          }
          case NEW_BUILDER: {
            return new Builder();
          }
          case BUILD_MESSAGE_INFO: {
              java.lang.Object[] objects = new java.lang.Object[] {
                "maxQueueDelayMicroseconds_",
                "minimumSlotUtilization_",
              };
              java.lang.String info =
                  "\u0000\u0002\u0000\u0000\u0001\u0002\u0002\u0000\u0000\u0000\u0001\u0003\u0002\u0001" +
                  "";
              return newMessageInfo(DEFAULT_INSTANCE, info, objects);
          }
          // fall through
          case GET_DEFAULT_INSTANCE: {
            return DEFAULT_INSTANCE;
          }
          case GET_PARSER: {
            com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect> parser = PARSER;
            if (parser == null) {
              synchronized (inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.class) {
                parser = PARSER;
                if (parser == null) {
                  parser =
                      new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect>(
                          DEFAULT_INSTANCE);
                  PARSER = parser;
                }
              }
            }
            return parser;
        }
        case GET_MEMOIZED_IS_INITIALIZED: {
          return (byte) 1;
        }
        case SET_MEMOIZED_IS_INITIALIZED: {
          return null;
        }
        }
        throw new UnsupportedOperationException();
      }


      // @@protoc_insertion_point(class_scope:inference.ModelSequenceBatching.StrategyDirect)
      private static final inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect DEFAULT_INSTANCE;
      static {
        StrategyDirect defaultInstance = new StrategyDirect();
        // New instances are implicitly immutable so no need to make
        // immutable.
        DEFAULT_INSTANCE = defaultInstance;
        com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
          StrategyDirect.class, defaultInstance);
      }

      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static volatile com.google.protobuf.Parser<StrategyDirect> PARSER;

      public static com.google.protobuf.Parser<StrategyDirect> parser() {
        return DEFAULT_INSTANCE.getParserForType();
      }
    }

    public interface StrategyOldestOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelSequenceBatching.StrategyOldest)
        com.google.protobuf.MessageLiteOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 max_candidate_sequences
       *&#64;&#64;
       *&#64;&#64;       Maximum number of candidate sequences that the batcher
       *&#64;&#64;       maintains. Excess seqences are kept in an ordered backlog
       *&#64;&#64;       and become candidates when existing candidate sequences
       *&#64;&#64;       complete.
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 max_candidate_sequences = 1;</code>
       * @return The maxCandidateSequences.
       */
      int getMaxCandidateSequences();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
       *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
       *&#64;&#64;       it will be executed immediately. If not specified a
       *&#64;&#64;       preferred batch size will be chosen automatically
       *&#64;&#64;       based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 2;</code>
       * @return A list containing the preferredBatchSize.
       */
      java.util.List<java.lang.Integer> getPreferredBatchSizeList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
       *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
       *&#64;&#64;       it will be executed immediately. If not specified a
       *&#64;&#64;       preferred batch size will be chosen automatically
       *&#64;&#64;       based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 2;</code>
       * @return The count of preferredBatchSize.
       */
      int getPreferredBatchSizeCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
       *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
       *&#64;&#64;       it will be executed immediately. If not specified a
       *&#64;&#64;       preferred batch size will be chosen automatically
       *&#64;&#64;       based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 2;</code>
       * @param index The index of the element to return.
       * @return The preferredBatchSize at the given index.
       */
      int getPreferredBatchSize(int index);

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;       The maximum time, in microseconds, a candidate request
       *&#64;&#64;       will be delayed in the dynamic batch scheduling queue to
       *&#64;&#64;       wait for additional requests for batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 3;</code>
       * @return The maxQueueDelayMicroseconds.
       */
      long getMaxQueueDelayMicroseconds();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message StrategyOldest
     *&#64;&#64;
     *&#64;&#64;     The sequence batcher maintains up to 'max_candidate_sequences'
     *&#64;&#64;     candidate sequences. 'max_candidate_sequences' can be greater
     *&#64;&#64;     than the model's 'max_batch_size'. For inferencing the batcher
     *&#64;&#64;     chooses from the candidate sequences up to 'max_batch_size'
     *&#64;&#64;     inference requests. Requests are chosen in an oldest-first
     *&#64;&#64;     manner across all candidate sequences. A given sequence is
     *&#64;&#64;     not guaranteed to be assigned to the same batch slot for
     *&#64;&#64;     all inference requests of that sequence.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelSequenceBatching.StrategyOldest}
     */
    public  static final class StrategyOldest extends
        com.google.protobuf.GeneratedMessageLite<
            StrategyOldest, StrategyOldest.Builder> implements
        // @@protoc_insertion_point(message_implements:inference.ModelSequenceBatching.StrategyOldest)
        StrategyOldestOrBuilder {
      private StrategyOldest() {
        preferredBatchSize_ = emptyIntList();
      }
      public static final int MAX_CANDIDATE_SEQUENCES_FIELD_NUMBER = 1;
      private int maxCandidateSequences_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 max_candidate_sequences
       *&#64;&#64;
       *&#64;&#64;       Maximum number of candidate sequences that the batcher
       *&#64;&#64;       maintains. Excess seqences are kept in an ordered backlog
       *&#64;&#64;       and become candidates when existing candidate sequences
       *&#64;&#64;       complete.
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 max_candidate_sequences = 1;</code>
       * @return The maxCandidateSequences.
       */
      @java.lang.Override
      public int getMaxCandidateSequences() {
        return maxCandidateSequences_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 max_candidate_sequences
       *&#64;&#64;
       *&#64;&#64;       Maximum number of candidate sequences that the batcher
       *&#64;&#64;       maintains. Excess seqences are kept in an ordered backlog
       *&#64;&#64;       and become candidates when existing candidate sequences
       *&#64;&#64;       complete.
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 max_candidate_sequences = 1;</code>
       * @param value The maxCandidateSequences to set.
       */
      private void setMaxCandidateSequences(int value) {
        
        maxCandidateSequences_ = value;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 max_candidate_sequences
       *&#64;&#64;
       *&#64;&#64;       Maximum number of candidate sequences that the batcher
       *&#64;&#64;       maintains. Excess seqences are kept in an ordered backlog
       *&#64;&#64;       and become candidates when existing candidate sequences
       *&#64;&#64;       complete.
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 max_candidate_sequences = 1;</code>
       */
      private void clearMaxCandidateSequences() {
        
        maxCandidateSequences_ = 0;
      }

      public static final int PREFERRED_BATCH_SIZE_FIELD_NUMBER = 2;
      private com.google.protobuf.Internal.IntList preferredBatchSize_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
       *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
       *&#64;&#64;       it will be executed immediately. If not specified a
       *&#64;&#64;       preferred batch size will be chosen automatically
       *&#64;&#64;       based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 2;</code>
       * @return A list containing the preferredBatchSize.
       */
      @java.lang.Override
      public java.util.List<java.lang.Integer>
          getPreferredBatchSizeList() {
        return preferredBatchSize_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
       *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
       *&#64;&#64;       it will be executed immediately. If not specified a
       *&#64;&#64;       preferred batch size will be chosen automatically
       *&#64;&#64;       based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 2;</code>
       * @return The count of preferredBatchSize.
       */
      @java.lang.Override
      public int getPreferredBatchSizeCount() {
        return preferredBatchSize_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
       *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
       *&#64;&#64;       it will be executed immediately. If not specified a
       *&#64;&#64;       preferred batch size will be chosen automatically
       *&#64;&#64;       based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 2;</code>
       * @param index The index of the element to return.
       * @return The preferredBatchSize at the given index.
       */
      @java.lang.Override
      public int getPreferredBatchSize(int index) {
        return preferredBatchSize_.getInt(index);
      }
      private int preferredBatchSizeMemoizedSerializedSize = -1;
      private void ensurePreferredBatchSizeIsMutable() {
        com.google.protobuf.Internal.IntList tmp = preferredBatchSize_;
        if (!tmp.isModifiable()) {
          preferredBatchSize_ =
              com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
         }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
       *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
       *&#64;&#64;       it will be executed immediately. If not specified a
       *&#64;&#64;       preferred batch size will be chosen automatically
       *&#64;&#64;       based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 2;</code>
       * @param index The index to set the value at.
       * @param value The preferredBatchSize to set.
       */
      private void setPreferredBatchSize(
          int index, int value) {
        ensurePreferredBatchSizeIsMutable();
        preferredBatchSize_.setInt(index, value);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
       *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
       *&#64;&#64;       it will be executed immediately. If not specified a
       *&#64;&#64;       preferred batch size will be chosen automatically
       *&#64;&#64;       based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 2;</code>
       * @param value The preferredBatchSize to add.
       */
      private void addPreferredBatchSize(int value) {
        ensurePreferredBatchSizeIsMutable();
        preferredBatchSize_.addInt(value);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
       *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
       *&#64;&#64;       it will be executed immediately. If not specified a
       *&#64;&#64;       preferred batch size will be chosen automatically
       *&#64;&#64;       based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 2;</code>
       * @param values The preferredBatchSize to add.
       */
      private void addAllPreferredBatchSize(
          java.lang.Iterable<? extends java.lang.Integer> values) {
        ensurePreferredBatchSizeIsMutable();
        com.google.protobuf.AbstractMessageLite.addAll(
            values, preferredBatchSize_);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
       *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
       *&#64;&#64;       it will be executed immediately. If not specified a
       *&#64;&#64;       preferred batch size will be chosen automatically
       *&#64;&#64;       based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 2;</code>
       */
      private void clearPreferredBatchSize() {
        preferredBatchSize_ = emptyIntList();
      }

      public static final int MAX_QUEUE_DELAY_MICROSECONDS_FIELD_NUMBER = 3;
      private long maxQueueDelayMicroseconds_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;       The maximum time, in microseconds, a candidate request
       *&#64;&#64;       will be delayed in the dynamic batch scheduling queue to
       *&#64;&#64;       wait for additional requests for batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 3;</code>
       * @return The maxQueueDelayMicroseconds.
       */
      @java.lang.Override
      public long getMaxQueueDelayMicroseconds() {
        return maxQueueDelayMicroseconds_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;       The maximum time, in microseconds, a candidate request
       *&#64;&#64;       will be delayed in the dynamic batch scheduling queue to
       *&#64;&#64;       wait for additional requests for batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 3;</code>
       * @param value The maxQueueDelayMicroseconds to set.
       */
      private void setMaxQueueDelayMicroseconds(long value) {
        
        maxQueueDelayMicroseconds_ = value;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;       The maximum time, in microseconds, a candidate request
       *&#64;&#64;       will be delayed in the dynamic batch scheduling queue to
       *&#64;&#64;       wait for additional requests for batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 3;</code>
       */
      private void clearMaxQueueDelayMicroseconds() {
        
        maxQueueDelayMicroseconds_ = 0L;
      }

      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }

      public static Builder newBuilder() {
        return (Builder) DEFAULT_INSTANCE.createBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest prototype) {
        return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message StrategyOldest
       *&#64;&#64;
       *&#64;&#64;     The sequence batcher maintains up to 'max_candidate_sequences'
       *&#64;&#64;     candidate sequences. 'max_candidate_sequences' can be greater
       *&#64;&#64;     than the model's 'max_batch_size'. For inferencing the batcher
       *&#64;&#64;     chooses from the candidate sequences up to 'max_batch_size'
       *&#64;&#64;     inference requests. Requests are chosen in an oldest-first
       *&#64;&#64;     manner across all candidate sequences. A given sequence is
       *&#64;&#64;     not guaranteed to be assigned to the same batch slot for
       *&#64;&#64;     all inference requests of that sequence.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelSequenceBatching.StrategyOldest}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageLite.Builder<
            inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest, Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelSequenceBatching.StrategyOldest)
          inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldestOrBuilder {
        // Construct using inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.newBuilder()
        private Builder() {
          super(DEFAULT_INSTANCE);
        }


        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 max_candidate_sequences
         *&#64;&#64;
         *&#64;&#64;       Maximum number of candidate sequences that the batcher
         *&#64;&#64;       maintains. Excess seqences are kept in an ordered backlog
         *&#64;&#64;       and become candidates when existing candidate sequences
         *&#64;&#64;       complete.
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 max_candidate_sequences = 1;</code>
         * @return The maxCandidateSequences.
         */
        @java.lang.Override
        public int getMaxCandidateSequences() {
          return instance.getMaxCandidateSequences();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 max_candidate_sequences
         *&#64;&#64;
         *&#64;&#64;       Maximum number of candidate sequences that the batcher
         *&#64;&#64;       maintains. Excess seqences are kept in an ordered backlog
         *&#64;&#64;       and become candidates when existing candidate sequences
         *&#64;&#64;       complete.
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 max_candidate_sequences = 1;</code>
         * @param value The maxCandidateSequences to set.
         * @return This builder for chaining.
         */
        public Builder setMaxCandidateSequences(int value) {
          copyOnWrite();
          instance.setMaxCandidateSequences(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 max_candidate_sequences
         *&#64;&#64;
         *&#64;&#64;       Maximum number of candidate sequences that the batcher
         *&#64;&#64;       maintains. Excess seqences are kept in an ordered backlog
         *&#64;&#64;       and become candidates when existing candidate sequences
         *&#64;&#64;       complete.
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 max_candidate_sequences = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearMaxCandidateSequences() {
          copyOnWrite();
          instance.clearMaxCandidateSequences();
          return this;
        }

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
         *&#64;&#64;
         *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
         *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
         *&#64;&#64;       it will be executed immediately. If not specified a
         *&#64;&#64;       preferred batch size will be chosen automatically
         *&#64;&#64;       based on model and GPU characteristics.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 preferred_batch_size = 2;</code>
         * @return A list containing the preferredBatchSize.
         */
        @java.lang.Override
        public java.util.List<java.lang.Integer>
            getPreferredBatchSizeList() {
          return java.util.Collections.unmodifiableList(
              instance.getPreferredBatchSizeList());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
         *&#64;&#64;
         *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
         *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
         *&#64;&#64;       it will be executed immediately. If not specified a
         *&#64;&#64;       preferred batch size will be chosen automatically
         *&#64;&#64;       based on model and GPU characteristics.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 preferred_batch_size = 2;</code>
         * @return The count of preferredBatchSize.
         */
        @java.lang.Override
        public int getPreferredBatchSizeCount() {
          return instance.getPreferredBatchSizeCount();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
         *&#64;&#64;
         *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
         *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
         *&#64;&#64;       it will be executed immediately. If not specified a
         *&#64;&#64;       preferred batch size will be chosen automatically
         *&#64;&#64;       based on model and GPU characteristics.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 preferred_batch_size = 2;</code>
         * @param index The index of the element to return.
         * @return The preferredBatchSize at the given index.
         */
        @java.lang.Override
        public int getPreferredBatchSize(int index) {
          return instance.getPreferredBatchSize(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
         *&#64;&#64;
         *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
         *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
         *&#64;&#64;       it will be executed immediately. If not specified a
         *&#64;&#64;       preferred batch size will be chosen automatically
         *&#64;&#64;       based on model and GPU characteristics.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 preferred_batch_size = 2;</code>
         * @param value The preferredBatchSize to set.
         * @return This builder for chaining.
         */
        public Builder setPreferredBatchSize(
            int index, int value) {
          copyOnWrite();
          instance.setPreferredBatchSize(index, value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
         *&#64;&#64;
         *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
         *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
         *&#64;&#64;       it will be executed immediately. If not specified a
         *&#64;&#64;       preferred batch size will be chosen automatically
         *&#64;&#64;       based on model and GPU characteristics.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 preferred_batch_size = 2;</code>
         * @param value The preferredBatchSize to add.
         * @return This builder for chaining.
         */
        public Builder addPreferredBatchSize(int value) {
          copyOnWrite();
          instance.addPreferredBatchSize(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
         *&#64;&#64;
         *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
         *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
         *&#64;&#64;       it will be executed immediately. If not specified a
         *&#64;&#64;       preferred batch size will be chosen automatically
         *&#64;&#64;       based on model and GPU characteristics.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 preferred_batch_size = 2;</code>
         * @param values The preferredBatchSize to add.
         * @return This builder for chaining.
         */
        public Builder addAllPreferredBatchSize(
            java.lang.Iterable<? extends java.lang.Integer> values) {
          copyOnWrite();
          instance.addAllPreferredBatchSize(values);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
         *&#64;&#64;
         *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
         *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
         *&#64;&#64;       it will be executed immediately. If not specified a
         *&#64;&#64;       preferred batch size will be chosen automatically
         *&#64;&#64;       based on model and GPU characteristics.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 preferred_batch_size = 2;</code>
         * @return This builder for chaining.
         */
        public Builder clearPreferredBatchSize() {
          copyOnWrite();
          instance.clearPreferredBatchSize();
          return this;
        }

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
         *&#64;&#64;
         *&#64;&#64;       The maximum time, in microseconds, a candidate request
         *&#64;&#64;       will be delayed in the dynamic batch scheduling queue to
         *&#64;&#64;       wait for additional requests for batching. Default is 0.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint64 max_queue_delay_microseconds = 3;</code>
         * @return The maxQueueDelayMicroseconds.
         */
        @java.lang.Override
        public long getMaxQueueDelayMicroseconds() {
          return instance.getMaxQueueDelayMicroseconds();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
         *&#64;&#64;
         *&#64;&#64;       The maximum time, in microseconds, a candidate request
         *&#64;&#64;       will be delayed in the dynamic batch scheduling queue to
         *&#64;&#64;       wait for additional requests for batching. Default is 0.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint64 max_queue_delay_microseconds = 3;</code>
         * @param value The maxQueueDelayMicroseconds to set.
         * @return This builder for chaining.
         */
        public Builder setMaxQueueDelayMicroseconds(long value) {
          copyOnWrite();
          instance.setMaxQueueDelayMicroseconds(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
         *&#64;&#64;
         *&#64;&#64;       The maximum time, in microseconds, a candidate request
         *&#64;&#64;       will be delayed in the dynamic batch scheduling queue to
         *&#64;&#64;       wait for additional requests for batching. Default is 0.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint64 max_queue_delay_microseconds = 3;</code>
         * @return This builder for chaining.
         */
        public Builder clearMaxQueueDelayMicroseconds() {
          copyOnWrite();
          instance.clearMaxQueueDelayMicroseconds();
          return this;
        }

        // @@protoc_insertion_point(builder_scope:inference.ModelSequenceBatching.StrategyOldest)
      }
      @java.lang.Override
      @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
      protected final java.lang.Object dynamicMethod(
          com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
          java.lang.Object arg0, java.lang.Object arg1) {
        switch (method) {
          case NEW_MUTABLE_INSTANCE: {
            return new inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest();
          }
          case NEW_BUILDER: {
            return new Builder();
          }
          case BUILD_MESSAGE_INFO: {
              java.lang.Object[] objects = new java.lang.Object[] {
                "maxCandidateSequences_",
                "preferredBatchSize_",
                "maxQueueDelayMicroseconds_",
              };
              java.lang.String info =
                  "\u0000\u0003\u0000\u0000\u0001\u0003\u0003\u0000\u0001\u0000\u0001\u0004\u0002\'" +
                  "\u0003\u0003";
              return newMessageInfo(DEFAULT_INSTANCE, info, objects);
          }
          // fall through
          case GET_DEFAULT_INSTANCE: {
            return DEFAULT_INSTANCE;
          }
          case GET_PARSER: {
            com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest> parser = PARSER;
            if (parser == null) {
              synchronized (inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.class) {
                parser = PARSER;
                if (parser == null) {
                  parser =
                      new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest>(
                          DEFAULT_INSTANCE);
                  PARSER = parser;
                }
              }
            }
            return parser;
        }
        case GET_MEMOIZED_IS_INITIALIZED: {
          return (byte) 1;
        }
        case SET_MEMOIZED_IS_INITIALIZED: {
          return null;
        }
        }
        throw new UnsupportedOperationException();
      }


      // @@protoc_insertion_point(class_scope:inference.ModelSequenceBatching.StrategyOldest)
      private static final inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest DEFAULT_INSTANCE;
      static {
        StrategyOldest defaultInstance = new StrategyOldest();
        // New instances are implicitly immutable so no need to make
        // immutable.
        DEFAULT_INSTANCE = defaultInstance;
        com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
          StrategyOldest.class, defaultInstance);
      }

      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static volatile com.google.protobuf.Parser<StrategyOldest> PARSER;

      public static com.google.protobuf.Parser<StrategyOldest> parser() {
        return DEFAULT_INSTANCE.getParserForType();
      }
    }

    private int strategyChoiceCase_ = 0;
    private java.lang.Object strategyChoice_;
    public enum StrategyChoiceCase {
      DIRECT(3),
      OLDEST(4),
      STRATEGYCHOICE_NOT_SET(0);
      private final int value;
      private StrategyChoiceCase(int value) {
        this.value = value;
      }
      /**
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static StrategyChoiceCase valueOf(int value) {
        return forNumber(value);
      }

      public static StrategyChoiceCase forNumber(int value) {
        switch (value) {
          case 3: return DIRECT;
          case 4: return OLDEST;
          case 0: return STRATEGYCHOICE_NOT_SET;
          default: return null;
        }
      }
      public int getNumber() {
        return this.value;
      }
    };

    @java.lang.Override
    public StrategyChoiceCase
    getStrategyChoiceCase() {
      return StrategyChoiceCase.forNumber(
          strategyChoiceCase_);
    }

    private void clearStrategyChoice() {
      strategyChoiceCase_ = 0;
      strategyChoice_ = null;
    }

    public static final int DIRECT_FIELD_NUMBER = 3;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyDirect direct
     *&#64;&#64;
     *&#64;&#64;       StrategyDirect scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
     */
    @java.lang.Override
    public boolean hasDirect() {
      return strategyChoiceCase_ == 3;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyDirect direct
     *&#64;&#64;
     *&#64;&#64;       StrategyDirect scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect getDirect() {
      if (strategyChoiceCase_ == 3) {
         return (inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) strategyChoice_;
      }
      return inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyDirect direct
     *&#64;&#64;
     *&#64;&#64;       StrategyDirect scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
     */
    private void setDirect(inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect value) {
      value.getClass();
  strategyChoice_ = value;
      strategyChoiceCase_ = 3;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyDirect direct
     *&#64;&#64;
     *&#64;&#64;       StrategyDirect scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
     */
    private void mergeDirect(inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect value) {
      value.getClass();
  if (strategyChoiceCase_ == 3 &&
          strategyChoice_ != inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.getDefaultInstance()) {
        strategyChoice_ = inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.newBuilder((inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) strategyChoice_)
            .mergeFrom(value).buildPartial();
      } else {
        strategyChoice_ = value;
      }
      strategyChoiceCase_ = 3;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyDirect direct
     *&#64;&#64;
     *&#64;&#64;       StrategyDirect scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
     */
    private void clearDirect() {
      if (strategyChoiceCase_ == 3) {
        strategyChoiceCase_ = 0;
        strategyChoice_ = null;
      }
    }

    public static final int OLDEST_FIELD_NUMBER = 4;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
     *&#64;&#64;
     *&#64;&#64;       StrategyOldest scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
     */
    @java.lang.Override
    public boolean hasOldest() {
      return strategyChoiceCase_ == 4;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
     *&#64;&#64;
     *&#64;&#64;       StrategyOldest scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest getOldest() {
      if (strategyChoiceCase_ == 4) {
         return (inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) strategyChoice_;
      }
      return inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
     *&#64;&#64;
     *&#64;&#64;       StrategyOldest scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
     */
    private void setOldest(inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest value) {
      value.getClass();
  strategyChoice_ = value;
      strategyChoiceCase_ = 4;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
     *&#64;&#64;
     *&#64;&#64;       StrategyOldest scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
     */
    private void mergeOldest(inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest value) {
      value.getClass();
  if (strategyChoiceCase_ == 4 &&
          strategyChoice_ != inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.getDefaultInstance()) {
        strategyChoice_ = inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.newBuilder((inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) strategyChoice_)
            .mergeFrom(value).buildPartial();
      } else {
        strategyChoice_ = value;
      }
      strategyChoiceCase_ = 4;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
     *&#64;&#64;
     *&#64;&#64;       StrategyOldest scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
     */
    private void clearOldest() {
      if (strategyChoiceCase_ == 4) {
        strategyChoiceCase_ = 0;
        strategyChoice_ = null;
      }
    }

    public static final int MAX_SEQUENCE_IDLE_MICROSECONDS_FIELD_NUMBER = 1;
    private long maxSequenceIdleMicroseconds_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 max_sequence_idle_microseconds
     *&#64;&#64;
     *&#64;&#64;     The maximum time, in microseconds, that a sequence is allowed to
     *&#64;&#64;     be idle before it is aborted. The inference server considers a
     *&#64;&#64;     sequence idle when it does not have any inference request queued
     *&#64;&#64;     for the sequence. If this limit is exceeded, the inference server
     *&#64;&#64;     will free the sequence slot allocated by the sequence and make it
     *&#64;&#64;     available for another sequence. If not specified (or specified as
     *&#64;&#64;     zero) a default value of 1000000 (1 second) is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 max_sequence_idle_microseconds = 1;</code>
     * @return The maxSequenceIdleMicroseconds.
     */
    @java.lang.Override
    public long getMaxSequenceIdleMicroseconds() {
      return maxSequenceIdleMicroseconds_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 max_sequence_idle_microseconds
     *&#64;&#64;
     *&#64;&#64;     The maximum time, in microseconds, that a sequence is allowed to
     *&#64;&#64;     be idle before it is aborted. The inference server considers a
     *&#64;&#64;     sequence idle when it does not have any inference request queued
     *&#64;&#64;     for the sequence. If this limit is exceeded, the inference server
     *&#64;&#64;     will free the sequence slot allocated by the sequence and make it
     *&#64;&#64;     available for another sequence. If not specified (or specified as
     *&#64;&#64;     zero) a default value of 1000000 (1 second) is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 max_sequence_idle_microseconds = 1;</code>
     * @param value The maxSequenceIdleMicroseconds to set.
     */
    private void setMaxSequenceIdleMicroseconds(long value) {
      
      maxSequenceIdleMicroseconds_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 max_sequence_idle_microseconds
     *&#64;&#64;
     *&#64;&#64;     The maximum time, in microseconds, that a sequence is allowed to
     *&#64;&#64;     be idle before it is aborted. The inference server considers a
     *&#64;&#64;     sequence idle when it does not have any inference request queued
     *&#64;&#64;     for the sequence. If this limit is exceeded, the inference server
     *&#64;&#64;     will free the sequence slot allocated by the sequence and make it
     *&#64;&#64;     available for another sequence. If not specified (or specified as
     *&#64;&#64;     zero) a default value of 1000000 (1 second) is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 max_sequence_idle_microseconds = 1;</code>
     */
    private void clearMaxSequenceIdleMicroseconds() {
      
      maxSequenceIdleMicroseconds_ = 0L;
    }

    public static final int CONTROL_INPUT_FIELD_NUMBER = 2;
    private com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> controlInput_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    @java.lang.Override
    public java.util.List<inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> getControlInputList() {
      return controlInput_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    public java.util.List<? extends inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder> 
        getControlInputOrBuilderList() {
      return controlInput_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    @java.lang.Override
    public int getControlInputCount() {
      return controlInput_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput getControlInput(int index) {
      return controlInput_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    public inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder getControlInputOrBuilder(
        int index) {
      return controlInput_.get(index);
    }
    private void ensureControlInputIsMutable() {
      com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> tmp = controlInput_;
      if (!tmp.isModifiable()) {
        controlInput_ =
            com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
       }
    }

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    private void setControlInput(
        int index, inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput value) {
      value.getClass();
  ensureControlInputIsMutable();
      controlInput_.set(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    private void addControlInput(inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput value) {
      value.getClass();
  ensureControlInputIsMutable();
      controlInput_.add(value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    private void addControlInput(
        int index, inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput value) {
      value.getClass();
  ensureControlInputIsMutable();
      controlInput_.add(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    private void addAllControlInput(
        java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> values) {
      ensureControlInputIsMutable();
      com.google.protobuf.AbstractMessageLite.addAll(
          values, controlInput_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    private void clearControlInput() {
      controlInput_ = emptyProtobufList();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    private void removeControlInput(int index) {
      ensureControlInputIsMutable();
      controlInput_.remove(index);
    }

    public static inference.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelSequenceBatching parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelSequenceBatching parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelSequenceBatching parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelSequenceBatching parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }

    public static Builder newBuilder() {
      return (Builder) DEFAULT_INSTANCE.createBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelSequenceBatching prototype) {
      return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelSequenceBatching
     *&#64;&#64;
     *&#64;&#64;   Sequence batching configuration. These settings control how sequence
     *&#64;&#64;   batching operates for the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelSequenceBatching}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageLite.Builder<
          inference.ModelConfigOuterClass.ModelSequenceBatching, Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelSequenceBatching)
        inference.ModelConfigOuterClass.ModelSequenceBatchingOrBuilder {
      // Construct using inference.ModelConfigOuterClass.ModelSequenceBatching.newBuilder()
      private Builder() {
        super(DEFAULT_INSTANCE);
      }

      @java.lang.Override
      public StrategyChoiceCase
          getStrategyChoiceCase() {
        return instance.getStrategyChoiceCase();
      }

      public Builder clearStrategyChoice() {
        copyOnWrite();
        instance.clearStrategyChoice();
        return this;
      }


      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyDirect direct
       *&#64;&#64;
       *&#64;&#64;       StrategyDirect scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
       */
      @java.lang.Override
      public boolean hasDirect() {
        return instance.hasDirect();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyDirect direct
       *&#64;&#64;
       *&#64;&#64;       StrategyDirect scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect getDirect() {
        return instance.getDirect();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyDirect direct
       *&#64;&#64;
       *&#64;&#64;       StrategyDirect scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
       */
      public Builder setDirect(inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect value) {
        copyOnWrite();
        instance.setDirect(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyDirect direct
       *&#64;&#64;
       *&#64;&#64;       StrategyDirect scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
       */
      public Builder setDirect(
          inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.Builder builderForValue) {
        copyOnWrite();
        instance.setDirect(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyDirect direct
       *&#64;&#64;
       *&#64;&#64;       StrategyDirect scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
       */
      public Builder mergeDirect(inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect value) {
        copyOnWrite();
        instance.mergeDirect(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyDirect direct
       *&#64;&#64;
       *&#64;&#64;       StrategyDirect scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
       */
      public Builder clearDirect() {
        copyOnWrite();
        instance.clearDirect();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
       *&#64;&#64;
       *&#64;&#64;       StrategyOldest scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
       */
      @java.lang.Override
      public boolean hasOldest() {
        return instance.hasOldest();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
       *&#64;&#64;
       *&#64;&#64;       StrategyOldest scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest getOldest() {
        return instance.getOldest();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
       *&#64;&#64;
       *&#64;&#64;       StrategyOldest scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
       */
      public Builder setOldest(inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest value) {
        copyOnWrite();
        instance.setOldest(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
       *&#64;&#64;
       *&#64;&#64;       StrategyOldest scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
       */
      public Builder setOldest(
          inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.Builder builderForValue) {
        copyOnWrite();
        instance.setOldest(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
       *&#64;&#64;
       *&#64;&#64;       StrategyOldest scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
       */
      public Builder mergeOldest(inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest value) {
        copyOnWrite();
        instance.mergeOldest(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
       *&#64;&#64;
       *&#64;&#64;       StrategyOldest scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
       */
      public Builder clearOldest() {
        copyOnWrite();
        instance.clearOldest();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 max_sequence_idle_microseconds
       *&#64;&#64;
       *&#64;&#64;     The maximum time, in microseconds, that a sequence is allowed to
       *&#64;&#64;     be idle before it is aborted. The inference server considers a
       *&#64;&#64;     sequence idle when it does not have any inference request queued
       *&#64;&#64;     for the sequence. If this limit is exceeded, the inference server
       *&#64;&#64;     will free the sequence slot allocated by the sequence and make it
       *&#64;&#64;     available for another sequence. If not specified (or specified as
       *&#64;&#64;     zero) a default value of 1000000 (1 second) is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_sequence_idle_microseconds = 1;</code>
       * @return The maxSequenceIdleMicroseconds.
       */
      @java.lang.Override
      public long getMaxSequenceIdleMicroseconds() {
        return instance.getMaxSequenceIdleMicroseconds();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 max_sequence_idle_microseconds
       *&#64;&#64;
       *&#64;&#64;     The maximum time, in microseconds, that a sequence is allowed to
       *&#64;&#64;     be idle before it is aborted. The inference server considers a
       *&#64;&#64;     sequence idle when it does not have any inference request queued
       *&#64;&#64;     for the sequence. If this limit is exceeded, the inference server
       *&#64;&#64;     will free the sequence slot allocated by the sequence and make it
       *&#64;&#64;     available for another sequence. If not specified (or specified as
       *&#64;&#64;     zero) a default value of 1000000 (1 second) is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_sequence_idle_microseconds = 1;</code>
       * @param value The maxSequenceIdleMicroseconds to set.
       * @return This builder for chaining.
       */
      public Builder setMaxSequenceIdleMicroseconds(long value) {
        copyOnWrite();
        instance.setMaxSequenceIdleMicroseconds(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 max_sequence_idle_microseconds
       *&#64;&#64;
       *&#64;&#64;     The maximum time, in microseconds, that a sequence is allowed to
       *&#64;&#64;     be idle before it is aborted. The inference server considers a
       *&#64;&#64;     sequence idle when it does not have any inference request queued
       *&#64;&#64;     for the sequence. If this limit is exceeded, the inference server
       *&#64;&#64;     will free the sequence slot allocated by the sequence and make it
       *&#64;&#64;     available for another sequence. If not specified (or specified as
       *&#64;&#64;     zero) a default value of 1000000 (1 second) is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_sequence_idle_microseconds = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearMaxSequenceIdleMicroseconds() {
        copyOnWrite();
        instance.clearMaxSequenceIdleMicroseconds();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      @java.lang.Override
      public java.util.List<inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> getControlInputList() {
        return java.util.Collections.unmodifiableList(
            instance.getControlInputList());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      @java.lang.Override
      public int getControlInputCount() {
        return instance.getControlInputCount();
      }/**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput getControlInput(int index) {
        return instance.getControlInput(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder setControlInput(
          int index, inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput value) {
        copyOnWrite();
        instance.setControlInput(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder setControlInput(
          int index, inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder builderForValue) {
        copyOnWrite();
        instance.setControlInput(index,
            builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder addControlInput(inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput value) {
        copyOnWrite();
        instance.addControlInput(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder addControlInput(
          int index, inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput value) {
        copyOnWrite();
        instance.addControlInput(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder addControlInput(
          inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder builderForValue) {
        copyOnWrite();
        instance.addControlInput(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder addControlInput(
          int index, inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder builderForValue) {
        copyOnWrite();
        instance.addControlInput(index,
            builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder addAllControlInput(
          java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> values) {
        copyOnWrite();
        instance.addAllControlInput(values);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder clearControlInput() {
        copyOnWrite();
        instance.clearControlInput();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder removeControlInput(int index) {
        copyOnWrite();
        instance.removeControlInput(index);
        return this;
      }

      // @@protoc_insertion_point(builder_scope:inference.ModelSequenceBatching)
    }
    @java.lang.Override
    @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
    protected final java.lang.Object dynamicMethod(
        com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
        java.lang.Object arg0, java.lang.Object arg1) {
      switch (method) {
        case NEW_MUTABLE_INSTANCE: {
          return new inference.ModelConfigOuterClass.ModelSequenceBatching();
        }
        case NEW_BUILDER: {
          return new Builder();
        }
        case BUILD_MESSAGE_INFO: {
            java.lang.Object[] objects = new java.lang.Object[] {
              "strategyChoice_",
              "strategyChoiceCase_",
              "maxSequenceIdleMicroseconds_",
              "controlInput_",
              inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.class,
              inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.class,
              inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.class,
            };
            java.lang.String info =
                "\u0000\u0004\u0001\u0000\u0001\u0004\u0004\u0000\u0001\u0000\u0001\u0003\u0002\u001b" +
                "\u0003<\u0000\u0004<\u0000";
            return newMessageInfo(DEFAULT_INSTANCE, info, objects);
        }
        // fall through
        case GET_DEFAULT_INSTANCE: {
          return DEFAULT_INSTANCE;
        }
        case GET_PARSER: {
          com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelSequenceBatching> parser = PARSER;
          if (parser == null) {
            synchronized (inference.ModelConfigOuterClass.ModelSequenceBatching.class) {
              parser = PARSER;
              if (parser == null) {
                parser =
                    new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelSequenceBatching>(
                        DEFAULT_INSTANCE);
                PARSER = parser;
              }
            }
          }
          return parser;
      }
      case GET_MEMOIZED_IS_INITIALIZED: {
        return (byte) 1;
      }
      case SET_MEMOIZED_IS_INITIALIZED: {
        return null;
      }
      }
      throw new UnsupportedOperationException();
    }


    // @@protoc_insertion_point(class_scope:inference.ModelSequenceBatching)
    private static final inference.ModelConfigOuterClass.ModelSequenceBatching DEFAULT_INSTANCE;
    static {
      ModelSequenceBatching defaultInstance = new ModelSequenceBatching();
      // New instances are implicitly immutable so no need to make
      // immutable.
      DEFAULT_INSTANCE = defaultInstance;
      com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
        ModelSequenceBatching.class, defaultInstance);
    }

    public static inference.ModelConfigOuterClass.ModelSequenceBatching getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static volatile com.google.protobuf.Parser<ModelSequenceBatching> PARSER;

    public static com.google.protobuf.Parser<ModelSequenceBatching> parser() {
      return DEFAULT_INSTANCE.getParserForType();
    }
  }

  public interface ModelEnsemblingOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelEnsembling)
      com.google.protobuf.MessageLiteOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
     */
    java.util.List<inference.ModelConfigOuterClass.ModelEnsembling.Step> 
        getStepList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
     */
    inference.ModelConfigOuterClass.ModelEnsembling.Step getStep(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
     */
    int getStepCount();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelEnsembling
   *&#64;&#64;
   *&#64;&#64;   Model ensembling configuration. These settings specify the models that
   *&#64;&#64;   compose the ensemble and how data flows between the models.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelEnsembling}
   */
  public  static final class ModelEnsembling extends
      com.google.protobuf.GeneratedMessageLite<
          ModelEnsembling, ModelEnsembling.Builder> implements
      // @@protoc_insertion_point(message_implements:inference.ModelEnsembling)
      ModelEnsemblingOrBuilder {
    private ModelEnsembling() {
      step_ = emptyProtobufList();
    }
    public interface StepOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelEnsembling.Step)
        com.google.protobuf.MessageLiteOrBuilder {

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string model_name
       *&#64;&#64;
       *&#64;&#64;     The name of the model to execute for this step of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>string model_name = 1;</code>
       * @return The modelName.
       */
      java.lang.String getModelName();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string model_name
       *&#64;&#64;
       *&#64;&#64;     The name of the model to execute for this step of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>string model_name = 1;</code>
       * @return The bytes for modelName.
       */
      com.google.protobuf.ByteString
          getModelNameBytes();

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 model_version
       *&#64;&#64;
       *&#64;&#64;     The version of the model to use for inference. If -1
       *&#64;&#64;     the latest/most-recent version of the model is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>int64 model_version = 2;</code>
       * @return The modelVersion.
       */
      long getModelVersion();

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */
      int getInputMapCount();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */
      boolean containsInputMap(
          java.lang.String key);
      /**
       * Use {@link #getInputMapMap()} instead.
       */
      @java.lang.Deprecated
      java.util.Map<java.lang.String, java.lang.String>
      getInputMap();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */
      java.util.Map<java.lang.String, java.lang.String>
      getInputMapMap();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */

      java.lang.String getInputMapOrDefault(
          java.lang.String key,
          java.lang.String defaultValue);
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */

      java.lang.String getInputMapOrThrow(
          java.lang.String key);

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */
      int getOutputMapCount();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */
      boolean containsOutputMap(
          java.lang.String key);
      /**
       * Use {@link #getOutputMapMap()} instead.
       */
      @java.lang.Deprecated
      java.util.Map<java.lang.String, java.lang.String>
      getOutputMap();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */
      java.util.Map<java.lang.String, java.lang.String>
      getOutputMapMap();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */

      java.lang.String getOutputMapOrDefault(
          java.lang.String key,
          java.lang.String defaultValue);
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */

      java.lang.String getOutputMapOrThrow(
          java.lang.String key);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message Step
     *&#64;&#64;
     *&#64;&#64;     Each step specifies a model included in the ensemble,
     *&#64;&#64;     maps ensemble tensor names to the model input tensors,
     *&#64;&#64;     and maps model output tensors to ensemble tensor names
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelEnsembling.Step}
     */
    public  static final class Step extends
        com.google.protobuf.GeneratedMessageLite<
            Step, Step.Builder> implements
        // @@protoc_insertion_point(message_implements:inference.ModelEnsembling.Step)
        StepOrBuilder {
      private Step() {
        modelName_ = "";
      }
      public static final int MODEL_NAME_FIELD_NUMBER = 1;
      private java.lang.String modelName_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string model_name
       *&#64;&#64;
       *&#64;&#64;     The name of the model to execute for this step of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>string model_name = 1;</code>
       * @return The modelName.
       */
      @java.lang.Override
      public java.lang.String getModelName() {
        return modelName_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string model_name
       *&#64;&#64;
       *&#64;&#64;     The name of the model to execute for this step of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>string model_name = 1;</code>
       * @return The bytes for modelName.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString
          getModelNameBytes() {
        return com.google.protobuf.ByteString.copyFromUtf8(modelName_);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string model_name
       *&#64;&#64;
       *&#64;&#64;     The name of the model to execute for this step of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>string model_name = 1;</code>
       * @param value The modelName to set.
       */
      private void setModelName(
          java.lang.String value) {
        java.lang.Class<?> valueClass = value.getClass();
  
        modelName_ = value;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string model_name
       *&#64;&#64;
       *&#64;&#64;     The name of the model to execute for this step of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>string model_name = 1;</code>
       */
      private void clearModelName() {
        
        modelName_ = getDefaultInstance().getModelName();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string model_name
       *&#64;&#64;
       *&#64;&#64;     The name of the model to execute for this step of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>string model_name = 1;</code>
       * @param value The bytes for modelName to set.
       */
      private void setModelNameBytes(
          com.google.protobuf.ByteString value) {
        checkByteStringIsUtf8(value);
        modelName_ = value.toStringUtf8();
        
      }

      public static final int MODEL_VERSION_FIELD_NUMBER = 2;
      private long modelVersion_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 model_version
       *&#64;&#64;
       *&#64;&#64;     The version of the model to use for inference. If -1
       *&#64;&#64;     the latest/most-recent version of the model is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>int64 model_version = 2;</code>
       * @return The modelVersion.
       */
      @java.lang.Override
      public long getModelVersion() {
        return modelVersion_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 model_version
       *&#64;&#64;
       *&#64;&#64;     The version of the model to use for inference. If -1
       *&#64;&#64;     the latest/most-recent version of the model is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>int64 model_version = 2;</code>
       * @param value The modelVersion to set.
       */
      private void setModelVersion(long value) {
        
        modelVersion_ = value;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 model_version
       *&#64;&#64;
       *&#64;&#64;     The version of the model to use for inference. If -1
       *&#64;&#64;     the latest/most-recent version of the model is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>int64 model_version = 2;</code>
       */
      private void clearModelVersion() {
        
        modelVersion_ = 0L;
      }

      public static final int INPUT_MAP_FIELD_NUMBER = 3;
      private static final class InputMapDefaultEntryHolder {
        static final com.google.protobuf.MapEntryLite<
            java.lang.String, java.lang.String> defaultEntry =
                com.google.protobuf.MapEntryLite
                .<java.lang.String, java.lang.String>newDefaultInstance(
                    com.google.protobuf.WireFormat.FieldType.STRING,
                    "",
                    com.google.protobuf.WireFormat.FieldType.STRING,
                    "");
      }
      private com.google.protobuf.MapFieldLite<
          java.lang.String, java.lang.String> inputMap_ =
              com.google.protobuf.MapFieldLite.emptyMapField();
      private com.google.protobuf.MapFieldLite<java.lang.String, java.lang.String>
      internalGetInputMap() {
        return inputMap_;
      }
      private com.google.protobuf.MapFieldLite<java.lang.String, java.lang.String>
      internalGetMutableInputMap() {
        if (!inputMap_.isMutable()) {
          inputMap_ = inputMap_.mutableCopy();
        }
        return inputMap_;
      }
      @java.lang.Override

      public int getInputMapCount() {
        return internalGetInputMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */
      @java.lang.Override

      public boolean containsInputMap(
          java.lang.String key) {
        java.lang.Class<?> keyClass = key.getClass();
        return internalGetInputMap().containsKey(key);
      }
      /**
       * Use {@link #getInputMapMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getInputMap() {
        return getInputMapMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, java.lang.String> getInputMapMap() {
        return java.util.Collections.unmodifiableMap(
            internalGetInputMap());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */
      @java.lang.Override

      public java.lang.String getInputMapOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        java.lang.Class<?> keyClass = key.getClass();
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetInputMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */
      @java.lang.Override

      public java.lang.String getInputMapOrThrow(
          java.lang.String key) {
        java.lang.Class<?> keyClass = key.getClass();
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetInputMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */
      private java.util.Map<java.lang.String, java.lang.String>
      getMutableInputMapMap() {
        return internalGetMutableInputMap();
      }

      public static final int OUTPUT_MAP_FIELD_NUMBER = 4;
      private static final class OutputMapDefaultEntryHolder {
        static final com.google.protobuf.MapEntryLite<
            java.lang.String, java.lang.String> defaultEntry =
                com.google.protobuf.MapEntryLite
                .<java.lang.String, java.lang.String>newDefaultInstance(
                    com.google.protobuf.WireFormat.FieldType.STRING,
                    "",
                    com.google.protobuf.WireFormat.FieldType.STRING,
                    "");
      }
      private com.google.protobuf.MapFieldLite<
          java.lang.String, java.lang.String> outputMap_ =
              com.google.protobuf.MapFieldLite.emptyMapField();
      private com.google.protobuf.MapFieldLite<java.lang.String, java.lang.String>
      internalGetOutputMap() {
        return outputMap_;
      }
      private com.google.protobuf.MapFieldLite<java.lang.String, java.lang.String>
      internalGetMutableOutputMap() {
        if (!outputMap_.isMutable()) {
          outputMap_ = outputMap_.mutableCopy();
        }
        return outputMap_;
      }
      @java.lang.Override

      public int getOutputMapCount() {
        return internalGetOutputMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */
      @java.lang.Override

      public boolean containsOutputMap(
          java.lang.String key) {
        java.lang.Class<?> keyClass = key.getClass();
        return internalGetOutputMap().containsKey(key);
      }
      /**
       * Use {@link #getOutputMapMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getOutputMap() {
        return getOutputMapMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, java.lang.String> getOutputMapMap() {
        return java.util.Collections.unmodifiableMap(
            internalGetOutputMap());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */
      @java.lang.Override

      public java.lang.String getOutputMapOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        java.lang.Class<?> keyClass = key.getClass();
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetOutputMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */
      @java.lang.Override

      public java.lang.String getOutputMapOrThrow(
          java.lang.String key) {
        java.lang.Class<?> keyClass = key.getClass();
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetOutputMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */
      private java.util.Map<java.lang.String, java.lang.String>
      getMutableOutputMapMap() {
        return internalGetMutableOutputMap();
      }

      public static inference.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelEnsembling.Step parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelEnsembling.Step parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }

      public static Builder newBuilder() {
        return (Builder) DEFAULT_INSTANCE.createBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelEnsembling.Step prototype) {
        return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message Step
       *&#64;&#64;
       *&#64;&#64;     Each step specifies a model included in the ensemble,
       *&#64;&#64;     maps ensemble tensor names to the model input tensors,
       *&#64;&#64;     and maps model output tensors to ensemble tensor names
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelEnsembling.Step}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageLite.Builder<
            inference.ModelConfigOuterClass.ModelEnsembling.Step, Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelEnsembling.Step)
          inference.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder {
        // Construct using inference.ModelConfigOuterClass.ModelEnsembling.Step.newBuilder()
        private Builder() {
          super(DEFAULT_INSTANCE);
        }


        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string model_name
         *&#64;&#64;
         *&#64;&#64;     The name of the model to execute for this step of the ensemble.
         *&#64;&#64;
         * </pre>
         *
         * <code>string model_name = 1;</code>
         * @return The modelName.
         */
        @java.lang.Override
        public java.lang.String getModelName() {
          return instance.getModelName();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string model_name
         *&#64;&#64;
         *&#64;&#64;     The name of the model to execute for this step of the ensemble.
         *&#64;&#64;
         * </pre>
         *
         * <code>string model_name = 1;</code>
         * @return The bytes for modelName.
         */
        @java.lang.Override
        public com.google.protobuf.ByteString
            getModelNameBytes() {
          return instance.getModelNameBytes();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string model_name
         *&#64;&#64;
         *&#64;&#64;     The name of the model to execute for this step of the ensemble.
         *&#64;&#64;
         * </pre>
         *
         * <code>string model_name = 1;</code>
         * @param value The modelName to set.
         * @return This builder for chaining.
         */
        public Builder setModelName(
            java.lang.String value) {
          copyOnWrite();
          instance.setModelName(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string model_name
         *&#64;&#64;
         *&#64;&#64;     The name of the model to execute for this step of the ensemble.
         *&#64;&#64;
         * </pre>
         *
         * <code>string model_name = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearModelName() {
          copyOnWrite();
          instance.clearModelName();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string model_name
         *&#64;&#64;
         *&#64;&#64;     The name of the model to execute for this step of the ensemble.
         *&#64;&#64;
         * </pre>
         *
         * <code>string model_name = 1;</code>
         * @param value The bytes for modelName to set.
         * @return This builder for chaining.
         */
        public Builder setModelNameBytes(
            com.google.protobuf.ByteString value) {
          copyOnWrite();
          instance.setModelNameBytes(value);
          return this;
        }

        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: int64 model_version
         *&#64;&#64;
         *&#64;&#64;     The version of the model to use for inference. If -1
         *&#64;&#64;     the latest/most-recent version of the model is used.
         *&#64;&#64;
         * </pre>
         *
         * <code>int64 model_version = 2;</code>
         * @return The modelVersion.
         */
        @java.lang.Override
        public long getModelVersion() {
          return instance.getModelVersion();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: int64 model_version
         *&#64;&#64;
         *&#64;&#64;     The version of the model to use for inference. If -1
         *&#64;&#64;     the latest/most-recent version of the model is used.
         *&#64;&#64;
         * </pre>
         *
         * <code>int64 model_version = 2;</code>
         * @param value The modelVersion to set.
         * @return This builder for chaining.
         */
        public Builder setModelVersion(long value) {
          copyOnWrite();
          instance.setModelVersion(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: int64 model_version
         *&#64;&#64;
         *&#64;&#64;     The version of the model to use for inference. If -1
         *&#64;&#64;     the latest/most-recent version of the model is used.
         *&#64;&#64;
         * </pre>
         *
         * <code>int64 model_version = 2;</code>
         * @return This builder for chaining.
         */
        public Builder clearModelVersion() {
          copyOnWrite();
          instance.clearModelVersion();
          return this;
        }

        @java.lang.Override

        public int getInputMapCount() {
          return instance.getInputMapMap().size();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */
        @java.lang.Override

        public boolean containsInputMap(
            java.lang.String key) {
          java.lang.Class<?> keyClass = key.getClass();
          return instance.getInputMapMap().containsKey(key);
        }

        public Builder clearInputMap() {
          copyOnWrite();
          instance.getMutableInputMapMap().clear();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */

        public Builder removeInputMap(
            java.lang.String key) {
          java.lang.Class<?> keyClass = key.getClass();
          copyOnWrite();
          instance.getMutableInputMapMap().remove(key);
          return this;
        }
        /**
         * Use {@link #getInputMapMap()} instead.
         */
        @java.lang.Override
        @java.lang.Deprecated
        public java.util.Map<java.lang.String, java.lang.String> getInputMap() {
          return getInputMapMap();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */
        @java.lang.Override
        public java.util.Map<java.lang.String, java.lang.String> getInputMapMap() {
          return java.util.Collections.unmodifiableMap(
              instance.getInputMapMap());
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */
        @java.lang.Override

        public java.lang.String getInputMapOrDefault(
            java.lang.String key,
            java.lang.String defaultValue) {
          java.lang.Class<?> keyClass = key.getClass();
          java.util.Map<java.lang.String, java.lang.String> map =
              instance.getInputMapMap();
          return map.containsKey(key) ? map.get(key) : defaultValue;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */
        @java.lang.Override

        public java.lang.String getInputMapOrThrow(
            java.lang.String key) {
          java.lang.Class<?> keyClass = key.getClass();
          java.util.Map<java.lang.String, java.lang.String> map =
              instance.getInputMapMap();
          if (!map.containsKey(key)) {
            throw new java.lang.IllegalArgumentException();
          }
          return map.get(key);
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */
        public Builder putInputMap(
            java.lang.String key,
            java.lang.String value) {
          java.lang.Class<?> keyClass = key.getClass();
          java.lang.Class<?> valueClass = value.getClass();
          copyOnWrite();
          instance.getMutableInputMapMap().put(key, value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */
        public Builder putAllInputMap(
            java.util.Map<java.lang.String, java.lang.String> values) {
          copyOnWrite();
          instance.getMutableInputMapMap().putAll(values);
          return this;
        }

        @java.lang.Override

        public int getOutputMapCount() {
          return instance.getOutputMapMap().size();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */
        @java.lang.Override

        public boolean containsOutputMap(
            java.lang.String key) {
          java.lang.Class<?> keyClass = key.getClass();
          return instance.getOutputMapMap().containsKey(key);
        }

        public Builder clearOutputMap() {
          copyOnWrite();
          instance.getMutableOutputMapMap().clear();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */

        public Builder removeOutputMap(
            java.lang.String key) {
          java.lang.Class<?> keyClass = key.getClass();
          copyOnWrite();
          instance.getMutableOutputMapMap().remove(key);
          return this;
        }
        /**
         * Use {@link #getOutputMapMap()} instead.
         */
        @java.lang.Override
        @java.lang.Deprecated
        public java.util.Map<java.lang.String, java.lang.String> getOutputMap() {
          return getOutputMapMap();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */
        @java.lang.Override
        public java.util.Map<java.lang.String, java.lang.String> getOutputMapMap() {
          return java.util.Collections.unmodifiableMap(
              instance.getOutputMapMap());
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */
        @java.lang.Override

        public java.lang.String getOutputMapOrDefault(
            java.lang.String key,
            java.lang.String defaultValue) {
          java.lang.Class<?> keyClass = key.getClass();
          java.util.Map<java.lang.String, java.lang.String> map =
              instance.getOutputMapMap();
          return map.containsKey(key) ? map.get(key) : defaultValue;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */
        @java.lang.Override

        public java.lang.String getOutputMapOrThrow(
            java.lang.String key) {
          java.lang.Class<?> keyClass = key.getClass();
          java.util.Map<java.lang.String, java.lang.String> map =
              instance.getOutputMapMap();
          if (!map.containsKey(key)) {
            throw new java.lang.IllegalArgumentException();
          }
          return map.get(key);
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */
        public Builder putOutputMap(
            java.lang.String key,
            java.lang.String value) {
          java.lang.Class<?> keyClass = key.getClass();
          java.lang.Class<?> valueClass = value.getClass();
          copyOnWrite();
          instance.getMutableOutputMapMap().put(key, value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */
        public Builder putAllOutputMap(
            java.util.Map<java.lang.String, java.lang.String> values) {
          copyOnWrite();
          instance.getMutableOutputMapMap().putAll(values);
          return this;
        }

        // @@protoc_insertion_point(builder_scope:inference.ModelEnsembling.Step)
      }
      @java.lang.Override
      @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
      protected final java.lang.Object dynamicMethod(
          com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
          java.lang.Object arg0, java.lang.Object arg1) {
        switch (method) {
          case NEW_MUTABLE_INSTANCE: {
            return new inference.ModelConfigOuterClass.ModelEnsembling.Step();
          }
          case NEW_BUILDER: {
            return new Builder();
          }
          case BUILD_MESSAGE_INFO: {
              java.lang.Object[] objects = new java.lang.Object[] {
                "modelName_",
                "modelVersion_",
                "inputMap_",
                InputMapDefaultEntryHolder.defaultEntry,
                "outputMap_",
                OutputMapDefaultEntryHolder.defaultEntry,
              };
              java.lang.String info =
                  "\u0000\u0004\u0000\u0000\u0001\u0004\u0004\u0002\u0000\u0000\u0001\u0208\u0002\u0002" +
                  "\u00032\u00042";
              return newMessageInfo(DEFAULT_INSTANCE, info, objects);
          }
          // fall through
          case GET_DEFAULT_INSTANCE: {
            return DEFAULT_INSTANCE;
          }
          case GET_PARSER: {
            com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelEnsembling.Step> parser = PARSER;
            if (parser == null) {
              synchronized (inference.ModelConfigOuterClass.ModelEnsembling.Step.class) {
                parser = PARSER;
                if (parser == null) {
                  parser =
                      new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelEnsembling.Step>(
                          DEFAULT_INSTANCE);
                  PARSER = parser;
                }
              }
            }
            return parser;
        }
        case GET_MEMOIZED_IS_INITIALIZED: {
          return (byte) 1;
        }
        case SET_MEMOIZED_IS_INITIALIZED: {
          return null;
        }
        }
        throw new UnsupportedOperationException();
      }


      // @@protoc_insertion_point(class_scope:inference.ModelEnsembling.Step)
      private static final inference.ModelConfigOuterClass.ModelEnsembling.Step DEFAULT_INSTANCE;
      static {
        Step defaultInstance = new Step();
        // New instances are implicitly immutable so no need to make
        // immutable.
        DEFAULT_INSTANCE = defaultInstance;
        com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
          Step.class, defaultInstance);
      }

      public static inference.ModelConfigOuterClass.ModelEnsembling.Step getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static volatile com.google.protobuf.Parser<Step> PARSER;

      public static com.google.protobuf.Parser<Step> parser() {
        return DEFAULT_INSTANCE.getParserForType();
      }
    }

    public static final int STEP_FIELD_NUMBER = 1;
    private com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.ModelEnsembling.Step> step_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
     */
    @java.lang.Override
    public java.util.List<inference.ModelConfigOuterClass.ModelEnsembling.Step> getStepList() {
      return step_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
     */
    public java.util.List<? extends inference.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder> 
        getStepOrBuilderList() {
      return step_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
     */
    @java.lang.Override
    public int getStepCount() {
      return step_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelEnsembling.Step getStep(int index) {
      return step_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
     */
    public inference.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder getStepOrBuilder(
        int index) {
      return step_.get(index);
    }
    private void ensureStepIsMutable() {
      com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.ModelEnsembling.Step> tmp = step_;
      if (!tmp.isModifiable()) {
        step_ =
            com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
       }
    }

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
     */
    private void setStep(
        int index, inference.ModelConfigOuterClass.ModelEnsembling.Step value) {
      value.getClass();
  ensureStepIsMutable();
      step_.set(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
     */
    private void addStep(inference.ModelConfigOuterClass.ModelEnsembling.Step value) {
      value.getClass();
  ensureStepIsMutable();
      step_.add(value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
     */
    private void addStep(
        int index, inference.ModelConfigOuterClass.ModelEnsembling.Step value) {
      value.getClass();
  ensureStepIsMutable();
      step_.add(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
     */
    private void addAllStep(
        java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelEnsembling.Step> values) {
      ensureStepIsMutable();
      com.google.protobuf.AbstractMessageLite.addAll(
          values, step_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
     */
    private void clearStep() {
      step_ = emptyProtobufList();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
     */
    private void removeStep(int index) {
      ensureStepIsMutable();
      step_.remove(index);
    }

    public static inference.ModelConfigOuterClass.ModelEnsembling parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelEnsembling parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelEnsembling parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelEnsembling parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelEnsembling parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelEnsembling parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelEnsembling parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelEnsembling parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelEnsembling parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelEnsembling parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelEnsembling parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelEnsembling parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }

    public static Builder newBuilder() {
      return (Builder) DEFAULT_INSTANCE.createBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelEnsembling prototype) {
      return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelEnsembling
     *&#64;&#64;
     *&#64;&#64;   Model ensembling configuration. These settings specify the models that
     *&#64;&#64;   compose the ensemble and how data flows between the models.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelEnsembling}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageLite.Builder<
          inference.ModelConfigOuterClass.ModelEnsembling, Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelEnsembling)
        inference.ModelConfigOuterClass.ModelEnsemblingOrBuilder {
      // Construct using inference.ModelConfigOuterClass.ModelEnsembling.newBuilder()
      private Builder() {
        super(DEFAULT_INSTANCE);
      }


      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      @java.lang.Override
      public java.util.List<inference.ModelConfigOuterClass.ModelEnsembling.Step> getStepList() {
        return java.util.Collections.unmodifiableList(
            instance.getStepList());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      @java.lang.Override
      public int getStepCount() {
        return instance.getStepCount();
      }/**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelEnsembling.Step getStep(int index) {
        return instance.getStep(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public Builder setStep(
          int index, inference.ModelConfigOuterClass.ModelEnsembling.Step value) {
        copyOnWrite();
        instance.setStep(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public Builder setStep(
          int index, inference.ModelConfigOuterClass.ModelEnsembling.Step.Builder builderForValue) {
        copyOnWrite();
        instance.setStep(index,
            builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public Builder addStep(inference.ModelConfigOuterClass.ModelEnsembling.Step value) {
        copyOnWrite();
        instance.addStep(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public Builder addStep(
          int index, inference.ModelConfigOuterClass.ModelEnsembling.Step value) {
        copyOnWrite();
        instance.addStep(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public Builder addStep(
          inference.ModelConfigOuterClass.ModelEnsembling.Step.Builder builderForValue) {
        copyOnWrite();
        instance.addStep(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public Builder addStep(
          int index, inference.ModelConfigOuterClass.ModelEnsembling.Step.Builder builderForValue) {
        copyOnWrite();
        instance.addStep(index,
            builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public Builder addAllStep(
          java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelEnsembling.Step> values) {
        copyOnWrite();
        instance.addAllStep(values);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public Builder clearStep() {
        copyOnWrite();
        instance.clearStep();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public Builder removeStep(int index) {
        copyOnWrite();
        instance.removeStep(index);
        return this;
      }

      // @@protoc_insertion_point(builder_scope:inference.ModelEnsembling)
    }
    @java.lang.Override
    @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
    protected final java.lang.Object dynamicMethod(
        com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
        java.lang.Object arg0, java.lang.Object arg1) {
      switch (method) {
        case NEW_MUTABLE_INSTANCE: {
          return new inference.ModelConfigOuterClass.ModelEnsembling();
        }
        case NEW_BUILDER: {
          return new Builder();
        }
        case BUILD_MESSAGE_INFO: {
            java.lang.Object[] objects = new java.lang.Object[] {
              "step_",
              inference.ModelConfigOuterClass.ModelEnsembling.Step.class,
            };
            java.lang.String info =
                "\u0000\u0001\u0000\u0000\u0001\u0001\u0001\u0000\u0001\u0000\u0001\u001b";
            return newMessageInfo(DEFAULT_INSTANCE, info, objects);
        }
        // fall through
        case GET_DEFAULT_INSTANCE: {
          return DEFAULT_INSTANCE;
        }
        case GET_PARSER: {
          com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelEnsembling> parser = PARSER;
          if (parser == null) {
            synchronized (inference.ModelConfigOuterClass.ModelEnsembling.class) {
              parser = PARSER;
              if (parser == null) {
                parser =
                    new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelEnsembling>(
                        DEFAULT_INSTANCE);
                PARSER = parser;
              }
            }
          }
          return parser;
      }
      case GET_MEMOIZED_IS_INITIALIZED: {
        return (byte) 1;
      }
      case SET_MEMOIZED_IS_INITIALIZED: {
        return null;
      }
      }
      throw new UnsupportedOperationException();
    }


    // @@protoc_insertion_point(class_scope:inference.ModelEnsembling)
    private static final inference.ModelConfigOuterClass.ModelEnsembling DEFAULT_INSTANCE;
    static {
      ModelEnsembling defaultInstance = new ModelEnsembling();
      // New instances are implicitly immutable so no need to make
      // immutable.
      DEFAULT_INSTANCE = defaultInstance;
      com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
        ModelEnsembling.class, defaultInstance);
    }

    public static inference.ModelConfigOuterClass.ModelEnsembling getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static volatile com.google.protobuf.Parser<ModelEnsembling> PARSER;

    public static com.google.protobuf.Parser<ModelEnsembling> parser() {
      return DEFAULT_INSTANCE.getParserForType();
    }
  }

  public interface ModelParameterOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelParameter)
      com.google.protobuf.MessageLiteOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string string_value
     *&#64;&#64;
     *&#64;&#64;     The string value of the parameter.
     *&#64;&#64;
     * </pre>
     *
     * <code>string string_value = 1;</code>
     * @return The stringValue.
     */
    java.lang.String getStringValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string string_value
     *&#64;&#64;
     *&#64;&#64;     The string value of the parameter.
     *&#64;&#64;
     * </pre>
     *
     * <code>string string_value = 1;</code>
     * @return The bytes for stringValue.
     */
    com.google.protobuf.ByteString
        getStringValueBytes();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelParameter
   *&#64;&#64;
   *&#64;&#64;   A model parameter.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelParameter}
   */
  public  static final class ModelParameter extends
      com.google.protobuf.GeneratedMessageLite<
          ModelParameter, ModelParameter.Builder> implements
      // @@protoc_insertion_point(message_implements:inference.ModelParameter)
      ModelParameterOrBuilder {
    private ModelParameter() {
      stringValue_ = "";
    }
    public static final int STRING_VALUE_FIELD_NUMBER = 1;
    private java.lang.String stringValue_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string string_value
     *&#64;&#64;
     *&#64;&#64;     The string value of the parameter.
     *&#64;&#64;
     * </pre>
     *
     * <code>string string_value = 1;</code>
     * @return The stringValue.
     */
    @java.lang.Override
    public java.lang.String getStringValue() {
      return stringValue_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string string_value
     *&#64;&#64;
     *&#64;&#64;     The string value of the parameter.
     *&#64;&#64;
     * </pre>
     *
     * <code>string string_value = 1;</code>
     * @return The bytes for stringValue.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getStringValueBytes() {
      return com.google.protobuf.ByteString.copyFromUtf8(stringValue_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string string_value
     *&#64;&#64;
     *&#64;&#64;     The string value of the parameter.
     *&#64;&#64;
     * </pre>
     *
     * <code>string string_value = 1;</code>
     * @param value The stringValue to set.
     */
    private void setStringValue(
        java.lang.String value) {
      java.lang.Class<?> valueClass = value.getClass();
  
      stringValue_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string string_value
     *&#64;&#64;
     *&#64;&#64;     The string value of the parameter.
     *&#64;&#64;
     * </pre>
     *
     * <code>string string_value = 1;</code>
     */
    private void clearStringValue() {
      
      stringValue_ = getDefaultInstance().getStringValue();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string string_value
     *&#64;&#64;
     *&#64;&#64;     The string value of the parameter.
     *&#64;&#64;
     * </pre>
     *
     * <code>string string_value = 1;</code>
     * @param value The bytes for stringValue to set.
     */
    private void setStringValueBytes(
        com.google.protobuf.ByteString value) {
      checkByteStringIsUtf8(value);
      stringValue_ = value.toStringUtf8();
      
    }

    public static inference.ModelConfigOuterClass.ModelParameter parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelParameter parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelParameter parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelParameter parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelParameter parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelParameter parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelParameter parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelParameter parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelParameter parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelParameter parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelParameter parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelParameter parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }

    public static Builder newBuilder() {
      return (Builder) DEFAULT_INSTANCE.createBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelParameter prototype) {
      return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelParameter
     *&#64;&#64;
     *&#64;&#64;   A model parameter.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelParameter}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageLite.Builder<
          inference.ModelConfigOuterClass.ModelParameter, Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelParameter)
        inference.ModelConfigOuterClass.ModelParameterOrBuilder {
      // Construct using inference.ModelConfigOuterClass.ModelParameter.newBuilder()
      private Builder() {
        super(DEFAULT_INSTANCE);
      }


      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string string_value
       *&#64;&#64;
       *&#64;&#64;     The string value of the parameter.
       *&#64;&#64;
       * </pre>
       *
       * <code>string string_value = 1;</code>
       * @return The stringValue.
       */
      @java.lang.Override
      public java.lang.String getStringValue() {
        return instance.getStringValue();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string string_value
       *&#64;&#64;
       *&#64;&#64;     The string value of the parameter.
       *&#64;&#64;
       * </pre>
       *
       * <code>string string_value = 1;</code>
       * @return The bytes for stringValue.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString
          getStringValueBytes() {
        return instance.getStringValueBytes();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string string_value
       *&#64;&#64;
       *&#64;&#64;     The string value of the parameter.
       *&#64;&#64;
       * </pre>
       *
       * <code>string string_value = 1;</code>
       * @param value The stringValue to set.
       * @return This builder for chaining.
       */
      public Builder setStringValue(
          java.lang.String value) {
        copyOnWrite();
        instance.setStringValue(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string string_value
       *&#64;&#64;
       *&#64;&#64;     The string value of the parameter.
       *&#64;&#64;
       * </pre>
       *
       * <code>string string_value = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearStringValue() {
        copyOnWrite();
        instance.clearStringValue();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string string_value
       *&#64;&#64;
       *&#64;&#64;     The string value of the parameter.
       *&#64;&#64;
       * </pre>
       *
       * <code>string string_value = 1;</code>
       * @param value The bytes for stringValue to set.
       * @return This builder for chaining.
       */
      public Builder setStringValueBytes(
          com.google.protobuf.ByteString value) {
        copyOnWrite();
        instance.setStringValueBytes(value);
        return this;
      }

      // @@protoc_insertion_point(builder_scope:inference.ModelParameter)
    }
    @java.lang.Override
    @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
    protected final java.lang.Object dynamicMethod(
        com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
        java.lang.Object arg0, java.lang.Object arg1) {
      switch (method) {
        case NEW_MUTABLE_INSTANCE: {
          return new inference.ModelConfigOuterClass.ModelParameter();
        }
        case NEW_BUILDER: {
          return new Builder();
        }
        case BUILD_MESSAGE_INFO: {
            java.lang.Object[] objects = new java.lang.Object[] {
              "stringValue_",
            };
            java.lang.String info =
                "\u0000\u0001\u0000\u0000\u0001\u0001\u0001\u0000\u0000\u0000\u0001\u0208";
            return newMessageInfo(DEFAULT_INSTANCE, info, objects);
        }
        // fall through
        case GET_DEFAULT_INSTANCE: {
          return DEFAULT_INSTANCE;
        }
        case GET_PARSER: {
          com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelParameter> parser = PARSER;
          if (parser == null) {
            synchronized (inference.ModelConfigOuterClass.ModelParameter.class) {
              parser = PARSER;
              if (parser == null) {
                parser =
                    new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelParameter>(
                        DEFAULT_INSTANCE);
                PARSER = parser;
              }
            }
          }
          return parser;
      }
      case GET_MEMOIZED_IS_INITIALIZED: {
        return (byte) 1;
      }
      case SET_MEMOIZED_IS_INITIALIZED: {
        return null;
      }
      }
      throw new UnsupportedOperationException();
    }


    // @@protoc_insertion_point(class_scope:inference.ModelParameter)
    private static final inference.ModelConfigOuterClass.ModelParameter DEFAULT_INSTANCE;
    static {
      ModelParameter defaultInstance = new ModelParameter();
      // New instances are implicitly immutable so no need to make
      // immutable.
      DEFAULT_INSTANCE = defaultInstance;
      com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
        ModelParameter.class, defaultInstance);
    }

    public static inference.ModelConfigOuterClass.ModelParameter getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static volatile com.google.protobuf.Parser<ModelParameter> PARSER;

    public static com.google.protobuf.Parser<ModelParameter> parser() {
      return DEFAULT_INSTANCE.getParserForType();
    }
  }

  public interface ModelWarmupOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelWarmup)
      com.google.protobuf.MessageLiteOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the request sample.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the request sample.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 batch_size
     *&#64;&#64;
     *&#64;&#64;     The batch size of the inference request. This must be &gt;= 1. For
     *&#64;&#64;     models that don't support batching, batch_size must be 1. If
     *&#64;&#64;     batch_size &gt; 1, the 'inputs' specified below will be duplicated to
     *&#64;&#64;     match the batch size requested.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 batch_size = 2;</code>
     * @return The batchSize.
     */
    int getBatchSize();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
     */
    int getInputsCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
     */
    boolean containsInputs(
        java.lang.String key);
    /**
     * Use {@link #getInputsMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input>
    getInputs();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
     */
    java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input>
    getInputsMap();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
     */

    inference.ModelConfigOuterClass.ModelWarmup.Input getInputsOrDefault(
        java.lang.String key,
        inference.ModelConfigOuterClass.ModelWarmup.Input defaultValue);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
     */

    inference.ModelConfigOuterClass.ModelWarmup.Input getInputsOrThrow(
        java.lang.String key);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelWarmup
   *&#64;&#64;
   *&#64;&#64;   Settings used to construct the request sample for model warmup.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelWarmup}
   */
  public  static final class ModelWarmup extends
      com.google.protobuf.GeneratedMessageLite<
          ModelWarmup, ModelWarmup.Builder> implements
      // @@protoc_insertion_point(message_implements:inference.ModelWarmup)
      ModelWarmupOrBuilder {
    private ModelWarmup() {
      name_ = "";
    }
    public interface InputOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelWarmup.Input)
        com.google.protobuf.MessageLiteOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 1;</code>
       * @return The enum numeric value on the wire for dataType.
       */
      int getDataTypeValue();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 1;</code>
       * @return The dataType.
       */
      inference.ModelConfigOuterClass.DataType getDataType();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       * @return A list containing the dims.
       */
      java.util.List<java.lang.Long> getDimsList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       * @return The count of dims.
       */
      int getDimsCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       * @param index The index of the element to return.
       * @return The dims at the given index.
       */
      long getDims(int index);

      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;    .. cpp:var:: bool zero_data
       *&#64;&#64;
       *&#64;&#64;       The identifier for using zeros as input data. Note that the
       *&#64;&#64;       value of 'zero_data' will not be checked, instead, zero data
       *&#64;&#64;       will be used as long as the field is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool zero_data = 3;</code>
       * @return Whether the zeroData field is set.
       */
      boolean hasZeroData();
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;    .. cpp:var:: bool zero_data
       *&#64;&#64;
       *&#64;&#64;       The identifier for using zeros as input data. Note that the
       *&#64;&#64;       value of 'zero_data' will not be checked, instead, zero data
       *&#64;&#64;       will be used as long as the field is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool zero_data = 3;</code>
       * @return The zeroData.
       */
      boolean getZeroData();

      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;    .. cpp:var:: bool random_data
       *&#64;&#64;
       *&#64;&#64;       The identifier for using random data as input data. Note that
       *&#64;&#64;       the value of 'random_data' will not be checked, instead,
       *&#64;&#64;       random data will be used as long as the field is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool random_data = 4;</code>
       * @return Whether the randomData field is set.
       */
      boolean hasRandomData();
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;    .. cpp:var:: bool random_data
       *&#64;&#64;
       *&#64;&#64;       The identifier for using random data as input data. Note that
       *&#64;&#64;       the value of 'random_data' will not be checked, instead,
       *&#64;&#64;       random data will be used as long as the field is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool random_data = 4;</code>
       * @return The randomData.
       */
      boolean getRandomData();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string input_data_file
       *&#64;&#64;
       *&#64;&#64;       The file whose content will be used as raw input data in
       *&#64;&#64;       row-major order. The file must be provided in a sub-directory
       *&#64;&#64;       'warmup' under the model directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>string input_data_file = 5;</code>
       * @return Whether the inputDataFile field is set.
       */
      boolean hasInputDataFile();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string input_data_file
       *&#64;&#64;
       *&#64;&#64;       The file whose content will be used as raw input data in
       *&#64;&#64;       row-major order. The file must be provided in a sub-directory
       *&#64;&#64;       'warmup' under the model directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>string input_data_file = 5;</code>
       * @return The inputDataFile.
       */
      java.lang.String getInputDataFile();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string input_data_file
       *&#64;&#64;
       *&#64;&#64;       The file whose content will be used as raw input data in
       *&#64;&#64;       row-major order. The file must be provided in a sub-directory
       *&#64;&#64;       'warmup' under the model directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>string input_data_file = 5;</code>
       * @return The bytes for inputDataFile.
       */
      com.google.protobuf.ByteString
          getInputDataFileBytes();

      public inference.ModelConfigOuterClass.ModelWarmup.Input.InputDataTypeCase getInputDataTypeCase();
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: message Input
     *&#64;&#64;
     *&#64;&#64;     Meta data associated with an input.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelWarmup.Input}
     */
    public  static final class Input extends
        com.google.protobuf.GeneratedMessageLite<
            Input, Input.Builder> implements
        // @@protoc_insertion_point(message_implements:inference.ModelWarmup.Input)
        InputOrBuilder {
      private Input() {
        dims_ = emptyLongList();
      }
      private int inputDataTypeCase_ = 0;
      private java.lang.Object inputDataType_;
      public enum InputDataTypeCase {
        ZERO_DATA(3),
        RANDOM_DATA(4),
        INPUT_DATA_FILE(5),
        INPUTDATATYPE_NOT_SET(0);
        private final int value;
        private InputDataTypeCase(int value) {
          this.value = value;
        }
        /**
         * @deprecated Use {@link #forNumber(int)} instead.
         */
        @java.lang.Deprecated
        public static InputDataTypeCase valueOf(int value) {
          return forNumber(value);
        }

        public static InputDataTypeCase forNumber(int value) {
          switch (value) {
            case 3: return ZERO_DATA;
            case 4: return RANDOM_DATA;
            case 5: return INPUT_DATA_FILE;
            case 0: return INPUTDATATYPE_NOT_SET;
            default: return null;
          }
        }
        public int getNumber() {
          return this.value;
        }
      };

      @java.lang.Override
      public InputDataTypeCase
      getInputDataTypeCase() {
        return InputDataTypeCase.forNumber(
            inputDataTypeCase_);
      }

      private void clearInputDataType() {
        inputDataTypeCase_ = 0;
        inputDataType_ = null;
      }

      public static final int DATA_TYPE_FIELD_NUMBER = 1;
      private int dataType_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 1;</code>
       * @return The enum numeric value on the wire for dataType.
       */
      @java.lang.Override
      public int getDataTypeValue() {
        return dataType_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 1;</code>
       * @return The dataType.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.DataType getDataType() {
        inference.ModelConfigOuterClass.DataType result = inference.ModelConfigOuterClass.DataType.forNumber(dataType_);
        return result == null ? inference.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 1;</code>
       * @param value The enum numeric value on the wire for dataType to set.
       */
      private void setDataTypeValue(int value) {
          dataType_ = value;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 1;</code>
       * @param value The dataType to set.
       */
      private void setDataType(inference.ModelConfigOuterClass.DataType value) {
        dataType_ = value.getNumber();
        
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 1;</code>
       */
      private void clearDataType() {
        
        dataType_ = 0;
      }

      public static final int DIMS_FIELD_NUMBER = 2;
      private com.google.protobuf.Internal.LongList dims_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       * @return A list containing the dims.
       */
      @java.lang.Override
      public java.util.List<java.lang.Long>
          getDimsList() {
        return dims_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       * @return The count of dims.
       */
      @java.lang.Override
      public int getDimsCount() {
        return dims_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       * @param index The index of the element to return.
       * @return The dims at the given index.
       */
      @java.lang.Override
      public long getDims(int index) {
        return dims_.getLong(index);
      }
      private int dimsMemoizedSerializedSize = -1;
      private void ensureDimsIsMutable() {
        com.google.protobuf.Internal.LongList tmp = dims_;
        if (!tmp.isModifiable()) {
          dims_ =
              com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
         }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       * @param index The index to set the value at.
       * @param value The dims to set.
       */
      private void setDims(
          int index, long value) {
        ensureDimsIsMutable();
        dims_.setLong(index, value);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       * @param value The dims to add.
       */
      private void addDims(long value) {
        ensureDimsIsMutable();
        dims_.addLong(value);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       * @param values The dims to add.
       */
      private void addAllDims(
          java.lang.Iterable<? extends java.lang.Long> values) {
        ensureDimsIsMutable();
        com.google.protobuf.AbstractMessageLite.addAll(
            values, dims_);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       */
      private void clearDims() {
        dims_ = emptyLongList();
      }

      public static final int ZERO_DATA_FIELD_NUMBER = 3;
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;    .. cpp:var:: bool zero_data
       *&#64;&#64;
       *&#64;&#64;       The identifier for using zeros as input data. Note that the
       *&#64;&#64;       value of 'zero_data' will not be checked, instead, zero data
       *&#64;&#64;       will be used as long as the field is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool zero_data = 3;</code>
       * @return Whether the zeroData field is set.
       */
      @java.lang.Override
      public boolean hasZeroData() {
        return inputDataTypeCase_ == 3;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;    .. cpp:var:: bool zero_data
       *&#64;&#64;
       *&#64;&#64;       The identifier for using zeros as input data. Note that the
       *&#64;&#64;       value of 'zero_data' will not be checked, instead, zero data
       *&#64;&#64;       will be used as long as the field is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool zero_data = 3;</code>
       * @return The zeroData.
       */
      @java.lang.Override
      public boolean getZeroData() {
        if (inputDataTypeCase_ == 3) {
          return (java.lang.Boolean) inputDataType_;
        }
        return false;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;    .. cpp:var:: bool zero_data
       *&#64;&#64;
       *&#64;&#64;       The identifier for using zeros as input data. Note that the
       *&#64;&#64;       value of 'zero_data' will not be checked, instead, zero data
       *&#64;&#64;       will be used as long as the field is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool zero_data = 3;</code>
       * @param value The zeroData to set.
       */
      private void setZeroData(boolean value) {
        inputDataTypeCase_ = 3;
        inputDataType_ = value;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;    .. cpp:var:: bool zero_data
       *&#64;&#64;
       *&#64;&#64;       The identifier for using zeros as input data. Note that the
       *&#64;&#64;       value of 'zero_data' will not be checked, instead, zero data
       *&#64;&#64;       will be used as long as the field is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool zero_data = 3;</code>
       */
      private void clearZeroData() {
        if (inputDataTypeCase_ == 3) {
          inputDataTypeCase_ = 0;
          inputDataType_ = null;
        }
      }

      public static final int RANDOM_DATA_FIELD_NUMBER = 4;
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;    .. cpp:var:: bool random_data
       *&#64;&#64;
       *&#64;&#64;       The identifier for using random data as input data. Note that
       *&#64;&#64;       the value of 'random_data' will not be checked, instead,
       *&#64;&#64;       random data will be used as long as the field is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool random_data = 4;</code>
       * @return Whether the randomData field is set.
       */
      @java.lang.Override
      public boolean hasRandomData() {
        return inputDataTypeCase_ == 4;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;    .. cpp:var:: bool random_data
       *&#64;&#64;
       *&#64;&#64;       The identifier for using random data as input data. Note that
       *&#64;&#64;       the value of 'random_data' will not be checked, instead,
       *&#64;&#64;       random data will be used as long as the field is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool random_data = 4;</code>
       * @return The randomData.
       */
      @java.lang.Override
      public boolean getRandomData() {
        if (inputDataTypeCase_ == 4) {
          return (java.lang.Boolean) inputDataType_;
        }
        return false;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;    .. cpp:var:: bool random_data
       *&#64;&#64;
       *&#64;&#64;       The identifier for using random data as input data. Note that
       *&#64;&#64;       the value of 'random_data' will not be checked, instead,
       *&#64;&#64;       random data will be used as long as the field is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool random_data = 4;</code>
       * @param value The randomData to set.
       */
      private void setRandomData(boolean value) {
        inputDataTypeCase_ = 4;
        inputDataType_ = value;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;    .. cpp:var:: bool random_data
       *&#64;&#64;
       *&#64;&#64;       The identifier for using random data as input data. Note that
       *&#64;&#64;       the value of 'random_data' will not be checked, instead,
       *&#64;&#64;       random data will be used as long as the field is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool random_data = 4;</code>
       */
      private void clearRandomData() {
        if (inputDataTypeCase_ == 4) {
          inputDataTypeCase_ = 0;
          inputDataType_ = null;
        }
      }

      public static final int INPUT_DATA_FILE_FIELD_NUMBER = 5;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string input_data_file
       *&#64;&#64;
       *&#64;&#64;       The file whose content will be used as raw input data in
       *&#64;&#64;       row-major order. The file must be provided in a sub-directory
       *&#64;&#64;       'warmup' under the model directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>string input_data_file = 5;</code>
       * @return Whether the inputDataFile field is set.
       */
      @java.lang.Override
      public boolean hasInputDataFile() {
        return inputDataTypeCase_ == 5;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string input_data_file
       *&#64;&#64;
       *&#64;&#64;       The file whose content will be used as raw input data in
       *&#64;&#64;       row-major order. The file must be provided in a sub-directory
       *&#64;&#64;       'warmup' under the model directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>string input_data_file = 5;</code>
       * @return The inputDataFile.
       */
      @java.lang.Override
      public java.lang.String getInputDataFile() {
        java.lang.String ref = "";
        if (inputDataTypeCase_ == 5) {
          ref = (java.lang.String) inputDataType_;
        }
        return ref;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string input_data_file
       *&#64;&#64;
       *&#64;&#64;       The file whose content will be used as raw input data in
       *&#64;&#64;       row-major order. The file must be provided in a sub-directory
       *&#64;&#64;       'warmup' under the model directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>string input_data_file = 5;</code>
       * @return The bytes for inputDataFile.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString
          getInputDataFileBytes() {
        java.lang.String ref = "";
        if (inputDataTypeCase_ == 5) {
          ref = (java.lang.String) inputDataType_;
        }
        return com.google.protobuf.ByteString.copyFromUtf8(ref);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string input_data_file
       *&#64;&#64;
       *&#64;&#64;       The file whose content will be used as raw input data in
       *&#64;&#64;       row-major order. The file must be provided in a sub-directory
       *&#64;&#64;       'warmup' under the model directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>string input_data_file = 5;</code>
       * @param value The inputDataFile to set.
       */
      private void setInputDataFile(
          java.lang.String value) {
        java.lang.Class<?> valueClass = value.getClass();
  inputDataTypeCase_ = 5;
        inputDataType_ = value;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string input_data_file
       *&#64;&#64;
       *&#64;&#64;       The file whose content will be used as raw input data in
       *&#64;&#64;       row-major order. The file must be provided in a sub-directory
       *&#64;&#64;       'warmup' under the model directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>string input_data_file = 5;</code>
       */
      private void clearInputDataFile() {
        if (inputDataTypeCase_ == 5) {
          inputDataTypeCase_ = 0;
          inputDataType_ = null;
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string input_data_file
       *&#64;&#64;
       *&#64;&#64;       The file whose content will be used as raw input data in
       *&#64;&#64;       row-major order. The file must be provided in a sub-directory
       *&#64;&#64;       'warmup' under the model directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>string input_data_file = 5;</code>
       * @param value The bytes for inputDataFile to set.
       */
      private void setInputDataFileBytes(
          com.google.protobuf.ByteString value) {
        checkByteStringIsUtf8(value);
        inputDataType_ = value.toStringUtf8();
        inputDataTypeCase_ = 5;
      }

      public static inference.ModelConfigOuterClass.ModelWarmup.Input parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelWarmup.Input parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelWarmup.Input parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelWarmup.Input parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelWarmup.Input parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelWarmup.Input parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelWarmup.Input parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelWarmup.Input parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelWarmup.Input parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelWarmup.Input parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelWarmup.Input parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelWarmup.Input parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }

      public static Builder newBuilder() {
        return (Builder) DEFAULT_INSTANCE.createBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelWarmup.Input prototype) {
        return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
      }

      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: message Input
       *&#64;&#64;
       *&#64;&#64;     Meta data associated with an input.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelWarmup.Input}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageLite.Builder<
            inference.ModelConfigOuterClass.ModelWarmup.Input, Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelWarmup.Input)
          inference.ModelConfigOuterClass.ModelWarmup.InputOrBuilder {
        // Construct using inference.ModelConfigOuterClass.ModelWarmup.Input.newBuilder()
        private Builder() {
          super(DEFAULT_INSTANCE);
        }

        @java.lang.Override
        public InputDataTypeCase
            getInputDataTypeCase() {
          return instance.getInputDataTypeCase();
        }

        public Builder clearInputDataType() {
          copyOnWrite();
          instance.clearInputDataType();
          return this;
        }


        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The data-type of the input.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.DataType data_type = 1;</code>
         * @return The enum numeric value on the wire for dataType.
         */
        @java.lang.Override
        public int getDataTypeValue() {
          return instance.getDataTypeValue();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The data-type of the input.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.DataType data_type = 1;</code>
         * @param value The dataType to set.
         * @return This builder for chaining.
         */
        public Builder setDataTypeValue(int value) {
          copyOnWrite();
          instance.setDataTypeValue(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The data-type of the input.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.DataType data_type = 1;</code>
         * @return The dataType.
         */
        @java.lang.Override
        public inference.ModelConfigOuterClass.DataType getDataType() {
          return instance.getDataType();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The data-type of the input.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.DataType data_type = 1;</code>
         * @param value The enum numeric value on the wire for dataType to set.
         * @return This builder for chaining.
         */
        public Builder setDataType(inference.ModelConfigOuterClass.DataType value) {
          copyOnWrite();
          instance.setDataType(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The data-type of the input.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.DataType data_type = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearDataType() {
          copyOnWrite();
          instance.clearDataType();
          return this;
        }

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         * @return A list containing the dims.
         */
        @java.lang.Override
        public java.util.List<java.lang.Long>
            getDimsList() {
          return java.util.Collections.unmodifiableList(
              instance.getDimsList());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         * @return The count of dims.
         */
        @java.lang.Override
        public int getDimsCount() {
          return instance.getDimsCount();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         * @param index The index of the element to return.
         * @return The dims at the given index.
         */
        @java.lang.Override
        public long getDims(int index) {
          return instance.getDims(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         * @param value The dims to set.
         * @return This builder for chaining.
         */
        public Builder setDims(
            int index, long value) {
          copyOnWrite();
          instance.setDims(index, value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         * @param value The dims to add.
         * @return This builder for chaining.
         */
        public Builder addDims(long value) {
          copyOnWrite();
          instance.addDims(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         * @param values The dims to add.
         * @return This builder for chaining.
         */
        public Builder addAllDims(
            java.lang.Iterable<? extends java.lang.Long> values) {
          copyOnWrite();
          instance.addAllDims(values);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         * @return This builder for chaining.
         */
        public Builder clearDims() {
          copyOnWrite();
          instance.clearDims();
          return this;
        }

        /**
         * <pre>
         *&#64;&#64;
         *&#64;&#64;    .. cpp:var:: bool zero_data
         *&#64;&#64;
         *&#64;&#64;       The identifier for using zeros as input data. Note that the
         *&#64;&#64;       value of 'zero_data' will not be checked, instead, zero data
         *&#64;&#64;       will be used as long as the field is set.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool zero_data = 3;</code>
         * @return Whether the zeroData field is set.
         */
        @java.lang.Override
        public boolean hasZeroData() {
          return instance.hasZeroData();
        }
        /**
         * <pre>
         *&#64;&#64;
         *&#64;&#64;    .. cpp:var:: bool zero_data
         *&#64;&#64;
         *&#64;&#64;       The identifier for using zeros as input data. Note that the
         *&#64;&#64;       value of 'zero_data' will not be checked, instead, zero data
         *&#64;&#64;       will be used as long as the field is set.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool zero_data = 3;</code>
         * @return The zeroData.
         */
        @java.lang.Override
        public boolean getZeroData() {
          return instance.getZeroData();
        }
        /**
         * <pre>
         *&#64;&#64;
         *&#64;&#64;    .. cpp:var:: bool zero_data
         *&#64;&#64;
         *&#64;&#64;       The identifier for using zeros as input data. Note that the
         *&#64;&#64;       value of 'zero_data' will not be checked, instead, zero data
         *&#64;&#64;       will be used as long as the field is set.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool zero_data = 3;</code>
         * @param value The zeroData to set.
         * @return This builder for chaining.
         */
        public Builder setZeroData(boolean value) {
          copyOnWrite();
          instance.setZeroData(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;
         *&#64;&#64;    .. cpp:var:: bool zero_data
         *&#64;&#64;
         *&#64;&#64;       The identifier for using zeros as input data. Note that the
         *&#64;&#64;       value of 'zero_data' will not be checked, instead, zero data
         *&#64;&#64;       will be used as long as the field is set.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool zero_data = 3;</code>
         * @return This builder for chaining.
         */
        public Builder clearZeroData() {
          copyOnWrite();
          instance.clearZeroData();
          return this;
        }

        /**
         * <pre>
         *&#64;&#64;
         *&#64;&#64;    .. cpp:var:: bool random_data
         *&#64;&#64;
         *&#64;&#64;       The identifier for using random data as input data. Note that
         *&#64;&#64;       the value of 'random_data' will not be checked, instead,
         *&#64;&#64;       random data will be used as long as the field is set.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool random_data = 4;</code>
         * @return Whether the randomData field is set.
         */
        @java.lang.Override
        public boolean hasRandomData() {
          return instance.hasRandomData();
        }
        /**
         * <pre>
         *&#64;&#64;
         *&#64;&#64;    .. cpp:var:: bool random_data
         *&#64;&#64;
         *&#64;&#64;       The identifier for using random data as input data. Note that
         *&#64;&#64;       the value of 'random_data' will not be checked, instead,
         *&#64;&#64;       random data will be used as long as the field is set.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool random_data = 4;</code>
         * @return The randomData.
         */
        @java.lang.Override
        public boolean getRandomData() {
          return instance.getRandomData();
        }
        /**
         * <pre>
         *&#64;&#64;
         *&#64;&#64;    .. cpp:var:: bool random_data
         *&#64;&#64;
         *&#64;&#64;       The identifier for using random data as input data. Note that
         *&#64;&#64;       the value of 'random_data' will not be checked, instead,
         *&#64;&#64;       random data will be used as long as the field is set.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool random_data = 4;</code>
         * @param value The randomData to set.
         * @return This builder for chaining.
         */
        public Builder setRandomData(boolean value) {
          copyOnWrite();
          instance.setRandomData(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;
         *&#64;&#64;    .. cpp:var:: bool random_data
         *&#64;&#64;
         *&#64;&#64;       The identifier for using random data as input data. Note that
         *&#64;&#64;       the value of 'random_data' will not be checked, instead,
         *&#64;&#64;       random data will be used as long as the field is set.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool random_data = 4;</code>
         * @return This builder for chaining.
         */
        public Builder clearRandomData() {
          copyOnWrite();
          instance.clearRandomData();
          return this;
        }

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string input_data_file
         *&#64;&#64;
         *&#64;&#64;       The file whose content will be used as raw input data in
         *&#64;&#64;       row-major order. The file must be provided in a sub-directory
         *&#64;&#64;       'warmup' under the model directory.
         *&#64;&#64;
         * </pre>
         *
         * <code>string input_data_file = 5;</code>
         * @return Whether the inputDataFile field is set.
         */
        @java.lang.Override
        public boolean hasInputDataFile() {
          return instance.hasInputDataFile();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string input_data_file
         *&#64;&#64;
         *&#64;&#64;       The file whose content will be used as raw input data in
         *&#64;&#64;       row-major order. The file must be provided in a sub-directory
         *&#64;&#64;       'warmup' under the model directory.
         *&#64;&#64;
         * </pre>
         *
         * <code>string input_data_file = 5;</code>
         * @return The inputDataFile.
         */
        @java.lang.Override
        public java.lang.String getInputDataFile() {
          return instance.getInputDataFile();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string input_data_file
         *&#64;&#64;
         *&#64;&#64;       The file whose content will be used as raw input data in
         *&#64;&#64;       row-major order. The file must be provided in a sub-directory
         *&#64;&#64;       'warmup' under the model directory.
         *&#64;&#64;
         * </pre>
         *
         * <code>string input_data_file = 5;</code>
         * @return The bytes for inputDataFile.
         */
        @java.lang.Override
        public com.google.protobuf.ByteString
            getInputDataFileBytes() {
          return instance.getInputDataFileBytes();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string input_data_file
         *&#64;&#64;
         *&#64;&#64;       The file whose content will be used as raw input data in
         *&#64;&#64;       row-major order. The file must be provided in a sub-directory
         *&#64;&#64;       'warmup' under the model directory.
         *&#64;&#64;
         * </pre>
         *
         * <code>string input_data_file = 5;</code>
         * @param value The inputDataFile to set.
         * @return This builder for chaining.
         */
        public Builder setInputDataFile(
            java.lang.String value) {
          copyOnWrite();
          instance.setInputDataFile(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string input_data_file
         *&#64;&#64;
         *&#64;&#64;       The file whose content will be used as raw input data in
         *&#64;&#64;       row-major order. The file must be provided in a sub-directory
         *&#64;&#64;       'warmup' under the model directory.
         *&#64;&#64;
         * </pre>
         *
         * <code>string input_data_file = 5;</code>
         * @return This builder for chaining.
         */
        public Builder clearInputDataFile() {
          copyOnWrite();
          instance.clearInputDataFile();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string input_data_file
         *&#64;&#64;
         *&#64;&#64;       The file whose content will be used as raw input data in
         *&#64;&#64;       row-major order. The file must be provided in a sub-directory
         *&#64;&#64;       'warmup' under the model directory.
         *&#64;&#64;
         * </pre>
         *
         * <code>string input_data_file = 5;</code>
         * @param value The bytes for inputDataFile to set.
         * @return This builder for chaining.
         */
        public Builder setInputDataFileBytes(
            com.google.protobuf.ByteString value) {
          copyOnWrite();
          instance.setInputDataFileBytes(value);
          return this;
        }

        // @@protoc_insertion_point(builder_scope:inference.ModelWarmup.Input)
      }
      @java.lang.Override
      @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
      protected final java.lang.Object dynamicMethod(
          com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
          java.lang.Object arg0, java.lang.Object arg1) {
        switch (method) {
          case NEW_MUTABLE_INSTANCE: {
            return new inference.ModelConfigOuterClass.ModelWarmup.Input();
          }
          case NEW_BUILDER: {
            return new Builder();
          }
          case BUILD_MESSAGE_INFO: {
              java.lang.Object[] objects = new java.lang.Object[] {
                "inputDataType_",
                "inputDataTypeCase_",
                "dataType_",
                "dims_",
              };
              java.lang.String info =
                  "\u0000\u0005\u0001\u0000\u0001\u0005\u0005\u0000\u0001\u0000\u0001\f\u0002%\u0003" +
                  ":\u0000\u0004:\u0000\u0005\u023b\u0000";
              return newMessageInfo(DEFAULT_INSTANCE, info, objects);
          }
          // fall through
          case GET_DEFAULT_INSTANCE: {
            return DEFAULT_INSTANCE;
          }
          case GET_PARSER: {
            com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelWarmup.Input> parser = PARSER;
            if (parser == null) {
              synchronized (inference.ModelConfigOuterClass.ModelWarmup.Input.class) {
                parser = PARSER;
                if (parser == null) {
                  parser =
                      new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelWarmup.Input>(
                          DEFAULT_INSTANCE);
                  PARSER = parser;
                }
              }
            }
            return parser;
        }
        case GET_MEMOIZED_IS_INITIALIZED: {
          return (byte) 1;
        }
        case SET_MEMOIZED_IS_INITIALIZED: {
          return null;
        }
        }
        throw new UnsupportedOperationException();
      }


      // @@protoc_insertion_point(class_scope:inference.ModelWarmup.Input)
      private static final inference.ModelConfigOuterClass.ModelWarmup.Input DEFAULT_INSTANCE;
      static {
        Input defaultInstance = new Input();
        // New instances are implicitly immutable so no need to make
        // immutable.
        DEFAULT_INSTANCE = defaultInstance;
        com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
          Input.class, defaultInstance);
      }

      public static inference.ModelConfigOuterClass.ModelWarmup.Input getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static volatile com.google.protobuf.Parser<Input> PARSER;

      public static com.google.protobuf.Parser<Input> parser() {
        return DEFAULT_INSTANCE.getParserForType();
      }
    }

    public static final int NAME_FIELD_NUMBER = 1;
    private java.lang.String name_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the request sample.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      return name_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the request sample.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      return com.google.protobuf.ByteString.copyFromUtf8(name_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the request sample.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @param value The name to set.
     */
    private void setName(
        java.lang.String value) {
      java.lang.Class<?> valueClass = value.getClass();
  
      name_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the request sample.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    private void clearName() {
      
      name_ = getDefaultInstance().getName();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the request sample.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @param value The bytes for name to set.
     */
    private void setNameBytes(
        com.google.protobuf.ByteString value) {
      checkByteStringIsUtf8(value);
      name_ = value.toStringUtf8();
      
    }

    public static final int BATCH_SIZE_FIELD_NUMBER = 2;
    private int batchSize_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 batch_size
     *&#64;&#64;
     *&#64;&#64;     The batch size of the inference request. This must be &gt;= 1. For
     *&#64;&#64;     models that don't support batching, batch_size must be 1. If
     *&#64;&#64;     batch_size &gt; 1, the 'inputs' specified below will be duplicated to
     *&#64;&#64;     match the batch size requested.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 batch_size = 2;</code>
     * @return The batchSize.
     */
    @java.lang.Override
    public int getBatchSize() {
      return batchSize_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 batch_size
     *&#64;&#64;
     *&#64;&#64;     The batch size of the inference request. This must be &gt;= 1. For
     *&#64;&#64;     models that don't support batching, batch_size must be 1. If
     *&#64;&#64;     batch_size &gt; 1, the 'inputs' specified below will be duplicated to
     *&#64;&#64;     match the batch size requested.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 batch_size = 2;</code>
     * @param value The batchSize to set.
     */
    private void setBatchSize(int value) {
      
      batchSize_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 batch_size
     *&#64;&#64;
     *&#64;&#64;     The batch size of the inference request. This must be &gt;= 1. For
     *&#64;&#64;     models that don't support batching, batch_size must be 1. If
     *&#64;&#64;     batch_size &gt; 1, the 'inputs' specified below will be duplicated to
     *&#64;&#64;     match the batch size requested.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 batch_size = 2;</code>
     */
    private void clearBatchSize() {
      
      batchSize_ = 0;
    }

    public static final int INPUTS_FIELD_NUMBER = 3;
    private static final class InputsDefaultEntryHolder {
      static final com.google.protobuf.MapEntryLite<
          java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input> defaultEntry =
              com.google.protobuf.MapEntryLite
              .<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input>newDefaultInstance(
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.MESSAGE,
                  inference.ModelConfigOuterClass.ModelWarmup.Input.getDefaultInstance());
    }
    private com.google.protobuf.MapFieldLite<
        java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input> inputs_ =
            com.google.protobuf.MapFieldLite.emptyMapField();
    private com.google.protobuf.MapFieldLite<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input>
    internalGetInputs() {
      return inputs_;
    }
    private com.google.protobuf.MapFieldLite<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input>
    internalGetMutableInputs() {
      if (!inputs_.isMutable()) {
        inputs_ = inputs_.mutableCopy();
      }
      return inputs_;
    }
    @java.lang.Override

    public int getInputsCount() {
      return internalGetInputs().size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
     */
    @java.lang.Override

    public boolean containsInputs(
        java.lang.String key) {
      java.lang.Class<?> keyClass = key.getClass();
      return internalGetInputs().containsKey(key);
    }
    /**
     * Use {@link #getInputsMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input> getInputs() {
      return getInputsMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input> getInputsMap() {
      return java.util.Collections.unmodifiableMap(
          internalGetInputs());
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
     */
    @java.lang.Override

    public inference.ModelConfigOuterClass.ModelWarmup.Input getInputsOrDefault(
        java.lang.String key,
        inference.ModelConfigOuterClass.ModelWarmup.Input defaultValue) {
      java.lang.Class<?> keyClass = key.getClass();
      java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input> map =
          internalGetInputs();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
     */
    @java.lang.Override

    public inference.ModelConfigOuterClass.ModelWarmup.Input getInputsOrThrow(
        java.lang.String key) {
      java.lang.Class<?> keyClass = key.getClass();
      java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input> map =
          internalGetInputs();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
     */
    private java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input>
    getMutableInputsMap() {
      return internalGetMutableInputs();
    }

    public static inference.ModelConfigOuterClass.ModelWarmup parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelWarmup parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelWarmup parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelWarmup parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelWarmup parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelWarmup parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelWarmup parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelWarmup parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelWarmup parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelWarmup parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelWarmup parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelWarmup parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }

    public static Builder newBuilder() {
      return (Builder) DEFAULT_INSTANCE.createBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelWarmup prototype) {
      return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelWarmup
     *&#64;&#64;
     *&#64;&#64;   Settings used to construct the request sample for model warmup.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelWarmup}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageLite.Builder<
          inference.ModelConfigOuterClass.ModelWarmup, Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelWarmup)
        inference.ModelConfigOuterClass.ModelWarmupOrBuilder {
      // Construct using inference.ModelConfigOuterClass.ModelWarmup.newBuilder()
      private Builder() {
        super(DEFAULT_INSTANCE);
      }


      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the request sample.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The name.
       */
      @java.lang.Override
      public java.lang.String getName() {
        return instance.getName();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the request sample.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString
          getNameBytes() {
        return instance.getNameBytes();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the request sample.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        copyOnWrite();
        instance.setName(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the request sample.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        copyOnWrite();
        instance.clearName();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the request sample.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        copyOnWrite();
        instance.setNameBytes(value);
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 batch_size
       *&#64;&#64;
       *&#64;&#64;     The batch size of the inference request. This must be &gt;= 1. For
       *&#64;&#64;     models that don't support batching, batch_size must be 1. If
       *&#64;&#64;     batch_size &gt; 1, the 'inputs' specified below will be duplicated to
       *&#64;&#64;     match the batch size requested.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 batch_size = 2;</code>
       * @return The batchSize.
       */
      @java.lang.Override
      public int getBatchSize() {
        return instance.getBatchSize();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 batch_size
       *&#64;&#64;
       *&#64;&#64;     The batch size of the inference request. This must be &gt;= 1. For
       *&#64;&#64;     models that don't support batching, batch_size must be 1. If
       *&#64;&#64;     batch_size &gt; 1, the 'inputs' specified below will be duplicated to
       *&#64;&#64;     match the batch size requested.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 batch_size = 2;</code>
       * @param value The batchSize to set.
       * @return This builder for chaining.
       */
      public Builder setBatchSize(int value) {
        copyOnWrite();
        instance.setBatchSize(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 batch_size
       *&#64;&#64;
       *&#64;&#64;     The batch size of the inference request. This must be &gt;= 1. For
       *&#64;&#64;     models that don't support batching, batch_size must be 1. If
       *&#64;&#64;     batch_size &gt; 1, the 'inputs' specified below will be duplicated to
       *&#64;&#64;     match the batch size requested.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 batch_size = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearBatchSize() {
        copyOnWrite();
        instance.clearBatchSize();
        return this;
      }

      @java.lang.Override

      public int getInputsCount() {
        return instance.getInputsMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
       *&#64;&#64;
       *&#64;&#64;     The warmup meta data associated with every model input, including
       *&#64;&#64;     control tensors.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
       */
      @java.lang.Override

      public boolean containsInputs(
          java.lang.String key) {
        java.lang.Class<?> keyClass = key.getClass();
        return instance.getInputsMap().containsKey(key);
      }

      public Builder clearInputs() {
        copyOnWrite();
        instance.getMutableInputsMap().clear();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
       *&#64;&#64;
       *&#64;&#64;     The warmup meta data associated with every model input, including
       *&#64;&#64;     control tensors.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
       */

      public Builder removeInputs(
          java.lang.String key) {
        java.lang.Class<?> keyClass = key.getClass();
        copyOnWrite();
        instance.getMutableInputsMap().remove(key);
        return this;
      }
      /**
       * Use {@link #getInputsMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input> getInputs() {
        return getInputsMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
       *&#64;&#64;
       *&#64;&#64;     The warmup meta data associated with every model input, including
       *&#64;&#64;     control tensors.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
       */
      @java.lang.Override
      public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input> getInputsMap() {
        return java.util.Collections.unmodifiableMap(
            instance.getInputsMap());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
       *&#64;&#64;
       *&#64;&#64;     The warmup meta data associated with every model input, including
       *&#64;&#64;     control tensors.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
       */
      @java.lang.Override

      public inference.ModelConfigOuterClass.ModelWarmup.Input getInputsOrDefault(
          java.lang.String key,
          inference.ModelConfigOuterClass.ModelWarmup.Input defaultValue) {
        java.lang.Class<?> keyClass = key.getClass();
        java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input> map =
            instance.getInputsMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
       *&#64;&#64;
       *&#64;&#64;     The warmup meta data associated with every model input, including
       *&#64;&#64;     control tensors.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
       */
      @java.lang.Override

      public inference.ModelConfigOuterClass.ModelWarmup.Input getInputsOrThrow(
          java.lang.String key) {
        java.lang.Class<?> keyClass = key.getClass();
        java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input> map =
            instance.getInputsMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
       *&#64;&#64;
       *&#64;&#64;     The warmup meta data associated with every model input, including
       *&#64;&#64;     control tensors.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
       */
      public Builder putInputs(
          java.lang.String key,
          inference.ModelConfigOuterClass.ModelWarmup.Input value) {
        java.lang.Class<?> keyClass = key.getClass();
        java.lang.Class<?> valueClass = value.getClass();
        copyOnWrite();
        instance.getMutableInputsMap().put(key, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
       *&#64;&#64;
       *&#64;&#64;     The warmup meta data associated with every model input, including
       *&#64;&#64;     control tensors.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
       */
      public Builder putAllInputs(
          java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input> values) {
        copyOnWrite();
        instance.getMutableInputsMap().putAll(values);
        return this;
      }

      // @@protoc_insertion_point(builder_scope:inference.ModelWarmup)
    }
    @java.lang.Override
    @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
    protected final java.lang.Object dynamicMethod(
        com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
        java.lang.Object arg0, java.lang.Object arg1) {
      switch (method) {
        case NEW_MUTABLE_INSTANCE: {
          return new inference.ModelConfigOuterClass.ModelWarmup();
        }
        case NEW_BUILDER: {
          return new Builder();
        }
        case BUILD_MESSAGE_INFO: {
            java.lang.Object[] objects = new java.lang.Object[] {
              "name_",
              "batchSize_",
              "inputs_",
              InputsDefaultEntryHolder.defaultEntry,
            };
            java.lang.String info =
                "\u0000\u0003\u0000\u0000\u0001\u0003\u0003\u0001\u0000\u0000\u0001\u0208\u0002\u000b" +
                "\u00032";
            return newMessageInfo(DEFAULT_INSTANCE, info, objects);
        }
        // fall through
        case GET_DEFAULT_INSTANCE: {
          return DEFAULT_INSTANCE;
        }
        case GET_PARSER: {
          com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelWarmup> parser = PARSER;
          if (parser == null) {
            synchronized (inference.ModelConfigOuterClass.ModelWarmup.class) {
              parser = PARSER;
              if (parser == null) {
                parser =
                    new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelWarmup>(
                        DEFAULT_INSTANCE);
                PARSER = parser;
              }
            }
          }
          return parser;
      }
      case GET_MEMOIZED_IS_INITIALIZED: {
        return (byte) 1;
      }
      case SET_MEMOIZED_IS_INITIALIZED: {
        return null;
      }
      }
      throw new UnsupportedOperationException();
    }


    // @@protoc_insertion_point(class_scope:inference.ModelWarmup)
    private static final inference.ModelConfigOuterClass.ModelWarmup DEFAULT_INSTANCE;
    static {
      ModelWarmup defaultInstance = new ModelWarmup();
      // New instances are implicitly immutable so no need to make
      // immutable.
      DEFAULT_INSTANCE = defaultInstance;
      com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
        ModelWarmup.class, defaultInstance);
    }

    public static inference.ModelConfigOuterClass.ModelWarmup getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static volatile com.google.protobuf.Parser<ModelWarmup> PARSER;

    public static com.google.protobuf.Parser<ModelWarmup> parser() {
      return DEFAULT_INSTANCE.getParserForType();
    }
  }

  public interface ModelOperationsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelOperations)
      com.google.protobuf.MessageLiteOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string op_library_filename (repeated)
     *&#64;&#64;
     *&#64;&#64;     Optional paths of the libraries providing custom operations for
     *&#64;&#64;     this model. Valid only for ONNX models.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string op_library_filename = 1;</code>
     * @return A list containing the opLibraryFilename.
     */
    java.util.List<java.lang.String>
        getOpLibraryFilenameList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string op_library_filename (repeated)
     *&#64;&#64;
     *&#64;&#64;     Optional paths of the libraries providing custom operations for
     *&#64;&#64;     this model. Valid only for ONNX models.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string op_library_filename = 1;</code>
     * @return The count of opLibraryFilename.
     */
    int getOpLibraryFilenameCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string op_library_filename (repeated)
     *&#64;&#64;
     *&#64;&#64;     Optional paths of the libraries providing custom operations for
     *&#64;&#64;     this model. Valid only for ONNX models.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string op_library_filename = 1;</code>
     * @param index The index of the element to return.
     * @return The opLibraryFilename at the given index.
     */
    java.lang.String getOpLibraryFilename(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string op_library_filename (repeated)
     *&#64;&#64;
     *&#64;&#64;     Optional paths of the libraries providing custom operations for
     *&#64;&#64;     this model. Valid only for ONNX models.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string op_library_filename = 1;</code>
     * @param index The index of the element to return.
     * @return The opLibraryFilename at the given index.
     */
    com.google.protobuf.ByteString
        getOpLibraryFilenameBytes(int index);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64; .. cpp:var:: message ModelOperations
   *&#64;&#64;
   *&#64;&#64;    The metadata of libraries providing custom operations for this model.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelOperations}
   */
  public  static final class ModelOperations extends
      com.google.protobuf.GeneratedMessageLite<
          ModelOperations, ModelOperations.Builder> implements
      // @@protoc_insertion_point(message_implements:inference.ModelOperations)
      ModelOperationsOrBuilder {
    private ModelOperations() {
      opLibraryFilename_ = com.google.protobuf.GeneratedMessageLite.emptyProtobufList();
    }
    public static final int OP_LIBRARY_FILENAME_FIELD_NUMBER = 1;
    private com.google.protobuf.Internal.ProtobufList<java.lang.String> opLibraryFilename_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string op_library_filename (repeated)
     *&#64;&#64;
     *&#64;&#64;     Optional paths of the libraries providing custom operations for
     *&#64;&#64;     this model. Valid only for ONNX models.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string op_library_filename = 1;</code>
     * @return A list containing the opLibraryFilename.
     */
    @java.lang.Override
    public java.util.List<java.lang.String> getOpLibraryFilenameList() {
      return opLibraryFilename_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string op_library_filename (repeated)
     *&#64;&#64;
     *&#64;&#64;     Optional paths of the libraries providing custom operations for
     *&#64;&#64;     this model. Valid only for ONNX models.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string op_library_filename = 1;</code>
     * @return The count of opLibraryFilename.
     */
    @java.lang.Override
    public int getOpLibraryFilenameCount() {
      return opLibraryFilename_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string op_library_filename (repeated)
     *&#64;&#64;
     *&#64;&#64;     Optional paths of the libraries providing custom operations for
     *&#64;&#64;     this model. Valid only for ONNX models.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string op_library_filename = 1;</code>
     * @param index The index of the element to return.
     * @return The opLibraryFilename at the given index.
     */
    @java.lang.Override
    public java.lang.String getOpLibraryFilename(int index) {
      return opLibraryFilename_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string op_library_filename (repeated)
     *&#64;&#64;
     *&#64;&#64;     Optional paths of the libraries providing custom operations for
     *&#64;&#64;     this model. Valid only for ONNX models.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string op_library_filename = 1;</code>
     * @param index The index of the value to return.
     * @return The bytes of the opLibraryFilename at the given index.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getOpLibraryFilenameBytes(int index) {
      return com.google.protobuf.ByteString.copyFromUtf8(
          opLibraryFilename_.get(index));
    }
    private void ensureOpLibraryFilenameIsMutable() {
      com.google.protobuf.Internal.ProtobufList<java.lang.String> tmp =
          opLibraryFilename_;  if (!tmp.isModifiable()) {
        opLibraryFilename_ =
            com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
       }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string op_library_filename (repeated)
     *&#64;&#64;
     *&#64;&#64;     Optional paths of the libraries providing custom operations for
     *&#64;&#64;     this model. Valid only for ONNX models.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string op_library_filename = 1;</code>
     * @param index The index to set the value at.
     * @param value The opLibraryFilename to set.
     */
    private void setOpLibraryFilename(
        int index, java.lang.String value) {
      java.lang.Class<?> valueClass = value.getClass();
  ensureOpLibraryFilenameIsMutable();
      opLibraryFilename_.set(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string op_library_filename (repeated)
     *&#64;&#64;
     *&#64;&#64;     Optional paths of the libraries providing custom operations for
     *&#64;&#64;     this model. Valid only for ONNX models.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string op_library_filename = 1;</code>
     * @param value The opLibraryFilename to add.
     */
    private void addOpLibraryFilename(
        java.lang.String value) {
      java.lang.Class<?> valueClass = value.getClass();
  ensureOpLibraryFilenameIsMutable();
      opLibraryFilename_.add(value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string op_library_filename (repeated)
     *&#64;&#64;
     *&#64;&#64;     Optional paths of the libraries providing custom operations for
     *&#64;&#64;     this model. Valid only for ONNX models.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string op_library_filename = 1;</code>
     * @param values The opLibraryFilename to add.
     */
    private void addAllOpLibraryFilename(
        java.lang.Iterable<java.lang.String> values) {
      ensureOpLibraryFilenameIsMutable();
      com.google.protobuf.AbstractMessageLite.addAll(
          values, opLibraryFilename_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string op_library_filename (repeated)
     *&#64;&#64;
     *&#64;&#64;     Optional paths of the libraries providing custom operations for
     *&#64;&#64;     this model. Valid only for ONNX models.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string op_library_filename = 1;</code>
     */
    private void clearOpLibraryFilename() {
      opLibraryFilename_ = com.google.protobuf.GeneratedMessageLite.emptyProtobufList();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string op_library_filename (repeated)
     *&#64;&#64;
     *&#64;&#64;     Optional paths of the libraries providing custom operations for
     *&#64;&#64;     this model. Valid only for ONNX models.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string op_library_filename = 1;</code>
     * @param value The bytes of the opLibraryFilename to add.
     */
    private void addOpLibraryFilenameBytes(
        com.google.protobuf.ByteString value) {
      checkByteStringIsUtf8(value);
      ensureOpLibraryFilenameIsMutable();
      opLibraryFilename_.add(value.toStringUtf8());
    }

    public static inference.ModelConfigOuterClass.ModelOperations parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelOperations parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOperations parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelOperations parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOperations parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelOperations parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOperations parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelOperations parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOperations parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelOperations parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOperations parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelOperations parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }

    public static Builder newBuilder() {
      return (Builder) DEFAULT_INSTANCE.createBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelOperations prototype) {
      return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64; .. cpp:var:: message ModelOperations
     *&#64;&#64;
     *&#64;&#64;    The metadata of libraries providing custom operations for this model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelOperations}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageLite.Builder<
          inference.ModelConfigOuterClass.ModelOperations, Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelOperations)
        inference.ModelConfigOuterClass.ModelOperationsOrBuilder {
      // Construct using inference.ModelConfigOuterClass.ModelOperations.newBuilder()
      private Builder() {
        super(DEFAULT_INSTANCE);
      }


      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string op_library_filename (repeated)
       *&#64;&#64;
       *&#64;&#64;     Optional paths of the libraries providing custom operations for
       *&#64;&#64;     this model. Valid only for ONNX models.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string op_library_filename = 1;</code>
       * @return A list containing the opLibraryFilename.
       */
      @java.lang.Override
      public java.util.List<java.lang.String>
          getOpLibraryFilenameList() {
        return java.util.Collections.unmodifiableList(
            instance.getOpLibraryFilenameList());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string op_library_filename (repeated)
       *&#64;&#64;
       *&#64;&#64;     Optional paths of the libraries providing custom operations for
       *&#64;&#64;     this model. Valid only for ONNX models.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string op_library_filename = 1;</code>
       * @return The count of opLibraryFilename.
       */
      @java.lang.Override
      public int getOpLibraryFilenameCount() {
        return instance.getOpLibraryFilenameCount();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string op_library_filename (repeated)
       *&#64;&#64;
       *&#64;&#64;     Optional paths of the libraries providing custom operations for
       *&#64;&#64;     this model. Valid only for ONNX models.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string op_library_filename = 1;</code>
       * @param index The index of the element to return.
       * @return The opLibraryFilename at the given index.
       */
      @java.lang.Override
      public java.lang.String getOpLibraryFilename(int index) {
        return instance.getOpLibraryFilename(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string op_library_filename (repeated)
       *&#64;&#64;
       *&#64;&#64;     Optional paths of the libraries providing custom operations for
       *&#64;&#64;     this model. Valid only for ONNX models.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string op_library_filename = 1;</code>
       * @param index The index of the value to return.
       * @return The bytes of the opLibraryFilename at the given index.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString
          getOpLibraryFilenameBytes(int index) {
        return instance.getOpLibraryFilenameBytes(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string op_library_filename (repeated)
       *&#64;&#64;
       *&#64;&#64;     Optional paths of the libraries providing custom operations for
       *&#64;&#64;     this model. Valid only for ONNX models.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string op_library_filename = 1;</code>
       * @param index The index to set the value at.
       * @param value The opLibraryFilename to set.
       * @return This builder for chaining.
       */
      public Builder setOpLibraryFilename(
          int index, java.lang.String value) {
        copyOnWrite();
        instance.setOpLibraryFilename(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string op_library_filename (repeated)
       *&#64;&#64;
       *&#64;&#64;     Optional paths of the libraries providing custom operations for
       *&#64;&#64;     this model. Valid only for ONNX models.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string op_library_filename = 1;</code>
       * @param value The opLibraryFilename to add.
       * @return This builder for chaining.
       */
      public Builder addOpLibraryFilename(
          java.lang.String value) {
        copyOnWrite();
        instance.addOpLibraryFilename(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string op_library_filename (repeated)
       *&#64;&#64;
       *&#64;&#64;     Optional paths of the libraries providing custom operations for
       *&#64;&#64;     this model. Valid only for ONNX models.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string op_library_filename = 1;</code>
       * @param values The opLibraryFilename to add.
       * @return This builder for chaining.
       */
      public Builder addAllOpLibraryFilename(
          java.lang.Iterable<java.lang.String> values) {
        copyOnWrite();
        instance.addAllOpLibraryFilename(values);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string op_library_filename (repeated)
       *&#64;&#64;
       *&#64;&#64;     Optional paths of the libraries providing custom operations for
       *&#64;&#64;     this model. Valid only for ONNX models.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string op_library_filename = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearOpLibraryFilename() {
        copyOnWrite();
        instance.clearOpLibraryFilename();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string op_library_filename (repeated)
       *&#64;&#64;
       *&#64;&#64;     Optional paths of the libraries providing custom operations for
       *&#64;&#64;     this model. Valid only for ONNX models.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string op_library_filename = 1;</code>
       * @param value The bytes of the opLibraryFilename to add.
       * @return This builder for chaining.
       */
      public Builder addOpLibraryFilenameBytes(
          com.google.protobuf.ByteString value) {
        copyOnWrite();
        instance.addOpLibraryFilenameBytes(value);
        return this;
      }

      // @@protoc_insertion_point(builder_scope:inference.ModelOperations)
    }
    @java.lang.Override
    @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
    protected final java.lang.Object dynamicMethod(
        com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
        java.lang.Object arg0, java.lang.Object arg1) {
      switch (method) {
        case NEW_MUTABLE_INSTANCE: {
          return new inference.ModelConfigOuterClass.ModelOperations();
        }
        case NEW_BUILDER: {
          return new Builder();
        }
        case BUILD_MESSAGE_INFO: {
            java.lang.Object[] objects = new java.lang.Object[] {
              "opLibraryFilename_",
            };
            java.lang.String info =
                "\u0000\u0001\u0000\u0000\u0001\u0001\u0001\u0000\u0001\u0000\u0001\u021a";
            return newMessageInfo(DEFAULT_INSTANCE, info, objects);
        }
        // fall through
        case GET_DEFAULT_INSTANCE: {
          return DEFAULT_INSTANCE;
        }
        case GET_PARSER: {
          com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelOperations> parser = PARSER;
          if (parser == null) {
            synchronized (inference.ModelConfigOuterClass.ModelOperations.class) {
              parser = PARSER;
              if (parser == null) {
                parser =
                    new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelOperations>(
                        DEFAULT_INSTANCE);
                PARSER = parser;
              }
            }
          }
          return parser;
      }
      case GET_MEMOIZED_IS_INITIALIZED: {
        return (byte) 1;
      }
      case SET_MEMOIZED_IS_INITIALIZED: {
        return null;
      }
      }
      throw new UnsupportedOperationException();
    }


    // @@protoc_insertion_point(class_scope:inference.ModelOperations)
    private static final inference.ModelConfigOuterClass.ModelOperations DEFAULT_INSTANCE;
    static {
      ModelOperations defaultInstance = new ModelOperations();
      // New instances are implicitly immutable so no need to make
      // immutable.
      DEFAULT_INSTANCE = defaultInstance;
      com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
        ModelOperations.class, defaultInstance);
    }

    public static inference.ModelConfigOuterClass.ModelOperations getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static volatile com.google.protobuf.Parser<ModelOperations> PARSER;

    public static com.google.protobuf.Parser<ModelOperations> parser() {
      return DEFAULT_INSTANCE.getParserForType();
    }
  }

  public interface ModelTransactionPolicyOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelTransactionPolicy)
      com.google.protobuf.MessageLiteOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool decoupled
     *&#64;&#64;
     *&#64;&#64;     Indicates whether responses generated by the model are decoupled with
     *&#64;&#64;     the requests issued to it, which means the number of responses
     *&#64;&#64;     generated by model may differ from number of requests issued, and
     *&#64;&#64;     that the responses may be out of order relative to the order of
     *&#64;&#64;     requests. The default is false, which means the model will generate
     *&#64;&#64;     exactly one response for each request.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool decoupled = 1;</code>
     * @return The decoupled.
     */
    boolean getDecoupled();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64; .. cpp:var:: message ModelTransactionPolicy
   *&#64;&#64;
   *&#64;&#64;    The specification that describes the nature of transactions
   *&#64;&#64;    to be expected from the model.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelTransactionPolicy}
   */
  public  static final class ModelTransactionPolicy extends
      com.google.protobuf.GeneratedMessageLite<
          ModelTransactionPolicy, ModelTransactionPolicy.Builder> implements
      // @@protoc_insertion_point(message_implements:inference.ModelTransactionPolicy)
      ModelTransactionPolicyOrBuilder {
    private ModelTransactionPolicy() {
    }
    public static final int DECOUPLED_FIELD_NUMBER = 1;
    private boolean decoupled_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool decoupled
     *&#64;&#64;
     *&#64;&#64;     Indicates whether responses generated by the model are decoupled with
     *&#64;&#64;     the requests issued to it, which means the number of responses
     *&#64;&#64;     generated by model may differ from number of requests issued, and
     *&#64;&#64;     that the responses may be out of order relative to the order of
     *&#64;&#64;     requests. The default is false, which means the model will generate
     *&#64;&#64;     exactly one response for each request.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool decoupled = 1;</code>
     * @return The decoupled.
     */
    @java.lang.Override
    public boolean getDecoupled() {
      return decoupled_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool decoupled
     *&#64;&#64;
     *&#64;&#64;     Indicates whether responses generated by the model are decoupled with
     *&#64;&#64;     the requests issued to it, which means the number of responses
     *&#64;&#64;     generated by model may differ from number of requests issued, and
     *&#64;&#64;     that the responses may be out of order relative to the order of
     *&#64;&#64;     requests. The default is false, which means the model will generate
     *&#64;&#64;     exactly one response for each request.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool decoupled = 1;</code>
     * @param value The decoupled to set.
     */
    private void setDecoupled(boolean value) {
      
      decoupled_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool decoupled
     *&#64;&#64;
     *&#64;&#64;     Indicates whether responses generated by the model are decoupled with
     *&#64;&#64;     the requests issued to it, which means the number of responses
     *&#64;&#64;     generated by model may differ from number of requests issued, and
     *&#64;&#64;     that the responses may be out of order relative to the order of
     *&#64;&#64;     requests. The default is false, which means the model will generate
     *&#64;&#64;     exactly one response for each request.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool decoupled = 1;</code>
     */
    private void clearDecoupled() {
      
      decoupled_ = false;
    }

    public static inference.ModelConfigOuterClass.ModelTransactionPolicy parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelTransactionPolicy parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelTransactionPolicy parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelTransactionPolicy parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelTransactionPolicy parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelTransactionPolicy parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelTransactionPolicy parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelTransactionPolicy parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelTransactionPolicy parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelTransactionPolicy parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelTransactionPolicy parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelTransactionPolicy parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }

    public static Builder newBuilder() {
      return (Builder) DEFAULT_INSTANCE.createBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelTransactionPolicy prototype) {
      return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64; .. cpp:var:: message ModelTransactionPolicy
     *&#64;&#64;
     *&#64;&#64;    The specification that describes the nature of transactions
     *&#64;&#64;    to be expected from the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelTransactionPolicy}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageLite.Builder<
          inference.ModelConfigOuterClass.ModelTransactionPolicy, Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelTransactionPolicy)
        inference.ModelConfigOuterClass.ModelTransactionPolicyOrBuilder {
      // Construct using inference.ModelConfigOuterClass.ModelTransactionPolicy.newBuilder()
      private Builder() {
        super(DEFAULT_INSTANCE);
      }


      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool decoupled
       *&#64;&#64;
       *&#64;&#64;     Indicates whether responses generated by the model are decoupled with
       *&#64;&#64;     the requests issued to it, which means the number of responses
       *&#64;&#64;     generated by model may differ from number of requests issued, and
       *&#64;&#64;     that the responses may be out of order relative to the order of
       *&#64;&#64;     requests. The default is false, which means the model will generate
       *&#64;&#64;     exactly one response for each request.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool decoupled = 1;</code>
       * @return The decoupled.
       */
      @java.lang.Override
      public boolean getDecoupled() {
        return instance.getDecoupled();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool decoupled
       *&#64;&#64;
       *&#64;&#64;     Indicates whether responses generated by the model are decoupled with
       *&#64;&#64;     the requests issued to it, which means the number of responses
       *&#64;&#64;     generated by model may differ from number of requests issued, and
       *&#64;&#64;     that the responses may be out of order relative to the order of
       *&#64;&#64;     requests. The default is false, which means the model will generate
       *&#64;&#64;     exactly one response for each request.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool decoupled = 1;</code>
       * @param value The decoupled to set.
       * @return This builder for chaining.
       */
      public Builder setDecoupled(boolean value) {
        copyOnWrite();
        instance.setDecoupled(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool decoupled
       *&#64;&#64;
       *&#64;&#64;     Indicates whether responses generated by the model are decoupled with
       *&#64;&#64;     the requests issued to it, which means the number of responses
       *&#64;&#64;     generated by model may differ from number of requests issued, and
       *&#64;&#64;     that the responses may be out of order relative to the order of
       *&#64;&#64;     requests. The default is false, which means the model will generate
       *&#64;&#64;     exactly one response for each request.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool decoupled = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearDecoupled() {
        copyOnWrite();
        instance.clearDecoupled();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:inference.ModelTransactionPolicy)
    }
    @java.lang.Override
    @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
    protected final java.lang.Object dynamicMethod(
        com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
        java.lang.Object arg0, java.lang.Object arg1) {
      switch (method) {
        case NEW_MUTABLE_INSTANCE: {
          return new inference.ModelConfigOuterClass.ModelTransactionPolicy();
        }
        case NEW_BUILDER: {
          return new Builder();
        }
        case BUILD_MESSAGE_INFO: {
            java.lang.Object[] objects = new java.lang.Object[] {
              "decoupled_",
            };
            java.lang.String info =
                "\u0000\u0001\u0000\u0000\u0001\u0001\u0001\u0000\u0000\u0000\u0001\u0007";
            return newMessageInfo(DEFAULT_INSTANCE, info, objects);
        }
        // fall through
        case GET_DEFAULT_INSTANCE: {
          return DEFAULT_INSTANCE;
        }
        case GET_PARSER: {
          com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelTransactionPolicy> parser = PARSER;
          if (parser == null) {
            synchronized (inference.ModelConfigOuterClass.ModelTransactionPolicy.class) {
              parser = PARSER;
              if (parser == null) {
                parser =
                    new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelTransactionPolicy>(
                        DEFAULT_INSTANCE);
                PARSER = parser;
              }
            }
          }
          return parser;
      }
      case GET_MEMOIZED_IS_INITIALIZED: {
        return (byte) 1;
      }
      case SET_MEMOIZED_IS_INITIALIZED: {
        return null;
      }
      }
      throw new UnsupportedOperationException();
    }


    // @@protoc_insertion_point(class_scope:inference.ModelTransactionPolicy)
    private static final inference.ModelConfigOuterClass.ModelTransactionPolicy DEFAULT_INSTANCE;
    static {
      ModelTransactionPolicy defaultInstance = new ModelTransactionPolicy();
      // New instances are implicitly immutable so no need to make
      // immutable.
      DEFAULT_INSTANCE = defaultInstance;
      com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
        ModelTransactionPolicy.class, defaultInstance);
    }

    public static inference.ModelConfigOuterClass.ModelTransactionPolicy getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static volatile com.google.protobuf.Parser<ModelTransactionPolicy> PARSER;

    public static com.google.protobuf.Parser<ModelTransactionPolicy> parser() {
      return DEFAULT_INSTANCE.getParserForType();
    }
  }

  public interface ModelRepositoryAgentsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelRepositoryAgents)
      com.google.protobuf.MessageLiteOrBuilder {

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
     *&#64;&#64;
     *&#64;&#64;     The ordered list of agents for the model. These agents will be
     *&#64;&#64;     invoked in order to respond to repository actions occuring for the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
     */
    java.util.List<inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent> 
        getAgentsList();
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
     *&#64;&#64;
     *&#64;&#64;     The ordered list of agents for the model. These agents will be
     *&#64;&#64;     invoked in order to respond to repository actions occuring for the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
     */
    inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent getAgents(int index);
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
     *&#64;&#64;
     *&#64;&#64;     The ordered list of agents for the model. These agents will be
     *&#64;&#64;     invoked in order to respond to repository actions occuring for the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
     */
    int getAgentsCount();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelRepositoryAgents
   *&#64;&#64;
   *&#64;&#64;   The repository agents for the model.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelRepositoryAgents}
   */
  public  static final class ModelRepositoryAgents extends
      com.google.protobuf.GeneratedMessageLite<
          ModelRepositoryAgents, ModelRepositoryAgents.Builder> implements
      // @@protoc_insertion_point(message_implements:inference.ModelRepositoryAgents)
      ModelRepositoryAgentsOrBuilder {
    private ModelRepositoryAgents() {
      agents_ = emptyProtobufList();
    }
    public interface AgentOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelRepositoryAgents.Agent)
        com.google.protobuf.MessageLiteOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the agent.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The name.
       */
      java.lang.String getName();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the agent.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      com.google.protobuf.ByteString
          getNameBytes();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
       *&#64;&#64;
       *&#64;&#64;       The parameters for the agent.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; parameters = 2;</code>
       */
      int getParametersCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
       *&#64;&#64;
       *&#64;&#64;       The parameters for the agent.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; parameters = 2;</code>
       */
      boolean containsParameters(
          java.lang.String key);
      /**
       * Use {@link #getParametersMap()} instead.
       */
      @java.lang.Deprecated
      java.util.Map<java.lang.String, java.lang.String>
      getParameters();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
       *&#64;&#64;
       *&#64;&#64;       The parameters for the agent.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; parameters = 2;</code>
       */
      java.util.Map<java.lang.String, java.lang.String>
      getParametersMap();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
       *&#64;&#64;
       *&#64;&#64;       The parameters for the agent.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; parameters = 2;</code>
       */

      java.lang.String getParametersOrDefault(
          java.lang.String key,
          java.lang.String defaultValue);
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
       *&#64;&#64;
       *&#64;&#64;       The parameters for the agent.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; parameters = 2;</code>
       */

      java.lang.String getParametersOrThrow(
          java.lang.String key);
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: message Agent
     *&#64;&#64;
     *&#64;&#64;     A repository agent that should be invoked for the specified
     *&#64;&#64;     repository actions for this model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelRepositoryAgents.Agent}
     */
    public  static final class Agent extends
        com.google.protobuf.GeneratedMessageLite<
            Agent, Agent.Builder> implements
        // @@protoc_insertion_point(message_implements:inference.ModelRepositoryAgents.Agent)
        AgentOrBuilder {
      private Agent() {
        name_ = "";
      }
      public static final int NAME_FIELD_NUMBER = 1;
      private java.lang.String name_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the agent.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The name.
       */
      @java.lang.Override
      public java.lang.String getName() {
        return name_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the agent.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString
          getNameBytes() {
        return com.google.protobuf.ByteString.copyFromUtf8(name_);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the agent.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The name to set.
       */
      private void setName(
          java.lang.String value) {
        java.lang.Class<?> valueClass = value.getClass();
  
        name_ = value;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the agent.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      private void clearName() {
        
        name_ = getDefaultInstance().getName();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the agent.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The bytes for name to set.
       */
      private void setNameBytes(
          com.google.protobuf.ByteString value) {
        checkByteStringIsUtf8(value);
        name_ = value.toStringUtf8();
        
      }

      public static final int PARAMETERS_FIELD_NUMBER = 2;
      private static final class ParametersDefaultEntryHolder {
        static final com.google.protobuf.MapEntryLite<
            java.lang.String, java.lang.String> defaultEntry =
                com.google.protobuf.MapEntryLite
                .<java.lang.String, java.lang.String>newDefaultInstance(
                    com.google.protobuf.WireFormat.FieldType.STRING,
                    "",
                    com.google.protobuf.WireFormat.FieldType.STRING,
                    "");
      }
      private com.google.protobuf.MapFieldLite<
          java.lang.String, java.lang.String> parameters_ =
              com.google.protobuf.MapFieldLite.emptyMapField();
      private com.google.protobuf.MapFieldLite<java.lang.String, java.lang.String>
      internalGetParameters() {
        return parameters_;
      }
      private com.google.protobuf.MapFieldLite<java.lang.String, java.lang.String>
      internalGetMutableParameters() {
        if (!parameters_.isMutable()) {
          parameters_ = parameters_.mutableCopy();
        }
        return parameters_;
      }
      @java.lang.Override

      public int getParametersCount() {
        return internalGetParameters().size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
       *&#64;&#64;
       *&#64;&#64;       The parameters for the agent.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; parameters = 2;</code>
       */
      @java.lang.Override

      public boolean containsParameters(
          java.lang.String key) {
        java.lang.Class<?> keyClass = key.getClass();
        return internalGetParameters().containsKey(key);
      }
      /**
       * Use {@link #getParametersMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getParameters() {
        return getParametersMap();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
       *&#64;&#64;
       *&#64;&#64;       The parameters for the agent.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; parameters = 2;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, java.lang.String> getParametersMap() {
        return java.util.Collections.unmodifiableMap(
            internalGetParameters());
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
       *&#64;&#64;
       *&#64;&#64;       The parameters for the agent.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; parameters = 2;</code>
       */
      @java.lang.Override

      public java.lang.String getParametersOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        java.lang.Class<?> keyClass = key.getClass();
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetParameters();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
       *&#64;&#64;
       *&#64;&#64;       The parameters for the agent.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; parameters = 2;</code>
       */
      @java.lang.Override

      public java.lang.String getParametersOrThrow(
          java.lang.String key) {
        java.lang.Class<?> keyClass = key.getClass();
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetParameters();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
       *&#64;&#64;
       *&#64;&#64;       The parameters for the agent.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; parameters = 2;</code>
       */
      private java.util.Map<java.lang.String, java.lang.String>
      getMutableParametersMap() {
        return internalGetMutableParameters();
      }

      public static inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data);
      }
      public static inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input);
      }
      public static inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageLite.parseFrom(
            DEFAULT_INSTANCE, input, extensionRegistry);
      }

      public static Builder newBuilder() {
        return (Builder) DEFAULT_INSTANCE.createBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent prototype) {
        return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
      }

      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: message Agent
       *&#64;&#64;
       *&#64;&#64;     A repository agent that should be invoked for the specified
       *&#64;&#64;     repository actions for this model.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelRepositoryAgents.Agent}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageLite.Builder<
            inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent, Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelRepositoryAgents.Agent)
          inference.ModelConfigOuterClass.ModelRepositoryAgents.AgentOrBuilder {
        // Construct using inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent.newBuilder()
        private Builder() {
          super(DEFAULT_INSTANCE);
        }


        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the agent.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @return The name.
         */
        @java.lang.Override
        public java.lang.String getName() {
          return instance.getName();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the agent.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @return The bytes for name.
         */
        @java.lang.Override
        public com.google.protobuf.ByteString
            getNameBytes() {
          return instance.getNameBytes();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the agent.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @param value The name to set.
         * @return This builder for chaining.
         */
        public Builder setName(
            java.lang.String value) {
          copyOnWrite();
          instance.setName(value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the agent.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearName() {
          copyOnWrite();
          instance.clearName();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the agent.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @param value The bytes for name to set.
         * @return This builder for chaining.
         */
        public Builder setNameBytes(
            com.google.protobuf.ByteString value) {
          copyOnWrite();
          instance.setNameBytes(value);
          return this;
        }

        @java.lang.Override

        public int getParametersCount() {
          return instance.getParametersMap().size();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       The parameters for the agent.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */
        @java.lang.Override

        public boolean containsParameters(
            java.lang.String key) {
          java.lang.Class<?> keyClass = key.getClass();
          return instance.getParametersMap().containsKey(key);
        }

        public Builder clearParameters() {
          copyOnWrite();
          instance.getMutableParametersMap().clear();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       The parameters for the agent.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */

        public Builder removeParameters(
            java.lang.String key) {
          java.lang.Class<?> keyClass = key.getClass();
          copyOnWrite();
          instance.getMutableParametersMap().remove(key);
          return this;
        }
        /**
         * Use {@link #getParametersMap()} instead.
         */
        @java.lang.Override
        @java.lang.Deprecated
        public java.util.Map<java.lang.String, java.lang.String> getParameters() {
          return getParametersMap();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       The parameters for the agent.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */
        @java.lang.Override
        public java.util.Map<java.lang.String, java.lang.String> getParametersMap() {
          return java.util.Collections.unmodifiableMap(
              instance.getParametersMap());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       The parameters for the agent.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */
        @java.lang.Override

        public java.lang.String getParametersOrDefault(
            java.lang.String key,
            java.lang.String defaultValue) {
          java.lang.Class<?> keyClass = key.getClass();
          java.util.Map<java.lang.String, java.lang.String> map =
              instance.getParametersMap();
          return map.containsKey(key) ? map.get(key) : defaultValue;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       The parameters for the agent.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */
        @java.lang.Override

        public java.lang.String getParametersOrThrow(
            java.lang.String key) {
          java.lang.Class<?> keyClass = key.getClass();
          java.util.Map<java.lang.String, java.lang.String> map =
              instance.getParametersMap();
          if (!map.containsKey(key)) {
            throw new java.lang.IllegalArgumentException();
          }
          return map.get(key);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       The parameters for the agent.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */
        public Builder putParameters(
            java.lang.String key,
            java.lang.String value) {
          java.lang.Class<?> keyClass = key.getClass();
          java.lang.Class<?> valueClass = value.getClass();
          copyOnWrite();
          instance.getMutableParametersMap().put(key, value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       The parameters for the agent.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */
        public Builder putAllParameters(
            java.util.Map<java.lang.String, java.lang.String> values) {
          copyOnWrite();
          instance.getMutableParametersMap().putAll(values);
          return this;
        }

        // @@protoc_insertion_point(builder_scope:inference.ModelRepositoryAgents.Agent)
      }
      @java.lang.Override
      @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
      protected final java.lang.Object dynamicMethod(
          com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
          java.lang.Object arg0, java.lang.Object arg1) {
        switch (method) {
          case NEW_MUTABLE_INSTANCE: {
            return new inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent();
          }
          case NEW_BUILDER: {
            return new Builder();
          }
          case BUILD_MESSAGE_INFO: {
              java.lang.Object[] objects = new java.lang.Object[] {
                "name_",
                "parameters_",
                ParametersDefaultEntryHolder.defaultEntry,
              };
              java.lang.String info =
                  "\u0000\u0002\u0000\u0000\u0001\u0002\u0002\u0001\u0000\u0000\u0001\u0208\u00022";
              return newMessageInfo(DEFAULT_INSTANCE, info, objects);
          }
          // fall through
          case GET_DEFAULT_INSTANCE: {
            return DEFAULT_INSTANCE;
          }
          case GET_PARSER: {
            com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent> parser = PARSER;
            if (parser == null) {
              synchronized (inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent.class) {
                parser = PARSER;
                if (parser == null) {
                  parser =
                      new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent>(
                          DEFAULT_INSTANCE);
                  PARSER = parser;
                }
              }
            }
            return parser;
        }
        case GET_MEMOIZED_IS_INITIALIZED: {
          return (byte) 1;
        }
        case SET_MEMOIZED_IS_INITIALIZED: {
          return null;
        }
        }
        throw new UnsupportedOperationException();
      }


      // @@protoc_insertion_point(class_scope:inference.ModelRepositoryAgents.Agent)
      private static final inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent DEFAULT_INSTANCE;
      static {
        Agent defaultInstance = new Agent();
        // New instances are implicitly immutable so no need to make
        // immutable.
        DEFAULT_INSTANCE = defaultInstance;
        com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
          Agent.class, defaultInstance);
      }

      public static inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static volatile com.google.protobuf.Parser<Agent> PARSER;

      public static com.google.protobuf.Parser<Agent> parser() {
        return DEFAULT_INSTANCE.getParserForType();
      }
    }

    public static final int AGENTS_FIELD_NUMBER = 1;
    private com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent> agents_;
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
     *&#64;&#64;
     *&#64;&#64;     The ordered list of agents for the model. These agents will be
     *&#64;&#64;     invoked in order to respond to repository actions occuring for the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
     */
    @java.lang.Override
    public java.util.List<inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent> getAgentsList() {
      return agents_;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
     *&#64;&#64;
     *&#64;&#64;     The ordered list of agents for the model. These agents will be
     *&#64;&#64;     invoked in order to respond to repository actions occuring for the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
     */
    public java.util.List<? extends inference.ModelConfigOuterClass.ModelRepositoryAgents.AgentOrBuilder> 
        getAgentsOrBuilderList() {
      return agents_;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
     *&#64;&#64;
     *&#64;&#64;     The ordered list of agents for the model. These agents will be
     *&#64;&#64;     invoked in order to respond to repository actions occuring for the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
     */
    @java.lang.Override
    public int getAgentsCount() {
      return agents_.size();
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
     *&#64;&#64;
     *&#64;&#64;     The ordered list of agents for the model. These agents will be
     *&#64;&#64;     invoked in order to respond to repository actions occuring for the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent getAgents(int index) {
      return agents_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
     *&#64;&#64;
     *&#64;&#64;     The ordered list of agents for the model. These agents will be
     *&#64;&#64;     invoked in order to respond to repository actions occuring for the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
     */
    public inference.ModelConfigOuterClass.ModelRepositoryAgents.AgentOrBuilder getAgentsOrBuilder(
        int index) {
      return agents_.get(index);
    }
    private void ensureAgentsIsMutable() {
      com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent> tmp = agents_;
      if (!tmp.isModifiable()) {
        agents_ =
            com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
       }
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
     *&#64;&#64;
     *&#64;&#64;     The ordered list of agents for the model. These agents will be
     *&#64;&#64;     invoked in order to respond to repository actions occuring for the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
     */
    private void setAgents(
        int index, inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent value) {
      value.getClass();
  ensureAgentsIsMutable();
      agents_.set(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
     *&#64;&#64;
     *&#64;&#64;     The ordered list of agents for the model. These agents will be
     *&#64;&#64;     invoked in order to respond to repository actions occuring for the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
     */
    private void addAgents(inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent value) {
      value.getClass();
  ensureAgentsIsMutable();
      agents_.add(value);
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
     *&#64;&#64;
     *&#64;&#64;     The ordered list of agents for the model. These agents will be
     *&#64;&#64;     invoked in order to respond to repository actions occuring for the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
     */
    private void addAgents(
        int index, inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent value) {
      value.getClass();
  ensureAgentsIsMutable();
      agents_.add(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
     *&#64;&#64;
     *&#64;&#64;     The ordered list of agents for the model. These agents will be
     *&#64;&#64;     invoked in order to respond to repository actions occuring for the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
     */
    private void addAllAgents(
        java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent> values) {
      ensureAgentsIsMutable();
      com.google.protobuf.AbstractMessageLite.addAll(
          values, agents_);
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
     *&#64;&#64;
     *&#64;&#64;     The ordered list of agents for the model. These agents will be
     *&#64;&#64;     invoked in order to respond to repository actions occuring for the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
     */
    private void clearAgents() {
      agents_ = emptyProtobufList();
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
     *&#64;&#64;
     *&#64;&#64;     The ordered list of agents for the model. These agents will be
     *&#64;&#64;     invoked in order to respond to repository actions occuring for the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
     */
    private void removeAgents(int index) {
      ensureAgentsIsMutable();
      agents_.remove(index);
    }

    public static inference.ModelConfigOuterClass.ModelRepositoryAgents parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelRepositoryAgents parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelRepositoryAgents parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelRepositoryAgents parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelRepositoryAgents parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelRepositoryAgents parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelRepositoryAgents parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelRepositoryAgents parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelRepositoryAgents parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelRepositoryAgents parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelRepositoryAgents parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelRepositoryAgents parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }

    public static Builder newBuilder() {
      return (Builder) DEFAULT_INSTANCE.createBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelRepositoryAgents prototype) {
      return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelRepositoryAgents
     *&#64;&#64;
     *&#64;&#64;   The repository agents for the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelRepositoryAgents}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageLite.Builder<
          inference.ModelConfigOuterClass.ModelRepositoryAgents, Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelRepositoryAgents)
        inference.ModelConfigOuterClass.ModelRepositoryAgentsOrBuilder {
      // Construct using inference.ModelConfigOuterClass.ModelRepositoryAgents.newBuilder()
      private Builder() {
        super(DEFAULT_INSTANCE);
      }


      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
       *&#64;&#64;
       *&#64;&#64;     The ordered list of agents for the model. These agents will be
       *&#64;&#64;     invoked in order to respond to repository actions occuring for the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
       */
      @java.lang.Override
      public java.util.List<inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent> getAgentsList() {
        return java.util.Collections.unmodifiableList(
            instance.getAgentsList());
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
       *&#64;&#64;
       *&#64;&#64;     The ordered list of agents for the model. These agents will be
       *&#64;&#64;     invoked in order to respond to repository actions occuring for the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
       */
      @java.lang.Override
      public int getAgentsCount() {
        return instance.getAgentsCount();
      }/**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
       *&#64;&#64;
       *&#64;&#64;     The ordered list of agents for the model. These agents will be
       *&#64;&#64;     invoked in order to respond to repository actions occuring for the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent getAgents(int index) {
        return instance.getAgents(index);
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
       *&#64;&#64;
       *&#64;&#64;     The ordered list of agents for the model. These agents will be
       *&#64;&#64;     invoked in order to respond to repository actions occuring for the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
       */
      public Builder setAgents(
          int index, inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent value) {
        copyOnWrite();
        instance.setAgents(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
       *&#64;&#64;
       *&#64;&#64;     The ordered list of agents for the model. These agents will be
       *&#64;&#64;     invoked in order to respond to repository actions occuring for the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
       */
      public Builder setAgents(
          int index, inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent.Builder builderForValue) {
        copyOnWrite();
        instance.setAgents(index,
            builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
       *&#64;&#64;
       *&#64;&#64;     The ordered list of agents for the model. These agents will be
       *&#64;&#64;     invoked in order to respond to repository actions occuring for the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
       */
      public Builder addAgents(inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent value) {
        copyOnWrite();
        instance.addAgents(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
       *&#64;&#64;
       *&#64;&#64;     The ordered list of agents for the model. These agents will be
       *&#64;&#64;     invoked in order to respond to repository actions occuring for the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
       */
      public Builder addAgents(
          int index, inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent value) {
        copyOnWrite();
        instance.addAgents(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
       *&#64;&#64;
       *&#64;&#64;     The ordered list of agents for the model. These agents will be
       *&#64;&#64;     invoked in order to respond to repository actions occuring for the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
       */
      public Builder addAgents(
          inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent.Builder builderForValue) {
        copyOnWrite();
        instance.addAgents(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
       *&#64;&#64;
       *&#64;&#64;     The ordered list of agents for the model. These agents will be
       *&#64;&#64;     invoked in order to respond to repository actions occuring for the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
       */
      public Builder addAgents(
          int index, inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent.Builder builderForValue) {
        copyOnWrite();
        instance.addAgents(index,
            builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
       *&#64;&#64;
       *&#64;&#64;     The ordered list of agents for the model. These agents will be
       *&#64;&#64;     invoked in order to respond to repository actions occuring for the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
       */
      public Builder addAllAgents(
          java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent> values) {
        copyOnWrite();
        instance.addAllAgents(values);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
       *&#64;&#64;
       *&#64;&#64;     The ordered list of agents for the model. These agents will be
       *&#64;&#64;     invoked in order to respond to repository actions occuring for the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
       */
      public Builder clearAgents() {
        copyOnWrite();
        instance.clearAgents();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: Agent agents (repeated)
       *&#64;&#64;
       *&#64;&#64;     The ordered list of agents for the model. These agents will be
       *&#64;&#64;     invoked in order to respond to repository actions occuring for the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRepositoryAgents.Agent agents = 1;</code>
       */
      public Builder removeAgents(int index) {
        copyOnWrite();
        instance.removeAgents(index);
        return this;
      }

      // @@protoc_insertion_point(builder_scope:inference.ModelRepositoryAgents)
    }
    @java.lang.Override
    @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
    protected final java.lang.Object dynamicMethod(
        com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
        java.lang.Object arg0, java.lang.Object arg1) {
      switch (method) {
        case NEW_MUTABLE_INSTANCE: {
          return new inference.ModelConfigOuterClass.ModelRepositoryAgents();
        }
        case NEW_BUILDER: {
          return new Builder();
        }
        case BUILD_MESSAGE_INFO: {
            java.lang.Object[] objects = new java.lang.Object[] {
              "agents_",
              inference.ModelConfigOuterClass.ModelRepositoryAgents.Agent.class,
            };
            java.lang.String info =
                "\u0000\u0001\u0000\u0000\u0001\u0001\u0001\u0000\u0001\u0000\u0001\u001b";
            return newMessageInfo(DEFAULT_INSTANCE, info, objects);
        }
        // fall through
        case GET_DEFAULT_INSTANCE: {
          return DEFAULT_INSTANCE;
        }
        case GET_PARSER: {
          com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelRepositoryAgents> parser = PARSER;
          if (parser == null) {
            synchronized (inference.ModelConfigOuterClass.ModelRepositoryAgents.class) {
              parser = PARSER;
              if (parser == null) {
                parser =
                    new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelRepositoryAgents>(
                        DEFAULT_INSTANCE);
                PARSER = parser;
              }
            }
          }
          return parser;
      }
      case GET_MEMOIZED_IS_INITIALIZED: {
        return (byte) 1;
      }
      case SET_MEMOIZED_IS_INITIALIZED: {
        return null;
      }
      }
      throw new UnsupportedOperationException();
    }


    // @@protoc_insertion_point(class_scope:inference.ModelRepositoryAgents)
    private static final inference.ModelConfigOuterClass.ModelRepositoryAgents DEFAULT_INSTANCE;
    static {
      ModelRepositoryAgents defaultInstance = new ModelRepositoryAgents();
      // New instances are implicitly immutable so no need to make
      // immutable.
      DEFAULT_INSTANCE = defaultInstance;
      com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
        ModelRepositoryAgents.class, defaultInstance);
    }

    public static inference.ModelConfigOuterClass.ModelRepositoryAgents getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static volatile com.google.protobuf.Parser<ModelRepositoryAgents> PARSER;

    public static com.google.protobuf.Parser<ModelRepositoryAgents> parser() {
      return DEFAULT_INSTANCE.getParserForType();
    }
  }

  public interface ModelConfigOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelConfig)
      com.google.protobuf.MessageLiteOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string platform
     *&#64;&#64;
     *&#64;&#64;     The framework for the model. Possible values are
     *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
     *&#64;&#64;     "tensorflow_savedmodel", "onnxruntime_onnx",
     *&#64;&#64;     "pytorch_libtorch" and "custom".
     *&#64;&#64;
     * </pre>
     *
     * <code>string platform = 2;</code>
     * @return The platform.
     */
    java.lang.String getPlatform();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string platform
     *&#64;&#64;
     *&#64;&#64;     The framework for the model. Possible values are
     *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
     *&#64;&#64;     "tensorflow_savedmodel", "onnxruntime_onnx",
     *&#64;&#64;     "pytorch_libtorch" and "custom".
     *&#64;&#64;
     * </pre>
     *
     * <code>string platform = 2;</code>
     * @return The bytes for platform.
     */
    com.google.protobuf.ByteString
        getPlatformBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string backend
     *&#64;&#64;
     *&#64;&#64;     The backend used by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string backend = 17;</code>
     * @return The backend.
     */
    java.lang.String getBackend();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string backend
     *&#64;&#64;
     *&#64;&#64;     The backend used by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string backend = 17;</code>
     * @return The bytes for backend.
     */
    com.google.protobuf.ByteString
        getBackendBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
     *&#64;&#64;
     *&#64;&#64;     Policy indicating which version(s) of the model will be served.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
     * @return Whether the versionPolicy field is set.
     */
    boolean hasVersionPolicy();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
     *&#64;&#64;
     *&#64;&#64;     Policy indicating which version(s) of the model will be served.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
     * @return The versionPolicy.
     */
    inference.ModelConfigOuterClass.ModelVersionPolicy getVersionPolicy();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 max_batch_size
     *&#64;&#64;
     *&#64;&#64;     Maximum batch size allowed for inference. This can only decrease
     *&#64;&#64;     what is allowed by the model itself. A max_batch_size value of 0
     *&#64;&#64;     indicates that batching is not allowed for the model and the
     *&#64;&#64;     dimension/shape of the input and output tensors must exactly
     *&#64;&#64;     match what is specified in the input and output configuration. A
     *&#64;&#64;     max_batch_size value &gt; 0 indicates that batching is allowed and
     *&#64;&#64;     so the model expects the input tensors to have an additional
     *&#64;&#64;     initial dimension for the batching that is not specified in the
     *&#64;&#64;     input (for example, if the model supports batched inputs of
     *&#64;&#64;     2-dimensional tensors then the model configuration will specify
     *&#64;&#64;     the input shape as [ X, Y ] but the model will expect the actual
     *&#64;&#64;     input tensors to have shape [ N, X, Y ]). For max_batch_size &gt; 0
     *&#64;&#64;     returned outputs will also have an additional initial dimension
     *&#64;&#64;     for the batch.
     *&#64;&#64;
     * </pre>
     *
     * <code>int32 max_batch_size = 4;</code>
     * @return The maxBatchSize.
     */
    int getMaxBatchSize();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInput input = 5;</code>
     */
    java.util.List<inference.ModelConfigOuterClass.ModelInput> 
        getInputList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInput input = 5;</code>
     */
    inference.ModelConfigOuterClass.ModelInput getInput(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInput input = 5;</code>
     */
    int getInputCount();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelOutput output = 6;</code>
     */
    java.util.List<inference.ModelConfigOuterClass.ModelOutput> 
        getOutputList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelOutput output = 6;</code>
     */
    inference.ModelConfigOuterClass.ModelOutput getOutput(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelOutput output = 6;</code>
     */
    int getOutputCount();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     batch related values to the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchInput batch_input = 20;</code>
     */
    java.util.List<inference.ModelConfigOuterClass.BatchInput> 
        getBatchInputList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     batch related values to the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchInput batch_input = 20;</code>
     */
    inference.ModelConfigOuterClass.BatchInput getBatchInput(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     batch related values to the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchInput batch_input = 20;</code>
     */
    int getBatchInputCount();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model that requires special handling
     *&#64;&#64;     by the model backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchOutput batch_output = 21;</code>
     */
    java.util.List<inference.ModelConfigOuterClass.BatchOutput> 
        getBatchOutputList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model that requires special handling
     *&#64;&#64;     by the model backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchOutput batch_output = 21;</code>
     */
    inference.ModelConfigOuterClass.BatchOutput getBatchOutput(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model that requires special handling
     *&#64;&#64;     by the model backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchOutput batch_output = 21;</code>
     */
    int getBatchOutputCount();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
     *&#64;&#64;
     *&#64;&#64;     Optimization configuration for the model. If not specified
     *&#64;&#64;     then default optimization policy is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
     * @return Whether the optimization field is set.
     */
    boolean hasOptimization();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
     *&#64;&#64;
     *&#64;&#64;     Optimization configuration for the model. If not specified
     *&#64;&#64;     then default optimization policy is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
     * @return The optimization.
     */
    inference.ModelConfigOuterClass.ModelOptimizationPolicy getOptimization();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the dynamic-batching scheduling
     *&#64;&#64;       policy. With dynamic-batching the scheduler may group
     *&#64;&#64;       together independent requests into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
     * @return Whether the dynamicBatching field is set.
     */
    boolean hasDynamicBatching();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the dynamic-batching scheduling
     *&#64;&#64;       policy. With dynamic-batching the scheduler may group
     *&#64;&#64;       together independent requests into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
     * @return The dynamicBatching.
     */
    inference.ModelConfigOuterClass.ModelDynamicBatching getDynamicBatching();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the sequence-batching scheduling
     *&#64;&#64;       policy. With sequence-batching, inference requests
     *&#64;&#64;       with the same correlation ID are routed to the same
     *&#64;&#64;       model instance. Multiple sequences of inference requests
     *&#64;&#64;       may be batched together into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
     * @return Whether the sequenceBatching field is set.
     */
    boolean hasSequenceBatching();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the sequence-batching scheduling
     *&#64;&#64;       policy. With sequence-batching, inference requests
     *&#64;&#64;       with the same correlation ID are routed to the same
     *&#64;&#64;       model instance. Multiple sequences of inference requests
     *&#64;&#64;       may be batched together into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
     * @return The sequenceBatching.
     */
    inference.ModelConfigOuterClass.ModelSequenceBatching getSequenceBatching();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the model-ensembling scheduling
     *&#64;&#64;       policy. With model-ensembling, inference requests
     *&#64;&#64;       will be processed according to the specification, such as an
     *&#64;&#64;       execution sequence of models. The input specified in this model
     *&#64;&#64;       config will be the input for the ensemble, and the output
     *&#64;&#64;       specified will be the output of the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
     * @return Whether the ensembleScheduling field is set.
     */
    boolean hasEnsembleScheduling();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the model-ensembling scheduling
     *&#64;&#64;       policy. With model-ensembling, inference requests
     *&#64;&#64;       will be processed according to the specification, such as an
     *&#64;&#64;       execution sequence of models. The input specified in this model
     *&#64;&#64;       config will be the input for the ensemble, and the output
     *&#64;&#64;       specified will be the output of the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
     * @return The ensembleScheduling.
     */
    inference.ModelConfigOuterClass.ModelEnsembling getEnsembleScheduling();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
     */
    java.util.List<inference.ModelConfigOuterClass.ModelInstanceGroup> 
        getInstanceGroupList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
     */
    inference.ModelConfigOuterClass.ModelInstanceGroup getInstanceGroup(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
     */
    int getInstanceGroupCount();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string default_model_filename
     *&#64;&#64;
     *&#64;&#64;     Optional filename of the model file to use if a
     *&#64;&#64;     compute-capability specific model is not specified in
     *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
     *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
     *&#64;&#64;     'model.pt' depending on the model type.
     *&#64;&#64;
     * </pre>
     *
     * <code>string default_model_filename = 8;</code>
     * @return The defaultModelFilename.
     */
    java.lang.String getDefaultModelFilename();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string default_model_filename
     *&#64;&#64;
     *&#64;&#64;     Optional filename of the model file to use if a
     *&#64;&#64;     compute-capability specific model is not specified in
     *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
     *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
     *&#64;&#64;     'model.pt' depending on the model type.
     *&#64;&#64;
     * </pre>
     *
     * <code>string default_model_filename = 8;</code>
     * @return The bytes for defaultModelFilename.
     */
    com.google.protobuf.ByteString
        getDefaultModelFilenameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */
    int getCcModelFilenamesCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */
    boolean containsCcModelFilenames(
        java.lang.String key);
    /**
     * Use {@link #getCcModelFilenamesMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.String>
    getCcModelFilenames();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */
    java.util.Map<java.lang.String, java.lang.String>
    getCcModelFilenamesMap();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */

    java.lang.String getCcModelFilenamesOrDefault(
        java.lang.String key,
        java.lang.String defaultValue);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */

    java.lang.String getCcModelFilenamesOrThrow(
        java.lang.String key);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */
    int getMetricTagsCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */
    boolean containsMetricTags(
        java.lang.String key);
    /**
     * Use {@link #getMetricTagsMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.String>
    getMetricTags();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */
    java.util.Map<java.lang.String, java.lang.String>
    getMetricTagsMap();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */

    java.lang.String getMetricTagsOrDefault(
        java.lang.String key,
        java.lang.String defaultValue);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */

    java.lang.String getMetricTagsOrThrow(
        java.lang.String key);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
     */
    int getParametersCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
     */
    boolean containsParameters(
        java.lang.String key);
    /**
     * Use {@link #getParametersMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelParameter>
    getParameters();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
     */
    java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelParameter>
    getParametersMap();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
     */

    inference.ModelConfigOuterClass.ModelParameter getParametersOrDefault(
        java.lang.String key,
        inference.ModelConfigOuterClass.ModelParameter defaultValue);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
     */

    inference.ModelConfigOuterClass.ModelParameter getParametersOrThrow(
        java.lang.String key);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
     */
    java.util.List<inference.ModelConfigOuterClass.ModelWarmup> 
        getModelWarmupList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
     */
    inference.ModelConfigOuterClass.ModelWarmup getModelWarmup(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
     */
    int getModelWarmupCount();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
     *&#64;&#64;
     *&#64;&#64;     Optional metadata of the libraries providing custom operations for
     *&#64;&#64;     this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOperations model_operations = 18;</code>
     * @return Whether the modelOperations field is set.
     */
    boolean hasModelOperations();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
     *&#64;&#64;
     *&#64;&#64;     Optional metadata of the libraries providing custom operations for
     *&#64;&#64;     this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOperations model_operations = 18;</code>
     * @return The modelOperations.
     */
    inference.ModelConfigOuterClass.ModelOperations getModelOperations();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
     *&#64;&#64;
     *&#64;&#64;     Optional specification that describes the nature of transactions
     *&#64;&#64;     to be expected from the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
     * @return Whether the modelTransactionPolicy field is set.
     */
    boolean hasModelTransactionPolicy();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
     *&#64;&#64;
     *&#64;&#64;     Optional specification that describes the nature of transactions
     *&#64;&#64;     to be expected from the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
     * @return The modelTransactionPolicy.
     */
    inference.ModelConfigOuterClass.ModelTransactionPolicy getModelTransactionPolicy();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelRepositoryAgents model_repository_agents
     *&#64;&#64;
     *&#64;&#64;     Optional specification of the agent(s) that should be invoked
     *&#64;&#64;     with repository actions are performed for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelRepositoryAgents model_repository_agents = 23;</code>
     * @return Whether the modelRepositoryAgents field is set.
     */
    boolean hasModelRepositoryAgents();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelRepositoryAgents model_repository_agents
     *&#64;&#64;
     *&#64;&#64;     Optional specification of the agent(s) that should be invoked
     *&#64;&#64;     with repository actions are performed for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelRepositoryAgents model_repository_agents = 23;</code>
     * @return The modelRepositoryAgents.
     */
    inference.ModelConfigOuterClass.ModelRepositoryAgents getModelRepositoryAgents();

    public inference.ModelConfigOuterClass.ModelConfig.SchedulingChoiceCase getSchedulingChoiceCase();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelConfig
   *&#64;&#64;
   *&#64;&#64;   A model configuration.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelConfig}
   */
  public  static final class ModelConfig extends
      com.google.protobuf.GeneratedMessageLite<
          ModelConfig, ModelConfig.Builder> implements
      // @@protoc_insertion_point(message_implements:inference.ModelConfig)
      ModelConfigOrBuilder {
    private ModelConfig() {
      name_ = "";
      platform_ = "";
      backend_ = "";
      input_ = emptyProtobufList();
      output_ = emptyProtobufList();
      batchInput_ = emptyProtobufList();
      batchOutput_ = emptyProtobufList();
      instanceGroup_ = emptyProtobufList();
      defaultModelFilename_ = "";
      modelWarmup_ = emptyProtobufList();
    }
    private int schedulingChoiceCase_ = 0;
    private java.lang.Object schedulingChoice_;
    public enum SchedulingChoiceCase {
      DYNAMIC_BATCHING(11),
      SEQUENCE_BATCHING(13),
      ENSEMBLE_SCHEDULING(15),
      SCHEDULINGCHOICE_NOT_SET(0);
      private final int value;
      private SchedulingChoiceCase(int value) {
        this.value = value;
      }
      /**
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static SchedulingChoiceCase valueOf(int value) {
        return forNumber(value);
      }

      public static SchedulingChoiceCase forNumber(int value) {
        switch (value) {
          case 11: return DYNAMIC_BATCHING;
          case 13: return SEQUENCE_BATCHING;
          case 15: return ENSEMBLE_SCHEDULING;
          case 0: return SCHEDULINGCHOICE_NOT_SET;
          default: return null;
        }
      }
      public int getNumber() {
        return this.value;
      }
    };

    @java.lang.Override
    public SchedulingChoiceCase
    getSchedulingChoiceCase() {
      return SchedulingChoiceCase.forNumber(
          schedulingChoiceCase_);
    }

    private void clearSchedulingChoice() {
      schedulingChoiceCase_ = 0;
      schedulingChoice_ = null;
    }

    public static final int NAME_FIELD_NUMBER = 1;
    private java.lang.String name_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      return name_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      return com.google.protobuf.ByteString.copyFromUtf8(name_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @param value The name to set.
     */
    private void setName(
        java.lang.String value) {
      java.lang.Class<?> valueClass = value.getClass();
  
      name_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    private void clearName() {
      
      name_ = getDefaultInstance().getName();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @param value The bytes for name to set.
     */
    private void setNameBytes(
        com.google.protobuf.ByteString value) {
      checkByteStringIsUtf8(value);
      name_ = value.toStringUtf8();
      
    }

    public static final int PLATFORM_FIELD_NUMBER = 2;
    private java.lang.String platform_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string platform
     *&#64;&#64;
     *&#64;&#64;     The framework for the model. Possible values are
     *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
     *&#64;&#64;     "tensorflow_savedmodel", "onnxruntime_onnx",
     *&#64;&#64;     "pytorch_libtorch" and "custom".
     *&#64;&#64;
     * </pre>
     *
     * <code>string platform = 2;</code>
     * @return The platform.
     */
    @java.lang.Override
    public java.lang.String getPlatform() {
      return platform_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string platform
     *&#64;&#64;
     *&#64;&#64;     The framework for the model. Possible values are
     *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
     *&#64;&#64;     "tensorflow_savedmodel", "onnxruntime_onnx",
     *&#64;&#64;     "pytorch_libtorch" and "custom".
     *&#64;&#64;
     * </pre>
     *
     * <code>string platform = 2;</code>
     * @return The bytes for platform.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getPlatformBytes() {
      return com.google.protobuf.ByteString.copyFromUtf8(platform_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string platform
     *&#64;&#64;
     *&#64;&#64;     The framework for the model. Possible values are
     *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
     *&#64;&#64;     "tensorflow_savedmodel", "onnxruntime_onnx",
     *&#64;&#64;     "pytorch_libtorch" and "custom".
     *&#64;&#64;
     * </pre>
     *
     * <code>string platform = 2;</code>
     * @param value The platform to set.
     */
    private void setPlatform(
        java.lang.String value) {
      java.lang.Class<?> valueClass = value.getClass();
  
      platform_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string platform
     *&#64;&#64;
     *&#64;&#64;     The framework for the model. Possible values are
     *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
     *&#64;&#64;     "tensorflow_savedmodel", "onnxruntime_onnx",
     *&#64;&#64;     "pytorch_libtorch" and "custom".
     *&#64;&#64;
     * </pre>
     *
     * <code>string platform = 2;</code>
     */
    private void clearPlatform() {
      
      platform_ = getDefaultInstance().getPlatform();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string platform
     *&#64;&#64;
     *&#64;&#64;     The framework for the model. Possible values are
     *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
     *&#64;&#64;     "tensorflow_savedmodel", "onnxruntime_onnx",
     *&#64;&#64;     "pytorch_libtorch" and "custom".
     *&#64;&#64;
     * </pre>
     *
     * <code>string platform = 2;</code>
     * @param value The bytes for platform to set.
     */
    private void setPlatformBytes(
        com.google.protobuf.ByteString value) {
      checkByteStringIsUtf8(value);
      platform_ = value.toStringUtf8();
      
    }

    public static final int BACKEND_FIELD_NUMBER = 17;
    private java.lang.String backend_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string backend
     *&#64;&#64;
     *&#64;&#64;     The backend used by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string backend = 17;</code>
     * @return The backend.
     */
    @java.lang.Override
    public java.lang.String getBackend() {
      return backend_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string backend
     *&#64;&#64;
     *&#64;&#64;     The backend used by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string backend = 17;</code>
     * @return The bytes for backend.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getBackendBytes() {
      return com.google.protobuf.ByteString.copyFromUtf8(backend_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string backend
     *&#64;&#64;
     *&#64;&#64;     The backend used by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string backend = 17;</code>
     * @param value The backend to set.
     */
    private void setBackend(
        java.lang.String value) {
      java.lang.Class<?> valueClass = value.getClass();
  
      backend_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string backend
     *&#64;&#64;
     *&#64;&#64;     The backend used by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string backend = 17;</code>
     */
    private void clearBackend() {
      
      backend_ = getDefaultInstance().getBackend();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string backend
     *&#64;&#64;
     *&#64;&#64;     The backend used by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string backend = 17;</code>
     * @param value The bytes for backend to set.
     */
    private void setBackendBytes(
        com.google.protobuf.ByteString value) {
      checkByteStringIsUtf8(value);
      backend_ = value.toStringUtf8();
      
    }

    public static final int VERSION_POLICY_FIELD_NUMBER = 3;
    private inference.ModelConfigOuterClass.ModelVersionPolicy versionPolicy_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
     *&#64;&#64;
     *&#64;&#64;     Policy indicating which version(s) of the model will be served.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
     */
    @java.lang.Override
    public boolean hasVersionPolicy() {
      return versionPolicy_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
     *&#64;&#64;
     *&#64;&#64;     Policy indicating which version(s) of the model will be served.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelVersionPolicy getVersionPolicy() {
      return versionPolicy_ == null ? inference.ModelConfigOuterClass.ModelVersionPolicy.getDefaultInstance() : versionPolicy_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
     *&#64;&#64;
     *&#64;&#64;     Policy indicating which version(s) of the model will be served.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
     */
    private void setVersionPolicy(inference.ModelConfigOuterClass.ModelVersionPolicy value) {
      value.getClass();
  versionPolicy_ = value;
      
      }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
     *&#64;&#64;
     *&#64;&#64;     Policy indicating which version(s) of the model will be served.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
     */
    @java.lang.SuppressWarnings({"ReferenceEquality"})
    private void mergeVersionPolicy(inference.ModelConfigOuterClass.ModelVersionPolicy value) {
      value.getClass();
  if (versionPolicy_ != null &&
          versionPolicy_ != inference.ModelConfigOuterClass.ModelVersionPolicy.getDefaultInstance()) {
        versionPolicy_ =
          inference.ModelConfigOuterClass.ModelVersionPolicy.newBuilder(versionPolicy_).mergeFrom(value).buildPartial();
      } else {
        versionPolicy_ = value;
      }
      
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
     *&#64;&#64;
     *&#64;&#64;     Policy indicating which version(s) of the model will be served.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
     */
    private void clearVersionPolicy() {  versionPolicy_ = null;
      
    }

    public static final int MAX_BATCH_SIZE_FIELD_NUMBER = 4;
    private int maxBatchSize_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 max_batch_size
     *&#64;&#64;
     *&#64;&#64;     Maximum batch size allowed for inference. This can only decrease
     *&#64;&#64;     what is allowed by the model itself. A max_batch_size value of 0
     *&#64;&#64;     indicates that batching is not allowed for the model and the
     *&#64;&#64;     dimension/shape of the input and output tensors must exactly
     *&#64;&#64;     match what is specified in the input and output configuration. A
     *&#64;&#64;     max_batch_size value &gt; 0 indicates that batching is allowed and
     *&#64;&#64;     so the model expects the input tensors to have an additional
     *&#64;&#64;     initial dimension for the batching that is not specified in the
     *&#64;&#64;     input (for example, if the model supports batched inputs of
     *&#64;&#64;     2-dimensional tensors then the model configuration will specify
     *&#64;&#64;     the input shape as [ X, Y ] but the model will expect the actual
     *&#64;&#64;     input tensors to have shape [ N, X, Y ]). For max_batch_size &gt; 0
     *&#64;&#64;     returned outputs will also have an additional initial dimension
     *&#64;&#64;     for the batch.
     *&#64;&#64;
     * </pre>
     *
     * <code>int32 max_batch_size = 4;</code>
     * @return The maxBatchSize.
     */
    @java.lang.Override
    public int getMaxBatchSize() {
      return maxBatchSize_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 max_batch_size
     *&#64;&#64;
     *&#64;&#64;     Maximum batch size allowed for inference. This can only decrease
     *&#64;&#64;     what is allowed by the model itself. A max_batch_size value of 0
     *&#64;&#64;     indicates that batching is not allowed for the model and the
     *&#64;&#64;     dimension/shape of the input and output tensors must exactly
     *&#64;&#64;     match what is specified in the input and output configuration. A
     *&#64;&#64;     max_batch_size value &gt; 0 indicates that batching is allowed and
     *&#64;&#64;     so the model expects the input tensors to have an additional
     *&#64;&#64;     initial dimension for the batching that is not specified in the
     *&#64;&#64;     input (for example, if the model supports batched inputs of
     *&#64;&#64;     2-dimensional tensors then the model configuration will specify
     *&#64;&#64;     the input shape as [ X, Y ] but the model will expect the actual
     *&#64;&#64;     input tensors to have shape [ N, X, Y ]). For max_batch_size &gt; 0
     *&#64;&#64;     returned outputs will also have an additional initial dimension
     *&#64;&#64;     for the batch.
     *&#64;&#64;
     * </pre>
     *
     * <code>int32 max_batch_size = 4;</code>
     * @param value The maxBatchSize to set.
     */
    private void setMaxBatchSize(int value) {
      
      maxBatchSize_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 max_batch_size
     *&#64;&#64;
     *&#64;&#64;     Maximum batch size allowed for inference. This can only decrease
     *&#64;&#64;     what is allowed by the model itself. A max_batch_size value of 0
     *&#64;&#64;     indicates that batching is not allowed for the model and the
     *&#64;&#64;     dimension/shape of the input and output tensors must exactly
     *&#64;&#64;     match what is specified in the input and output configuration. A
     *&#64;&#64;     max_batch_size value &gt; 0 indicates that batching is allowed and
     *&#64;&#64;     so the model expects the input tensors to have an additional
     *&#64;&#64;     initial dimension for the batching that is not specified in the
     *&#64;&#64;     input (for example, if the model supports batched inputs of
     *&#64;&#64;     2-dimensional tensors then the model configuration will specify
     *&#64;&#64;     the input shape as [ X, Y ] but the model will expect the actual
     *&#64;&#64;     input tensors to have shape [ N, X, Y ]). For max_batch_size &gt; 0
     *&#64;&#64;     returned outputs will also have an additional initial dimension
     *&#64;&#64;     for the batch.
     *&#64;&#64;
     * </pre>
     *
     * <code>int32 max_batch_size = 4;</code>
     */
    private void clearMaxBatchSize() {
      
      maxBatchSize_ = 0;
    }

    public static final int INPUT_FIELD_NUMBER = 5;
    private com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.ModelInput> input_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInput input = 5;</code>
     */
    @java.lang.Override
    public java.util.List<inference.ModelConfigOuterClass.ModelInput> getInputList() {
      return input_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInput input = 5;</code>
     */
    public java.util.List<? extends inference.ModelConfigOuterClass.ModelInputOrBuilder> 
        getInputOrBuilderList() {
      return input_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInput input = 5;</code>
     */
    @java.lang.Override
    public int getInputCount() {
      return input_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInput input = 5;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelInput getInput(int index) {
      return input_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInput input = 5;</code>
     */
    public inference.ModelConfigOuterClass.ModelInputOrBuilder getInputOrBuilder(
        int index) {
      return input_.get(index);
    }
    private void ensureInputIsMutable() {
      com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.ModelInput> tmp = input_;
      if (!tmp.isModifiable()) {
        input_ =
            com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
       }
    }

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInput input = 5;</code>
     */
    private void setInput(
        int index, inference.ModelConfigOuterClass.ModelInput value) {
      value.getClass();
  ensureInputIsMutable();
      input_.set(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInput input = 5;</code>
     */
    private void addInput(inference.ModelConfigOuterClass.ModelInput value) {
      value.getClass();
  ensureInputIsMutable();
      input_.add(value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInput input = 5;</code>
     */
    private void addInput(
        int index, inference.ModelConfigOuterClass.ModelInput value) {
      value.getClass();
  ensureInputIsMutable();
      input_.add(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInput input = 5;</code>
     */
    private void addAllInput(
        java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelInput> values) {
      ensureInputIsMutable();
      com.google.protobuf.AbstractMessageLite.addAll(
          values, input_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInput input = 5;</code>
     */
    private void clearInput() {
      input_ = emptyProtobufList();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInput input = 5;</code>
     */
    private void removeInput(int index) {
      ensureInputIsMutable();
      input_.remove(index);
    }

    public static final int OUTPUT_FIELD_NUMBER = 6;
    private com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.ModelOutput> output_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelOutput output = 6;</code>
     */
    @java.lang.Override
    public java.util.List<inference.ModelConfigOuterClass.ModelOutput> getOutputList() {
      return output_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelOutput output = 6;</code>
     */
    public java.util.List<? extends inference.ModelConfigOuterClass.ModelOutputOrBuilder> 
        getOutputOrBuilderList() {
      return output_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelOutput output = 6;</code>
     */
    @java.lang.Override
    public int getOutputCount() {
      return output_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelOutput output = 6;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOutput getOutput(int index) {
      return output_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelOutput output = 6;</code>
     */
    public inference.ModelConfigOuterClass.ModelOutputOrBuilder getOutputOrBuilder(
        int index) {
      return output_.get(index);
    }
    private void ensureOutputIsMutable() {
      com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.ModelOutput> tmp = output_;
      if (!tmp.isModifiable()) {
        output_ =
            com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
       }
    }

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelOutput output = 6;</code>
     */
    private void setOutput(
        int index, inference.ModelConfigOuterClass.ModelOutput value) {
      value.getClass();
  ensureOutputIsMutable();
      output_.set(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelOutput output = 6;</code>
     */
    private void addOutput(inference.ModelConfigOuterClass.ModelOutput value) {
      value.getClass();
  ensureOutputIsMutable();
      output_.add(value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelOutput output = 6;</code>
     */
    private void addOutput(
        int index, inference.ModelConfigOuterClass.ModelOutput value) {
      value.getClass();
  ensureOutputIsMutable();
      output_.add(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelOutput output = 6;</code>
     */
    private void addAllOutput(
        java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelOutput> values) {
      ensureOutputIsMutable();
      com.google.protobuf.AbstractMessageLite.addAll(
          values, output_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelOutput output = 6;</code>
     */
    private void clearOutput() {
      output_ = emptyProtobufList();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelOutput output = 6;</code>
     */
    private void removeOutput(int index) {
      ensureOutputIsMutable();
      output_.remove(index);
    }

    public static final int BATCH_INPUT_FIELD_NUMBER = 20;
    private com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.BatchInput> batchInput_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     batch related values to the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchInput batch_input = 20;</code>
     */
    @java.lang.Override
    public java.util.List<inference.ModelConfigOuterClass.BatchInput> getBatchInputList() {
      return batchInput_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     batch related values to the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchInput batch_input = 20;</code>
     */
    public java.util.List<? extends inference.ModelConfigOuterClass.BatchInputOrBuilder> 
        getBatchInputOrBuilderList() {
      return batchInput_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     batch related values to the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchInput batch_input = 20;</code>
     */
    @java.lang.Override
    public int getBatchInputCount() {
      return batchInput_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     batch related values to the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchInput batch_input = 20;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.BatchInput getBatchInput(int index) {
      return batchInput_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     batch related values to the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchInput batch_input = 20;</code>
     */
    public inference.ModelConfigOuterClass.BatchInputOrBuilder getBatchInputOrBuilder(
        int index) {
      return batchInput_.get(index);
    }
    private void ensureBatchInputIsMutable() {
      com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.BatchInput> tmp = batchInput_;
      if (!tmp.isModifiable()) {
        batchInput_ =
            com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
       }
    }

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     batch related values to the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchInput batch_input = 20;</code>
     */
    private void setBatchInput(
        int index, inference.ModelConfigOuterClass.BatchInput value) {
      value.getClass();
  ensureBatchInputIsMutable();
      batchInput_.set(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     batch related values to the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchInput batch_input = 20;</code>
     */
    private void addBatchInput(inference.ModelConfigOuterClass.BatchInput value) {
      value.getClass();
  ensureBatchInputIsMutable();
      batchInput_.add(value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     batch related values to the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchInput batch_input = 20;</code>
     */
    private void addBatchInput(
        int index, inference.ModelConfigOuterClass.BatchInput value) {
      value.getClass();
  ensureBatchInputIsMutable();
      batchInput_.add(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     batch related values to the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchInput batch_input = 20;</code>
     */
    private void addAllBatchInput(
        java.lang.Iterable<? extends inference.ModelConfigOuterClass.BatchInput> values) {
      ensureBatchInputIsMutable();
      com.google.protobuf.AbstractMessageLite.addAll(
          values, batchInput_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     batch related values to the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchInput batch_input = 20;</code>
     */
    private void clearBatchInput() {
      batchInput_ = emptyProtobufList();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     batch related values to the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchInput batch_input = 20;</code>
     */
    private void removeBatchInput(int index) {
      ensureBatchInputIsMutable();
      batchInput_.remove(index);
    }

    public static final int BATCH_OUTPUT_FIELD_NUMBER = 21;
    private com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.BatchOutput> batchOutput_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model that requires special handling
     *&#64;&#64;     by the model backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchOutput batch_output = 21;</code>
     */
    @java.lang.Override
    public java.util.List<inference.ModelConfigOuterClass.BatchOutput> getBatchOutputList() {
      return batchOutput_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model that requires special handling
     *&#64;&#64;     by the model backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchOutput batch_output = 21;</code>
     */
    public java.util.List<? extends inference.ModelConfigOuterClass.BatchOutputOrBuilder> 
        getBatchOutputOrBuilderList() {
      return batchOutput_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model that requires special handling
     *&#64;&#64;     by the model backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchOutput batch_output = 21;</code>
     */
    @java.lang.Override
    public int getBatchOutputCount() {
      return batchOutput_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model that requires special handling
     *&#64;&#64;     by the model backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchOutput batch_output = 21;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.BatchOutput getBatchOutput(int index) {
      return batchOutput_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model that requires special handling
     *&#64;&#64;     by the model backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchOutput batch_output = 21;</code>
     */
    public inference.ModelConfigOuterClass.BatchOutputOrBuilder getBatchOutputOrBuilder(
        int index) {
      return batchOutput_.get(index);
    }
    private void ensureBatchOutputIsMutable() {
      com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.BatchOutput> tmp = batchOutput_;
      if (!tmp.isModifiable()) {
        batchOutput_ =
            com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
       }
    }

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model that requires special handling
     *&#64;&#64;     by the model backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchOutput batch_output = 21;</code>
     */
    private void setBatchOutput(
        int index, inference.ModelConfigOuterClass.BatchOutput value) {
      value.getClass();
  ensureBatchOutputIsMutable();
      batchOutput_.set(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model that requires special handling
     *&#64;&#64;     by the model backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchOutput batch_output = 21;</code>
     */
    private void addBatchOutput(inference.ModelConfigOuterClass.BatchOutput value) {
      value.getClass();
  ensureBatchOutputIsMutable();
      batchOutput_.add(value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model that requires special handling
     *&#64;&#64;     by the model backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchOutput batch_output = 21;</code>
     */
    private void addBatchOutput(
        int index, inference.ModelConfigOuterClass.BatchOutput value) {
      value.getClass();
  ensureBatchOutputIsMutable();
      batchOutput_.add(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model that requires special handling
     *&#64;&#64;     by the model backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchOutput batch_output = 21;</code>
     */
    private void addAllBatchOutput(
        java.lang.Iterable<? extends inference.ModelConfigOuterClass.BatchOutput> values) {
      ensureBatchOutputIsMutable();
      com.google.protobuf.AbstractMessageLite.addAll(
          values, batchOutput_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model that requires special handling
     *&#64;&#64;     by the model backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchOutput batch_output = 21;</code>
     */
    private void clearBatchOutput() {
      batchOutput_ = emptyProtobufList();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model that requires special handling
     *&#64;&#64;     by the model backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchOutput batch_output = 21;</code>
     */
    private void removeBatchOutput(int index) {
      ensureBatchOutputIsMutable();
      batchOutput_.remove(index);
    }

    public static final int OPTIMIZATION_FIELD_NUMBER = 12;
    private inference.ModelConfigOuterClass.ModelOptimizationPolicy optimization_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
     *&#64;&#64;
     *&#64;&#64;     Optimization configuration for the model. If not specified
     *&#64;&#64;     then default optimization policy is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
     */
    @java.lang.Override
    public boolean hasOptimization() {
      return optimization_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
     *&#64;&#64;
     *&#64;&#64;     Optimization configuration for the model. If not specified
     *&#64;&#64;     then default optimization policy is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOptimizationPolicy getOptimization() {
      return optimization_ == null ? inference.ModelConfigOuterClass.ModelOptimizationPolicy.getDefaultInstance() : optimization_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
     *&#64;&#64;
     *&#64;&#64;     Optimization configuration for the model. If not specified
     *&#64;&#64;     then default optimization policy is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
     */
    private void setOptimization(inference.ModelConfigOuterClass.ModelOptimizationPolicy value) {
      value.getClass();
  optimization_ = value;
      
      }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
     *&#64;&#64;
     *&#64;&#64;     Optimization configuration for the model. If not specified
     *&#64;&#64;     then default optimization policy is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
     */
    @java.lang.SuppressWarnings({"ReferenceEquality"})
    private void mergeOptimization(inference.ModelConfigOuterClass.ModelOptimizationPolicy value) {
      value.getClass();
  if (optimization_ != null &&
          optimization_ != inference.ModelConfigOuterClass.ModelOptimizationPolicy.getDefaultInstance()) {
        optimization_ =
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.newBuilder(optimization_).mergeFrom(value).buildPartial();
      } else {
        optimization_ = value;
      }
      
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
     *&#64;&#64;
     *&#64;&#64;     Optimization configuration for the model. If not specified
     *&#64;&#64;     then default optimization policy is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
     */
    private void clearOptimization() {  optimization_ = null;
      
    }

    public static final int DYNAMIC_BATCHING_FIELD_NUMBER = 11;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the dynamic-batching scheduling
     *&#64;&#64;       policy. With dynamic-batching the scheduler may group
     *&#64;&#64;       together independent requests into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
     */
    @java.lang.Override
    public boolean hasDynamicBatching() {
      return schedulingChoiceCase_ == 11;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the dynamic-batching scheduling
     *&#64;&#64;       policy. With dynamic-batching the scheduler may group
     *&#64;&#64;       together independent requests into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelDynamicBatching getDynamicBatching() {
      if (schedulingChoiceCase_ == 11) {
         return (inference.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_;
      }
      return inference.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the dynamic-batching scheduling
     *&#64;&#64;       policy. With dynamic-batching the scheduler may group
     *&#64;&#64;       together independent requests into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
     */
    private void setDynamicBatching(inference.ModelConfigOuterClass.ModelDynamicBatching value) {
      value.getClass();
  schedulingChoice_ = value;
      schedulingChoiceCase_ = 11;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the dynamic-batching scheduling
     *&#64;&#64;       policy. With dynamic-batching the scheduler may group
     *&#64;&#64;       together independent requests into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
     */
    private void mergeDynamicBatching(inference.ModelConfigOuterClass.ModelDynamicBatching value) {
      value.getClass();
  if (schedulingChoiceCase_ == 11 &&
          schedulingChoice_ != inference.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance()) {
        schedulingChoice_ = inference.ModelConfigOuterClass.ModelDynamicBatching.newBuilder((inference.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_)
            .mergeFrom(value).buildPartial();
      } else {
        schedulingChoice_ = value;
      }
      schedulingChoiceCase_ = 11;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the dynamic-batching scheduling
     *&#64;&#64;       policy. With dynamic-batching the scheduler may group
     *&#64;&#64;       together independent requests into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
     */
    private void clearDynamicBatching() {
      if (schedulingChoiceCase_ == 11) {
        schedulingChoiceCase_ = 0;
        schedulingChoice_ = null;
      }
    }

    public static final int SEQUENCE_BATCHING_FIELD_NUMBER = 13;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the sequence-batching scheduling
     *&#64;&#64;       policy. With sequence-batching, inference requests
     *&#64;&#64;       with the same correlation ID are routed to the same
     *&#64;&#64;       model instance. Multiple sequences of inference requests
     *&#64;&#64;       may be batched together into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
     */
    @java.lang.Override
    public boolean hasSequenceBatching() {
      return schedulingChoiceCase_ == 13;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the sequence-batching scheduling
     *&#64;&#64;       policy. With sequence-batching, inference requests
     *&#64;&#64;       with the same correlation ID are routed to the same
     *&#64;&#64;       model instance. Multiple sequences of inference requests
     *&#64;&#64;       may be batched together into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelSequenceBatching getSequenceBatching() {
      if (schedulingChoiceCase_ == 13) {
         return (inference.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_;
      }
      return inference.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the sequence-batching scheduling
     *&#64;&#64;       policy. With sequence-batching, inference requests
     *&#64;&#64;       with the same correlation ID are routed to the same
     *&#64;&#64;       model instance. Multiple sequences of inference requests
     *&#64;&#64;       may be batched together into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
     */
    private void setSequenceBatching(inference.ModelConfigOuterClass.ModelSequenceBatching value) {
      value.getClass();
  schedulingChoice_ = value;
      schedulingChoiceCase_ = 13;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the sequence-batching scheduling
     *&#64;&#64;       policy. With sequence-batching, inference requests
     *&#64;&#64;       with the same correlation ID are routed to the same
     *&#64;&#64;       model instance. Multiple sequences of inference requests
     *&#64;&#64;       may be batched together into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
     */
    private void mergeSequenceBatching(inference.ModelConfigOuterClass.ModelSequenceBatching value) {
      value.getClass();
  if (schedulingChoiceCase_ == 13 &&
          schedulingChoice_ != inference.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance()) {
        schedulingChoice_ = inference.ModelConfigOuterClass.ModelSequenceBatching.newBuilder((inference.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_)
            .mergeFrom(value).buildPartial();
      } else {
        schedulingChoice_ = value;
      }
      schedulingChoiceCase_ = 13;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the sequence-batching scheduling
     *&#64;&#64;       policy. With sequence-batching, inference requests
     *&#64;&#64;       with the same correlation ID are routed to the same
     *&#64;&#64;       model instance. Multiple sequences of inference requests
     *&#64;&#64;       may be batched together into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
     */
    private void clearSequenceBatching() {
      if (schedulingChoiceCase_ == 13) {
        schedulingChoiceCase_ = 0;
        schedulingChoice_ = null;
      }
    }

    public static final int ENSEMBLE_SCHEDULING_FIELD_NUMBER = 15;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the model-ensembling scheduling
     *&#64;&#64;       policy. With model-ensembling, inference requests
     *&#64;&#64;       will be processed according to the specification, such as an
     *&#64;&#64;       execution sequence of models. The input specified in this model
     *&#64;&#64;       config will be the input for the ensemble, and the output
     *&#64;&#64;       specified will be the output of the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
     */
    @java.lang.Override
    public boolean hasEnsembleScheduling() {
      return schedulingChoiceCase_ == 15;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the model-ensembling scheduling
     *&#64;&#64;       policy. With model-ensembling, inference requests
     *&#64;&#64;       will be processed according to the specification, such as an
     *&#64;&#64;       execution sequence of models. The input specified in this model
     *&#64;&#64;       config will be the input for the ensemble, and the output
     *&#64;&#64;       specified will be the output of the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelEnsembling getEnsembleScheduling() {
      if (schedulingChoiceCase_ == 15) {
         return (inference.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_;
      }
      return inference.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the model-ensembling scheduling
     *&#64;&#64;       policy. With model-ensembling, inference requests
     *&#64;&#64;       will be processed according to the specification, such as an
     *&#64;&#64;       execution sequence of models. The input specified in this model
     *&#64;&#64;       config will be the input for the ensemble, and the output
     *&#64;&#64;       specified will be the output of the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
     */
    private void setEnsembleScheduling(inference.ModelConfigOuterClass.ModelEnsembling value) {
      value.getClass();
  schedulingChoice_ = value;
      schedulingChoiceCase_ = 15;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the model-ensembling scheduling
     *&#64;&#64;       policy. With model-ensembling, inference requests
     *&#64;&#64;       will be processed according to the specification, such as an
     *&#64;&#64;       execution sequence of models. The input specified in this model
     *&#64;&#64;       config will be the input for the ensemble, and the output
     *&#64;&#64;       specified will be the output of the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
     */
    private void mergeEnsembleScheduling(inference.ModelConfigOuterClass.ModelEnsembling value) {
      value.getClass();
  if (schedulingChoiceCase_ == 15 &&
          schedulingChoice_ != inference.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance()) {
        schedulingChoice_ = inference.ModelConfigOuterClass.ModelEnsembling.newBuilder((inference.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_)
            .mergeFrom(value).buildPartial();
      } else {
        schedulingChoice_ = value;
      }
      schedulingChoiceCase_ = 15;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the model-ensembling scheduling
     *&#64;&#64;       policy. With model-ensembling, inference requests
     *&#64;&#64;       will be processed according to the specification, such as an
     *&#64;&#64;       execution sequence of models. The input specified in this model
     *&#64;&#64;       config will be the input for the ensemble, and the output
     *&#64;&#64;       specified will be the output of the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
     */
    private void clearEnsembleScheduling() {
      if (schedulingChoiceCase_ == 15) {
        schedulingChoiceCase_ = 0;
        schedulingChoice_ = null;
      }
    }

    public static final int INSTANCE_GROUP_FIELD_NUMBER = 7;
    private com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.ModelInstanceGroup> instanceGroup_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
     */
    @java.lang.Override
    public java.util.List<inference.ModelConfigOuterClass.ModelInstanceGroup> getInstanceGroupList() {
      return instanceGroup_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
     */
    public java.util.List<? extends inference.ModelConfigOuterClass.ModelInstanceGroupOrBuilder> 
        getInstanceGroupOrBuilderList() {
      return instanceGroup_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
     */
    @java.lang.Override
    public int getInstanceGroupCount() {
      return instanceGroup_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelInstanceGroup getInstanceGroup(int index) {
      return instanceGroup_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
     */
    public inference.ModelConfigOuterClass.ModelInstanceGroupOrBuilder getInstanceGroupOrBuilder(
        int index) {
      return instanceGroup_.get(index);
    }
    private void ensureInstanceGroupIsMutable() {
      com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.ModelInstanceGroup> tmp = instanceGroup_;
      if (!tmp.isModifiable()) {
        instanceGroup_ =
            com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
       }
    }

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
     */
    private void setInstanceGroup(
        int index, inference.ModelConfigOuterClass.ModelInstanceGroup value) {
      value.getClass();
  ensureInstanceGroupIsMutable();
      instanceGroup_.set(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
     */
    private void addInstanceGroup(inference.ModelConfigOuterClass.ModelInstanceGroup value) {
      value.getClass();
  ensureInstanceGroupIsMutable();
      instanceGroup_.add(value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
     */
    private void addInstanceGroup(
        int index, inference.ModelConfigOuterClass.ModelInstanceGroup value) {
      value.getClass();
  ensureInstanceGroupIsMutable();
      instanceGroup_.add(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
     */
    private void addAllInstanceGroup(
        java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelInstanceGroup> values) {
      ensureInstanceGroupIsMutable();
      com.google.protobuf.AbstractMessageLite.addAll(
          values, instanceGroup_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
     */
    private void clearInstanceGroup() {
      instanceGroup_ = emptyProtobufList();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
     */
    private void removeInstanceGroup(int index) {
      ensureInstanceGroupIsMutable();
      instanceGroup_.remove(index);
    }

    public static final int DEFAULT_MODEL_FILENAME_FIELD_NUMBER = 8;
    private java.lang.String defaultModelFilename_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string default_model_filename
     *&#64;&#64;
     *&#64;&#64;     Optional filename of the model file to use if a
     *&#64;&#64;     compute-capability specific model is not specified in
     *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
     *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
     *&#64;&#64;     'model.pt' depending on the model type.
     *&#64;&#64;
     * </pre>
     *
     * <code>string default_model_filename = 8;</code>
     * @return The defaultModelFilename.
     */
    @java.lang.Override
    public java.lang.String getDefaultModelFilename() {
      return defaultModelFilename_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string default_model_filename
     *&#64;&#64;
     *&#64;&#64;     Optional filename of the model file to use if a
     *&#64;&#64;     compute-capability specific model is not specified in
     *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
     *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
     *&#64;&#64;     'model.pt' depending on the model type.
     *&#64;&#64;
     * </pre>
     *
     * <code>string default_model_filename = 8;</code>
     * @return The bytes for defaultModelFilename.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getDefaultModelFilenameBytes() {
      return com.google.protobuf.ByteString.copyFromUtf8(defaultModelFilename_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string default_model_filename
     *&#64;&#64;
     *&#64;&#64;     Optional filename of the model file to use if a
     *&#64;&#64;     compute-capability specific model is not specified in
     *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
     *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
     *&#64;&#64;     'model.pt' depending on the model type.
     *&#64;&#64;
     * </pre>
     *
     * <code>string default_model_filename = 8;</code>
     * @param value The defaultModelFilename to set.
     */
    private void setDefaultModelFilename(
        java.lang.String value) {
      java.lang.Class<?> valueClass = value.getClass();
  
      defaultModelFilename_ = value;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string default_model_filename
     *&#64;&#64;
     *&#64;&#64;     Optional filename of the model file to use if a
     *&#64;&#64;     compute-capability specific model is not specified in
     *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
     *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
     *&#64;&#64;     'model.pt' depending on the model type.
     *&#64;&#64;
     * </pre>
     *
     * <code>string default_model_filename = 8;</code>
     */
    private void clearDefaultModelFilename() {
      
      defaultModelFilename_ = getDefaultInstance().getDefaultModelFilename();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string default_model_filename
     *&#64;&#64;
     *&#64;&#64;     Optional filename of the model file to use if a
     *&#64;&#64;     compute-capability specific model is not specified in
     *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
     *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
     *&#64;&#64;     'model.pt' depending on the model type.
     *&#64;&#64;
     * </pre>
     *
     * <code>string default_model_filename = 8;</code>
     * @param value The bytes for defaultModelFilename to set.
     */
    private void setDefaultModelFilenameBytes(
        com.google.protobuf.ByteString value) {
      checkByteStringIsUtf8(value);
      defaultModelFilename_ = value.toStringUtf8();
      
    }

    public static final int CC_MODEL_FILENAMES_FIELD_NUMBER = 9;
    private static final class CcModelFilenamesDefaultEntryHolder {
      static final com.google.protobuf.MapEntryLite<
          java.lang.String, java.lang.String> defaultEntry =
              com.google.protobuf.MapEntryLite
              .<java.lang.String, java.lang.String>newDefaultInstance(
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "");
    }
    private com.google.protobuf.MapFieldLite<
        java.lang.String, java.lang.String> ccModelFilenames_ =
            com.google.protobuf.MapFieldLite.emptyMapField();
    private com.google.protobuf.MapFieldLite<java.lang.String, java.lang.String>
    internalGetCcModelFilenames() {
      return ccModelFilenames_;
    }
    private com.google.protobuf.MapFieldLite<java.lang.String, java.lang.String>
    internalGetMutableCcModelFilenames() {
      if (!ccModelFilenames_.isMutable()) {
        ccModelFilenames_ = ccModelFilenames_.mutableCopy();
      }
      return ccModelFilenames_;
    }
    @java.lang.Override

    public int getCcModelFilenamesCount() {
      return internalGetCcModelFilenames().size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */
    @java.lang.Override

    public boolean containsCcModelFilenames(
        java.lang.String key) {
      java.lang.Class<?> keyClass = key.getClass();
      return internalGetCcModelFilenames().containsKey(key);
    }
    /**
     * Use {@link #getCcModelFilenamesMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.String> getCcModelFilenames() {
      return getCcModelFilenamesMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, java.lang.String> getCcModelFilenamesMap() {
      return java.util.Collections.unmodifiableMap(
          internalGetCcModelFilenames());
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */
    @java.lang.Override

    public java.lang.String getCcModelFilenamesOrDefault(
        java.lang.String key,
        java.lang.String defaultValue) {
      java.lang.Class<?> keyClass = key.getClass();
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetCcModelFilenames();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */
    @java.lang.Override

    public java.lang.String getCcModelFilenamesOrThrow(
        java.lang.String key) {
      java.lang.Class<?> keyClass = key.getClass();
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetCcModelFilenames();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */
    private java.util.Map<java.lang.String, java.lang.String>
    getMutableCcModelFilenamesMap() {
      return internalGetMutableCcModelFilenames();
    }

    public static final int METRIC_TAGS_FIELD_NUMBER = 10;
    private static final class MetricTagsDefaultEntryHolder {
      static final com.google.protobuf.MapEntryLite<
          java.lang.String, java.lang.String> defaultEntry =
              com.google.protobuf.MapEntryLite
              .<java.lang.String, java.lang.String>newDefaultInstance(
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "");
    }
    private com.google.protobuf.MapFieldLite<
        java.lang.String, java.lang.String> metricTags_ =
            com.google.protobuf.MapFieldLite.emptyMapField();
    private com.google.protobuf.MapFieldLite<java.lang.String, java.lang.String>
    internalGetMetricTags() {
      return metricTags_;
    }
    private com.google.protobuf.MapFieldLite<java.lang.String, java.lang.String>
    internalGetMutableMetricTags() {
      if (!metricTags_.isMutable()) {
        metricTags_ = metricTags_.mutableCopy();
      }
      return metricTags_;
    }
    @java.lang.Override

    public int getMetricTagsCount() {
      return internalGetMetricTags().size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */
    @java.lang.Override

    public boolean containsMetricTags(
        java.lang.String key) {
      java.lang.Class<?> keyClass = key.getClass();
      return internalGetMetricTags().containsKey(key);
    }
    /**
     * Use {@link #getMetricTagsMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.String> getMetricTags() {
      return getMetricTagsMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, java.lang.String> getMetricTagsMap() {
      return java.util.Collections.unmodifiableMap(
          internalGetMetricTags());
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */
    @java.lang.Override

    public java.lang.String getMetricTagsOrDefault(
        java.lang.String key,
        java.lang.String defaultValue) {
      java.lang.Class<?> keyClass = key.getClass();
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetMetricTags();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */
    @java.lang.Override

    public java.lang.String getMetricTagsOrThrow(
        java.lang.String key) {
      java.lang.Class<?> keyClass = key.getClass();
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetMetricTags();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */
    private java.util.Map<java.lang.String, java.lang.String>
    getMutableMetricTagsMap() {
      return internalGetMutableMetricTags();
    }

    public static final int PARAMETERS_FIELD_NUMBER = 14;
    private static final class ParametersDefaultEntryHolder {
      static final com.google.protobuf.MapEntryLite<
          java.lang.String, inference.ModelConfigOuterClass.ModelParameter> defaultEntry =
              com.google.protobuf.MapEntryLite
              .<java.lang.String, inference.ModelConfigOuterClass.ModelParameter>newDefaultInstance(
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.MESSAGE,
                  inference.ModelConfigOuterClass.ModelParameter.getDefaultInstance());
    }
    private com.google.protobuf.MapFieldLite<
        java.lang.String, inference.ModelConfigOuterClass.ModelParameter> parameters_ =
            com.google.protobuf.MapFieldLite.emptyMapField();
    private com.google.protobuf.MapFieldLite<java.lang.String, inference.ModelConfigOuterClass.ModelParameter>
    internalGetParameters() {
      return parameters_;
    }
    private com.google.protobuf.MapFieldLite<java.lang.String, inference.ModelConfigOuterClass.ModelParameter>
    internalGetMutableParameters() {
      if (!parameters_.isMutable()) {
        parameters_ = parameters_.mutableCopy();
      }
      return parameters_;
    }
    @java.lang.Override

    public int getParametersCount() {
      return internalGetParameters().size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
     */
    @java.lang.Override

    public boolean containsParameters(
        java.lang.String key) {
      java.lang.Class<?> keyClass = key.getClass();
      return internalGetParameters().containsKey(key);
    }
    /**
     * Use {@link #getParametersMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelParameter> getParameters() {
      return getParametersMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelParameter> getParametersMap() {
      return java.util.Collections.unmodifiableMap(
          internalGetParameters());
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
     */
    @java.lang.Override

    public inference.ModelConfigOuterClass.ModelParameter getParametersOrDefault(
        java.lang.String key,
        inference.ModelConfigOuterClass.ModelParameter defaultValue) {
      java.lang.Class<?> keyClass = key.getClass();
      java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelParameter> map =
          internalGetParameters();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
     */
    @java.lang.Override

    public inference.ModelConfigOuterClass.ModelParameter getParametersOrThrow(
        java.lang.String key) {
      java.lang.Class<?> keyClass = key.getClass();
      java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelParameter> map =
          internalGetParameters();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
     */
    private java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelParameter>
    getMutableParametersMap() {
      return internalGetMutableParameters();
    }

    public static final int MODEL_WARMUP_FIELD_NUMBER = 16;
    private com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.ModelWarmup> modelWarmup_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
     */
    @java.lang.Override
    public java.util.List<inference.ModelConfigOuterClass.ModelWarmup> getModelWarmupList() {
      return modelWarmup_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
     */
    public java.util.List<? extends inference.ModelConfigOuterClass.ModelWarmupOrBuilder> 
        getModelWarmupOrBuilderList() {
      return modelWarmup_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
     */
    @java.lang.Override
    public int getModelWarmupCount() {
      return modelWarmup_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelWarmup getModelWarmup(int index) {
      return modelWarmup_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
     */
    public inference.ModelConfigOuterClass.ModelWarmupOrBuilder getModelWarmupOrBuilder(
        int index) {
      return modelWarmup_.get(index);
    }
    private void ensureModelWarmupIsMutable() {
      com.google.protobuf.Internal.ProtobufList<inference.ModelConfigOuterClass.ModelWarmup> tmp = modelWarmup_;
      if (!tmp.isModifiable()) {
        modelWarmup_ =
            com.google.protobuf.GeneratedMessageLite.mutableCopy(tmp);
       }
    }

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
     */
    private void setModelWarmup(
        int index, inference.ModelConfigOuterClass.ModelWarmup value) {
      value.getClass();
  ensureModelWarmupIsMutable();
      modelWarmup_.set(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
     */
    private void addModelWarmup(inference.ModelConfigOuterClass.ModelWarmup value) {
      value.getClass();
  ensureModelWarmupIsMutable();
      modelWarmup_.add(value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
     */
    private void addModelWarmup(
        int index, inference.ModelConfigOuterClass.ModelWarmup value) {
      value.getClass();
  ensureModelWarmupIsMutable();
      modelWarmup_.add(index, value);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
     */
    private void addAllModelWarmup(
        java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelWarmup> values) {
      ensureModelWarmupIsMutable();
      com.google.protobuf.AbstractMessageLite.addAll(
          values, modelWarmup_);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
     */
    private void clearModelWarmup() {
      modelWarmup_ = emptyProtobufList();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
     */
    private void removeModelWarmup(int index) {
      ensureModelWarmupIsMutable();
      modelWarmup_.remove(index);
    }

    public static final int MODEL_OPERATIONS_FIELD_NUMBER = 18;
    private inference.ModelConfigOuterClass.ModelOperations modelOperations_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
     *&#64;&#64;
     *&#64;&#64;     Optional metadata of the libraries providing custom operations for
     *&#64;&#64;     this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOperations model_operations = 18;</code>
     */
    @java.lang.Override
    public boolean hasModelOperations() {
      return modelOperations_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
     *&#64;&#64;
     *&#64;&#64;     Optional metadata of the libraries providing custom operations for
     *&#64;&#64;     this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOperations model_operations = 18;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOperations getModelOperations() {
      return modelOperations_ == null ? inference.ModelConfigOuterClass.ModelOperations.getDefaultInstance() : modelOperations_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
     *&#64;&#64;
     *&#64;&#64;     Optional metadata of the libraries providing custom operations for
     *&#64;&#64;     this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOperations model_operations = 18;</code>
     */
    private void setModelOperations(inference.ModelConfigOuterClass.ModelOperations value) {
      value.getClass();
  modelOperations_ = value;
      
      }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
     *&#64;&#64;
     *&#64;&#64;     Optional metadata of the libraries providing custom operations for
     *&#64;&#64;     this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOperations model_operations = 18;</code>
     */
    @java.lang.SuppressWarnings({"ReferenceEquality"})
    private void mergeModelOperations(inference.ModelConfigOuterClass.ModelOperations value) {
      value.getClass();
  if (modelOperations_ != null &&
          modelOperations_ != inference.ModelConfigOuterClass.ModelOperations.getDefaultInstance()) {
        modelOperations_ =
          inference.ModelConfigOuterClass.ModelOperations.newBuilder(modelOperations_).mergeFrom(value).buildPartial();
      } else {
        modelOperations_ = value;
      }
      
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
     *&#64;&#64;
     *&#64;&#64;     Optional metadata of the libraries providing custom operations for
     *&#64;&#64;     this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOperations model_operations = 18;</code>
     */
    private void clearModelOperations() {  modelOperations_ = null;
      
    }

    public static final int MODEL_TRANSACTION_POLICY_FIELD_NUMBER = 19;
    private inference.ModelConfigOuterClass.ModelTransactionPolicy modelTransactionPolicy_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
     *&#64;&#64;
     *&#64;&#64;     Optional specification that describes the nature of transactions
     *&#64;&#64;     to be expected from the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
     */
    @java.lang.Override
    public boolean hasModelTransactionPolicy() {
      return modelTransactionPolicy_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
     *&#64;&#64;
     *&#64;&#64;     Optional specification that describes the nature of transactions
     *&#64;&#64;     to be expected from the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelTransactionPolicy getModelTransactionPolicy() {
      return modelTransactionPolicy_ == null ? inference.ModelConfigOuterClass.ModelTransactionPolicy.getDefaultInstance() : modelTransactionPolicy_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
     *&#64;&#64;
     *&#64;&#64;     Optional specification that describes the nature of transactions
     *&#64;&#64;     to be expected from the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
     */
    private void setModelTransactionPolicy(inference.ModelConfigOuterClass.ModelTransactionPolicy value) {
      value.getClass();
  modelTransactionPolicy_ = value;
      
      }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
     *&#64;&#64;
     *&#64;&#64;     Optional specification that describes the nature of transactions
     *&#64;&#64;     to be expected from the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
     */
    @java.lang.SuppressWarnings({"ReferenceEquality"})
    private void mergeModelTransactionPolicy(inference.ModelConfigOuterClass.ModelTransactionPolicy value) {
      value.getClass();
  if (modelTransactionPolicy_ != null &&
          modelTransactionPolicy_ != inference.ModelConfigOuterClass.ModelTransactionPolicy.getDefaultInstance()) {
        modelTransactionPolicy_ =
          inference.ModelConfigOuterClass.ModelTransactionPolicy.newBuilder(modelTransactionPolicy_).mergeFrom(value).buildPartial();
      } else {
        modelTransactionPolicy_ = value;
      }
      
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
     *&#64;&#64;
     *&#64;&#64;     Optional specification that describes the nature of transactions
     *&#64;&#64;     to be expected from the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
     */
    private void clearModelTransactionPolicy() {  modelTransactionPolicy_ = null;
      
    }

    public static final int MODEL_REPOSITORY_AGENTS_FIELD_NUMBER = 23;
    private inference.ModelConfigOuterClass.ModelRepositoryAgents modelRepositoryAgents_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelRepositoryAgents model_repository_agents
     *&#64;&#64;
     *&#64;&#64;     Optional specification of the agent(s) that should be invoked
     *&#64;&#64;     with repository actions are performed for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelRepositoryAgents model_repository_agents = 23;</code>
     */
    @java.lang.Override
    public boolean hasModelRepositoryAgents() {
      return modelRepositoryAgents_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelRepositoryAgents model_repository_agents
     *&#64;&#64;
     *&#64;&#64;     Optional specification of the agent(s) that should be invoked
     *&#64;&#64;     with repository actions are performed for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelRepositoryAgents model_repository_agents = 23;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelRepositoryAgents getModelRepositoryAgents() {
      return modelRepositoryAgents_ == null ? inference.ModelConfigOuterClass.ModelRepositoryAgents.getDefaultInstance() : modelRepositoryAgents_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelRepositoryAgents model_repository_agents
     *&#64;&#64;
     *&#64;&#64;     Optional specification of the agent(s) that should be invoked
     *&#64;&#64;     with repository actions are performed for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelRepositoryAgents model_repository_agents = 23;</code>
     */
    private void setModelRepositoryAgents(inference.ModelConfigOuterClass.ModelRepositoryAgents value) {
      value.getClass();
  modelRepositoryAgents_ = value;
      
      }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelRepositoryAgents model_repository_agents
     *&#64;&#64;
     *&#64;&#64;     Optional specification of the agent(s) that should be invoked
     *&#64;&#64;     with repository actions are performed for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelRepositoryAgents model_repository_agents = 23;</code>
     */
    @java.lang.SuppressWarnings({"ReferenceEquality"})
    private void mergeModelRepositoryAgents(inference.ModelConfigOuterClass.ModelRepositoryAgents value) {
      value.getClass();
  if (modelRepositoryAgents_ != null &&
          modelRepositoryAgents_ != inference.ModelConfigOuterClass.ModelRepositoryAgents.getDefaultInstance()) {
        modelRepositoryAgents_ =
          inference.ModelConfigOuterClass.ModelRepositoryAgents.newBuilder(modelRepositoryAgents_).mergeFrom(value).buildPartial();
      } else {
        modelRepositoryAgents_ = value;
      }
      
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelRepositoryAgents model_repository_agents
     *&#64;&#64;
     *&#64;&#64;     Optional specification of the agent(s) that should be invoked
     *&#64;&#64;     with repository actions are performed for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelRepositoryAgents model_repository_agents = 23;</code>
     */
    private void clearModelRepositoryAgents() {  modelRepositoryAgents_ = null;
      
    }

    public static inference.ModelConfigOuterClass.ModelConfig parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelConfig parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelConfig parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelConfig parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelConfig parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data);
    }
    public static inference.ModelConfigOuterClass.ModelConfig parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelConfig parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelConfig parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelConfig parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelConfig parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelConfig parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input);
    }
    public static inference.ModelConfigOuterClass.ModelConfig parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageLite.parseFrom(
          DEFAULT_INSTANCE, input, extensionRegistry);
    }

    public static Builder newBuilder() {
      return (Builder) DEFAULT_INSTANCE.createBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelConfig prototype) {
      return (Builder) DEFAULT_INSTANCE.createBuilder(prototype);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelConfig
     *&#64;&#64;
     *&#64;&#64;   A model configuration.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelConfig}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageLite.Builder<
          inference.ModelConfigOuterClass.ModelConfig, Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelConfig)
        inference.ModelConfigOuterClass.ModelConfigOrBuilder {
      // Construct using inference.ModelConfigOuterClass.ModelConfig.newBuilder()
      private Builder() {
        super(DEFAULT_INSTANCE);
      }

      @java.lang.Override
      public SchedulingChoiceCase
          getSchedulingChoiceCase() {
        return instance.getSchedulingChoiceCase();
      }

      public Builder clearSchedulingChoice() {
        copyOnWrite();
        instance.clearSchedulingChoice();
        return this;
      }


      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The name.
       */
      @java.lang.Override
      public java.lang.String getName() {
        return instance.getName();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString
          getNameBytes() {
        return instance.getNameBytes();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        copyOnWrite();
        instance.setName(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        copyOnWrite();
        instance.clearName();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        copyOnWrite();
        instance.setNameBytes(value);
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string platform
       *&#64;&#64;
       *&#64;&#64;     The framework for the model. Possible values are
       *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
       *&#64;&#64;     "tensorflow_savedmodel", "onnxruntime_onnx",
       *&#64;&#64;     "pytorch_libtorch" and "custom".
       *&#64;&#64;
       * </pre>
       *
       * <code>string platform = 2;</code>
       * @return The platform.
       */
      @java.lang.Override
      public java.lang.String getPlatform() {
        return instance.getPlatform();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string platform
       *&#64;&#64;
       *&#64;&#64;     The framework for the model. Possible values are
       *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
       *&#64;&#64;     "tensorflow_savedmodel", "onnxruntime_onnx",
       *&#64;&#64;     "pytorch_libtorch" and "custom".
       *&#64;&#64;
       * </pre>
       *
       * <code>string platform = 2;</code>
       * @return The bytes for platform.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString
          getPlatformBytes() {
        return instance.getPlatformBytes();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string platform
       *&#64;&#64;
       *&#64;&#64;     The framework for the model. Possible values are
       *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
       *&#64;&#64;     "tensorflow_savedmodel", "onnxruntime_onnx",
       *&#64;&#64;     "pytorch_libtorch" and "custom".
       *&#64;&#64;
       * </pre>
       *
       * <code>string platform = 2;</code>
       * @param value The platform to set.
       * @return This builder for chaining.
       */
      public Builder setPlatform(
          java.lang.String value) {
        copyOnWrite();
        instance.setPlatform(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string platform
       *&#64;&#64;
       *&#64;&#64;     The framework for the model. Possible values are
       *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
       *&#64;&#64;     "tensorflow_savedmodel", "onnxruntime_onnx",
       *&#64;&#64;     "pytorch_libtorch" and "custom".
       *&#64;&#64;
       * </pre>
       *
       * <code>string platform = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearPlatform() {
        copyOnWrite();
        instance.clearPlatform();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string platform
       *&#64;&#64;
       *&#64;&#64;     The framework for the model. Possible values are
       *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
       *&#64;&#64;     "tensorflow_savedmodel", "onnxruntime_onnx",
       *&#64;&#64;     "pytorch_libtorch" and "custom".
       *&#64;&#64;
       * </pre>
       *
       * <code>string platform = 2;</code>
       * @param value The bytes for platform to set.
       * @return This builder for chaining.
       */
      public Builder setPlatformBytes(
          com.google.protobuf.ByteString value) {
        copyOnWrite();
        instance.setPlatformBytes(value);
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string backend
       *&#64;&#64;
       *&#64;&#64;     The backend used by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string backend = 17;</code>
       * @return The backend.
       */
      @java.lang.Override
      public java.lang.String getBackend() {
        return instance.getBackend();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string backend
       *&#64;&#64;
       *&#64;&#64;     The backend used by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string backend = 17;</code>
       * @return The bytes for backend.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString
          getBackendBytes() {
        return instance.getBackendBytes();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string backend
       *&#64;&#64;
       *&#64;&#64;     The backend used by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string backend = 17;</code>
       * @param value The backend to set.
       * @return This builder for chaining.
       */
      public Builder setBackend(
          java.lang.String value) {
        copyOnWrite();
        instance.setBackend(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string backend
       *&#64;&#64;
       *&#64;&#64;     The backend used by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string backend = 17;</code>
       * @return This builder for chaining.
       */
      public Builder clearBackend() {
        copyOnWrite();
        instance.clearBackend();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string backend
       *&#64;&#64;
       *&#64;&#64;     The backend used by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string backend = 17;</code>
       * @param value The bytes for backend to set.
       * @return This builder for chaining.
       */
      public Builder setBackendBytes(
          com.google.protobuf.ByteString value) {
        copyOnWrite();
        instance.setBackendBytes(value);
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
       */
      @java.lang.Override
      public boolean hasVersionPolicy() {
        return instance.hasVersionPolicy();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelVersionPolicy getVersionPolicy() {
        return instance.getVersionPolicy();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
       */
      public Builder setVersionPolicy(inference.ModelConfigOuterClass.ModelVersionPolicy value) {
        copyOnWrite();
        instance.setVersionPolicy(value);
        return this;
        }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
       */
      public Builder setVersionPolicy(
          inference.ModelConfigOuterClass.ModelVersionPolicy.Builder builderForValue) {
        copyOnWrite();
        instance.setVersionPolicy(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
       */
      public Builder mergeVersionPolicy(inference.ModelConfigOuterClass.ModelVersionPolicy value) {
        copyOnWrite();
        instance.mergeVersionPolicy(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
       */
      public Builder clearVersionPolicy() {  copyOnWrite();
        instance.clearVersionPolicy();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 max_batch_size
       *&#64;&#64;
       *&#64;&#64;     Maximum batch size allowed for inference. This can only decrease
       *&#64;&#64;     what is allowed by the model itself. A max_batch_size value of 0
       *&#64;&#64;     indicates that batching is not allowed for the model and the
       *&#64;&#64;     dimension/shape of the input and output tensors must exactly
       *&#64;&#64;     match what is specified in the input and output configuration. A
       *&#64;&#64;     max_batch_size value &gt; 0 indicates that batching is allowed and
       *&#64;&#64;     so the model expects the input tensors to have an additional
       *&#64;&#64;     initial dimension for the batching that is not specified in the
       *&#64;&#64;     input (for example, if the model supports batched inputs of
       *&#64;&#64;     2-dimensional tensors then the model configuration will specify
       *&#64;&#64;     the input shape as [ X, Y ] but the model will expect the actual
       *&#64;&#64;     input tensors to have shape [ N, X, Y ]). For max_batch_size &gt; 0
       *&#64;&#64;     returned outputs will also have an additional initial dimension
       *&#64;&#64;     for the batch.
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 max_batch_size = 4;</code>
       * @return The maxBatchSize.
       */
      @java.lang.Override
      public int getMaxBatchSize() {
        return instance.getMaxBatchSize();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 max_batch_size
       *&#64;&#64;
       *&#64;&#64;     Maximum batch size allowed for inference. This can only decrease
       *&#64;&#64;     what is allowed by the model itself. A max_batch_size value of 0
       *&#64;&#64;     indicates that batching is not allowed for the model and the
       *&#64;&#64;     dimension/shape of the input and output tensors must exactly
       *&#64;&#64;     match what is specified in the input and output configuration. A
       *&#64;&#64;     max_batch_size value &gt; 0 indicates that batching is allowed and
       *&#64;&#64;     so the model expects the input tensors to have an additional
       *&#64;&#64;     initial dimension for the batching that is not specified in the
       *&#64;&#64;     input (for example, if the model supports batched inputs of
       *&#64;&#64;     2-dimensional tensors then the model configuration will specify
       *&#64;&#64;     the input shape as [ X, Y ] but the model will expect the actual
       *&#64;&#64;     input tensors to have shape [ N, X, Y ]). For max_batch_size &gt; 0
       *&#64;&#64;     returned outputs will also have an additional initial dimension
       *&#64;&#64;     for the batch.
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 max_batch_size = 4;</code>
       * @param value The maxBatchSize to set.
       * @return This builder for chaining.
       */
      public Builder setMaxBatchSize(int value) {
        copyOnWrite();
        instance.setMaxBatchSize(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 max_batch_size
       *&#64;&#64;
       *&#64;&#64;     Maximum batch size allowed for inference. This can only decrease
       *&#64;&#64;     what is allowed by the model itself. A max_batch_size value of 0
       *&#64;&#64;     indicates that batching is not allowed for the model and the
       *&#64;&#64;     dimension/shape of the input and output tensors must exactly
       *&#64;&#64;     match what is specified in the input and output configuration. A
       *&#64;&#64;     max_batch_size value &gt; 0 indicates that batching is allowed and
       *&#64;&#64;     so the model expects the input tensors to have an additional
       *&#64;&#64;     initial dimension for the batching that is not specified in the
       *&#64;&#64;     input (for example, if the model supports batched inputs of
       *&#64;&#64;     2-dimensional tensors then the model configuration will specify
       *&#64;&#64;     the input shape as [ X, Y ] but the model will expect the actual
       *&#64;&#64;     input tensors to have shape [ N, X, Y ]). For max_batch_size &gt; 0
       *&#64;&#64;     returned outputs will also have an additional initial dimension
       *&#64;&#64;     for the batch.
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 max_batch_size = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearMaxBatchSize() {
        copyOnWrite();
        instance.clearMaxBatchSize();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      @java.lang.Override
      public java.util.List<inference.ModelConfigOuterClass.ModelInput> getInputList() {
        return java.util.Collections.unmodifiableList(
            instance.getInputList());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      @java.lang.Override
      public int getInputCount() {
        return instance.getInputCount();
      }/**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelInput getInput(int index) {
        return instance.getInput(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public Builder setInput(
          int index, inference.ModelConfigOuterClass.ModelInput value) {
        copyOnWrite();
        instance.setInput(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public Builder setInput(
          int index, inference.ModelConfigOuterClass.ModelInput.Builder builderForValue) {
        copyOnWrite();
        instance.setInput(index,
            builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public Builder addInput(inference.ModelConfigOuterClass.ModelInput value) {
        copyOnWrite();
        instance.addInput(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public Builder addInput(
          int index, inference.ModelConfigOuterClass.ModelInput value) {
        copyOnWrite();
        instance.addInput(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public Builder addInput(
          inference.ModelConfigOuterClass.ModelInput.Builder builderForValue) {
        copyOnWrite();
        instance.addInput(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public Builder addInput(
          int index, inference.ModelConfigOuterClass.ModelInput.Builder builderForValue) {
        copyOnWrite();
        instance.addInput(index,
            builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public Builder addAllInput(
          java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelInput> values) {
        copyOnWrite();
        instance.addAllInput(values);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public Builder clearInput() {
        copyOnWrite();
        instance.clearInput();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public Builder removeInput(int index) {
        copyOnWrite();
        instance.removeInput(index);
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      @java.lang.Override
      public java.util.List<inference.ModelConfigOuterClass.ModelOutput> getOutputList() {
        return java.util.Collections.unmodifiableList(
            instance.getOutputList());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      @java.lang.Override
      public int getOutputCount() {
        return instance.getOutputCount();
      }/**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOutput getOutput(int index) {
        return instance.getOutput(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public Builder setOutput(
          int index, inference.ModelConfigOuterClass.ModelOutput value) {
        copyOnWrite();
        instance.setOutput(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public Builder setOutput(
          int index, inference.ModelConfigOuterClass.ModelOutput.Builder builderForValue) {
        copyOnWrite();
        instance.setOutput(index,
            builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public Builder addOutput(inference.ModelConfigOuterClass.ModelOutput value) {
        copyOnWrite();
        instance.addOutput(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public Builder addOutput(
          int index, inference.ModelConfigOuterClass.ModelOutput value) {
        copyOnWrite();
        instance.addOutput(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public Builder addOutput(
          inference.ModelConfigOuterClass.ModelOutput.Builder builderForValue) {
        copyOnWrite();
        instance.addOutput(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public Builder addOutput(
          int index, inference.ModelConfigOuterClass.ModelOutput.Builder builderForValue) {
        copyOnWrite();
        instance.addOutput(index,
            builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public Builder addAllOutput(
          java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelOutput> values) {
        copyOnWrite();
        instance.addAllOutput(values);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public Builder clearOutput() {
        copyOnWrite();
        instance.clearOutput();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public Builder removeOutput(int index) {
        copyOnWrite();
        instance.removeOutput(index);
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      @java.lang.Override
      public java.util.List<inference.ModelConfigOuterClass.BatchInput> getBatchInputList() {
        return java.util.Collections.unmodifiableList(
            instance.getBatchInputList());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      @java.lang.Override
      public int getBatchInputCount() {
        return instance.getBatchInputCount();
      }/**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.BatchInput getBatchInput(int index) {
        return instance.getBatchInput(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public Builder setBatchInput(
          int index, inference.ModelConfigOuterClass.BatchInput value) {
        copyOnWrite();
        instance.setBatchInput(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public Builder setBatchInput(
          int index, inference.ModelConfigOuterClass.BatchInput.Builder builderForValue) {
        copyOnWrite();
        instance.setBatchInput(index,
            builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public Builder addBatchInput(inference.ModelConfigOuterClass.BatchInput value) {
        copyOnWrite();
        instance.addBatchInput(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public Builder addBatchInput(
          int index, inference.ModelConfigOuterClass.BatchInput value) {
        copyOnWrite();
        instance.addBatchInput(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public Builder addBatchInput(
          inference.ModelConfigOuterClass.BatchInput.Builder builderForValue) {
        copyOnWrite();
        instance.addBatchInput(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public Builder addBatchInput(
          int index, inference.ModelConfigOuterClass.BatchInput.Builder builderForValue) {
        copyOnWrite();
        instance.addBatchInput(index,
            builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public Builder addAllBatchInput(
          java.lang.Iterable<? extends inference.ModelConfigOuterClass.BatchInput> values) {
        copyOnWrite();
        instance.addAllBatchInput(values);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public Builder clearBatchInput() {
        copyOnWrite();
        instance.clearBatchInput();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public Builder removeBatchInput(int index) {
        copyOnWrite();
        instance.removeBatchInput(index);
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      @java.lang.Override
      public java.util.List<inference.ModelConfigOuterClass.BatchOutput> getBatchOutputList() {
        return java.util.Collections.unmodifiableList(
            instance.getBatchOutputList());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      @java.lang.Override
      public int getBatchOutputCount() {
        return instance.getBatchOutputCount();
      }/**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.BatchOutput getBatchOutput(int index) {
        return instance.getBatchOutput(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public Builder setBatchOutput(
          int index, inference.ModelConfigOuterClass.BatchOutput value) {
        copyOnWrite();
        instance.setBatchOutput(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public Builder setBatchOutput(
          int index, inference.ModelConfigOuterClass.BatchOutput.Builder builderForValue) {
        copyOnWrite();
        instance.setBatchOutput(index,
            builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public Builder addBatchOutput(inference.ModelConfigOuterClass.BatchOutput value) {
        copyOnWrite();
        instance.addBatchOutput(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public Builder addBatchOutput(
          int index, inference.ModelConfigOuterClass.BatchOutput value) {
        copyOnWrite();
        instance.addBatchOutput(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public Builder addBatchOutput(
          inference.ModelConfigOuterClass.BatchOutput.Builder builderForValue) {
        copyOnWrite();
        instance.addBatchOutput(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public Builder addBatchOutput(
          int index, inference.ModelConfigOuterClass.BatchOutput.Builder builderForValue) {
        copyOnWrite();
        instance.addBatchOutput(index,
            builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public Builder addAllBatchOutput(
          java.lang.Iterable<? extends inference.ModelConfigOuterClass.BatchOutput> values) {
        copyOnWrite();
        instance.addAllBatchOutput(values);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public Builder clearBatchOutput() {
        copyOnWrite();
        instance.clearBatchOutput();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public Builder removeBatchOutput(int index) {
        copyOnWrite();
        instance.removeBatchOutput(index);
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
       */
      @java.lang.Override
      public boolean hasOptimization() {
        return instance.hasOptimization();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy getOptimization() {
        return instance.getOptimization();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
       */
      public Builder setOptimization(inference.ModelConfigOuterClass.ModelOptimizationPolicy value) {
        copyOnWrite();
        instance.setOptimization(value);
        return this;
        }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
       */
      public Builder setOptimization(
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.Builder builderForValue) {
        copyOnWrite();
        instance.setOptimization(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
       */
      public Builder mergeOptimization(inference.ModelConfigOuterClass.ModelOptimizationPolicy value) {
        copyOnWrite();
        instance.mergeOptimization(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
       */
      public Builder clearOptimization() {  copyOnWrite();
        instance.clearOptimization();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      @java.lang.Override
      public boolean hasDynamicBatching() {
        return instance.hasDynamicBatching();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelDynamicBatching getDynamicBatching() {
        return instance.getDynamicBatching();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      public Builder setDynamicBatching(inference.ModelConfigOuterClass.ModelDynamicBatching value) {
        copyOnWrite();
        instance.setDynamicBatching(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      public Builder setDynamicBatching(
          inference.ModelConfigOuterClass.ModelDynamicBatching.Builder builderForValue) {
        copyOnWrite();
        instance.setDynamicBatching(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      public Builder mergeDynamicBatching(inference.ModelConfigOuterClass.ModelDynamicBatching value) {
        copyOnWrite();
        instance.mergeDynamicBatching(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      public Builder clearDynamicBatching() {
        copyOnWrite();
        instance.clearDynamicBatching();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
       */
      @java.lang.Override
      public boolean hasSequenceBatching() {
        return instance.hasSequenceBatching();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelSequenceBatching getSequenceBatching() {
        return instance.getSequenceBatching();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
       */
      public Builder setSequenceBatching(inference.ModelConfigOuterClass.ModelSequenceBatching value) {
        copyOnWrite();
        instance.setSequenceBatching(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
       */
      public Builder setSequenceBatching(
          inference.ModelConfigOuterClass.ModelSequenceBatching.Builder builderForValue) {
        copyOnWrite();
        instance.setSequenceBatching(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
       */
      public Builder mergeSequenceBatching(inference.ModelConfigOuterClass.ModelSequenceBatching value) {
        copyOnWrite();
        instance.mergeSequenceBatching(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
       */
      public Builder clearSequenceBatching() {
        copyOnWrite();
        instance.clearSequenceBatching();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      @java.lang.Override
      public boolean hasEnsembleScheduling() {
        return instance.hasEnsembleScheduling();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelEnsembling getEnsembleScheduling() {
        return instance.getEnsembleScheduling();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      public Builder setEnsembleScheduling(inference.ModelConfigOuterClass.ModelEnsembling value) {
        copyOnWrite();
        instance.setEnsembleScheduling(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      public Builder setEnsembleScheduling(
          inference.ModelConfigOuterClass.ModelEnsembling.Builder builderForValue) {
        copyOnWrite();
        instance.setEnsembleScheduling(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      public Builder mergeEnsembleScheduling(inference.ModelConfigOuterClass.ModelEnsembling value) {
        copyOnWrite();
        instance.mergeEnsembleScheduling(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      public Builder clearEnsembleScheduling() {
        copyOnWrite();
        instance.clearEnsembleScheduling();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      @java.lang.Override
      public java.util.List<inference.ModelConfigOuterClass.ModelInstanceGroup> getInstanceGroupList() {
        return java.util.Collections.unmodifiableList(
            instance.getInstanceGroupList());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      @java.lang.Override
      public int getInstanceGroupCount() {
        return instance.getInstanceGroupCount();
      }/**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelInstanceGroup getInstanceGroup(int index) {
        return instance.getInstanceGroup(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder setInstanceGroup(
          int index, inference.ModelConfigOuterClass.ModelInstanceGroup value) {
        copyOnWrite();
        instance.setInstanceGroup(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder setInstanceGroup(
          int index, inference.ModelConfigOuterClass.ModelInstanceGroup.Builder builderForValue) {
        copyOnWrite();
        instance.setInstanceGroup(index,
            builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder addInstanceGroup(inference.ModelConfigOuterClass.ModelInstanceGroup value) {
        copyOnWrite();
        instance.addInstanceGroup(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder addInstanceGroup(
          int index, inference.ModelConfigOuterClass.ModelInstanceGroup value) {
        copyOnWrite();
        instance.addInstanceGroup(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder addInstanceGroup(
          inference.ModelConfigOuterClass.ModelInstanceGroup.Builder builderForValue) {
        copyOnWrite();
        instance.addInstanceGroup(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder addInstanceGroup(
          int index, inference.ModelConfigOuterClass.ModelInstanceGroup.Builder builderForValue) {
        copyOnWrite();
        instance.addInstanceGroup(index,
            builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder addAllInstanceGroup(
          java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelInstanceGroup> values) {
        copyOnWrite();
        instance.addAllInstanceGroup(values);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder clearInstanceGroup() {
        copyOnWrite();
        instance.clearInstanceGroup();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder removeInstanceGroup(int index) {
        copyOnWrite();
        instance.removeInstanceGroup(index);
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string default_model_filename
       *&#64;&#64;
       *&#64;&#64;     Optional filename of the model file to use if a
       *&#64;&#64;     compute-capability specific model is not specified in
       *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
       *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
       *&#64;&#64;     'model.pt' depending on the model type.
       *&#64;&#64;
       * </pre>
       *
       * <code>string default_model_filename = 8;</code>
       * @return The defaultModelFilename.
       */
      @java.lang.Override
      public java.lang.String getDefaultModelFilename() {
        return instance.getDefaultModelFilename();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string default_model_filename
       *&#64;&#64;
       *&#64;&#64;     Optional filename of the model file to use if a
       *&#64;&#64;     compute-capability specific model is not specified in
       *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
       *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
       *&#64;&#64;     'model.pt' depending on the model type.
       *&#64;&#64;
       * </pre>
       *
       * <code>string default_model_filename = 8;</code>
       * @return The bytes for defaultModelFilename.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString
          getDefaultModelFilenameBytes() {
        return instance.getDefaultModelFilenameBytes();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string default_model_filename
       *&#64;&#64;
       *&#64;&#64;     Optional filename of the model file to use if a
       *&#64;&#64;     compute-capability specific model is not specified in
       *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
       *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
       *&#64;&#64;     'model.pt' depending on the model type.
       *&#64;&#64;
       * </pre>
       *
       * <code>string default_model_filename = 8;</code>
       * @param value The defaultModelFilename to set.
       * @return This builder for chaining.
       */
      public Builder setDefaultModelFilename(
          java.lang.String value) {
        copyOnWrite();
        instance.setDefaultModelFilename(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string default_model_filename
       *&#64;&#64;
       *&#64;&#64;     Optional filename of the model file to use if a
       *&#64;&#64;     compute-capability specific model is not specified in
       *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
       *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
       *&#64;&#64;     'model.pt' depending on the model type.
       *&#64;&#64;
       * </pre>
       *
       * <code>string default_model_filename = 8;</code>
       * @return This builder for chaining.
       */
      public Builder clearDefaultModelFilename() {
        copyOnWrite();
        instance.clearDefaultModelFilename();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string default_model_filename
       *&#64;&#64;
       *&#64;&#64;     Optional filename of the model file to use if a
       *&#64;&#64;     compute-capability specific model is not specified in
       *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
       *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
       *&#64;&#64;     'model.pt' depending on the model type.
       *&#64;&#64;
       * </pre>
       *
       * <code>string default_model_filename = 8;</code>
       * @param value The bytes for defaultModelFilename to set.
       * @return This builder for chaining.
       */
      public Builder setDefaultModelFilenameBytes(
          com.google.protobuf.ByteString value) {
        copyOnWrite();
        instance.setDefaultModelFilenameBytes(value);
        return this;
      }

      @java.lang.Override

      public int getCcModelFilenamesCount() {
        return instance.getCcModelFilenamesMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */
      @java.lang.Override

      public boolean containsCcModelFilenames(
          java.lang.String key) {
        java.lang.Class<?> keyClass = key.getClass();
        return instance.getCcModelFilenamesMap().containsKey(key);
      }

      public Builder clearCcModelFilenames() {
        copyOnWrite();
        instance.getMutableCcModelFilenamesMap().clear();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */

      public Builder removeCcModelFilenames(
          java.lang.String key) {
        java.lang.Class<?> keyClass = key.getClass();
        copyOnWrite();
        instance.getMutableCcModelFilenamesMap().remove(key);
        return this;
      }
      /**
       * Use {@link #getCcModelFilenamesMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getCcModelFilenames() {
        return getCcModelFilenamesMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */
      @java.lang.Override
      public java.util.Map<java.lang.String, java.lang.String> getCcModelFilenamesMap() {
        return java.util.Collections.unmodifiableMap(
            instance.getCcModelFilenamesMap());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */
      @java.lang.Override

      public java.lang.String getCcModelFilenamesOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        java.lang.Class<?> keyClass = key.getClass();
        java.util.Map<java.lang.String, java.lang.String> map =
            instance.getCcModelFilenamesMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */
      @java.lang.Override

      public java.lang.String getCcModelFilenamesOrThrow(
          java.lang.String key) {
        java.lang.Class<?> keyClass = key.getClass();
        java.util.Map<java.lang.String, java.lang.String> map =
            instance.getCcModelFilenamesMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */
      public Builder putCcModelFilenames(
          java.lang.String key,
          java.lang.String value) {
        java.lang.Class<?> keyClass = key.getClass();
        java.lang.Class<?> valueClass = value.getClass();
        copyOnWrite();
        instance.getMutableCcModelFilenamesMap().put(key, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */
      public Builder putAllCcModelFilenames(
          java.util.Map<java.lang.String, java.lang.String> values) {
        copyOnWrite();
        instance.getMutableCcModelFilenamesMap().putAll(values);
        return this;
      }

      @java.lang.Override

      public int getMetricTagsCount() {
        return instance.getMetricTagsMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */
      @java.lang.Override

      public boolean containsMetricTags(
          java.lang.String key) {
        java.lang.Class<?> keyClass = key.getClass();
        return instance.getMetricTagsMap().containsKey(key);
      }

      public Builder clearMetricTags() {
        copyOnWrite();
        instance.getMutableMetricTagsMap().clear();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */

      public Builder removeMetricTags(
          java.lang.String key) {
        java.lang.Class<?> keyClass = key.getClass();
        copyOnWrite();
        instance.getMutableMetricTagsMap().remove(key);
        return this;
      }
      /**
       * Use {@link #getMetricTagsMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getMetricTags() {
        return getMetricTagsMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */
      @java.lang.Override
      public java.util.Map<java.lang.String, java.lang.String> getMetricTagsMap() {
        return java.util.Collections.unmodifiableMap(
            instance.getMetricTagsMap());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */
      @java.lang.Override

      public java.lang.String getMetricTagsOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        java.lang.Class<?> keyClass = key.getClass();
        java.util.Map<java.lang.String, java.lang.String> map =
            instance.getMetricTagsMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */
      @java.lang.Override

      public java.lang.String getMetricTagsOrThrow(
          java.lang.String key) {
        java.lang.Class<?> keyClass = key.getClass();
        java.util.Map<java.lang.String, java.lang.String> map =
            instance.getMetricTagsMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */
      public Builder putMetricTags(
          java.lang.String key,
          java.lang.String value) {
        java.lang.Class<?> keyClass = key.getClass();
        java.lang.Class<?> valueClass = value.getClass();
        copyOnWrite();
        instance.getMutableMetricTagsMap().put(key, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */
      public Builder putAllMetricTags(
          java.util.Map<java.lang.String, java.lang.String> values) {
        copyOnWrite();
        instance.getMutableMetricTagsMap().putAll(values);
        return this;
      }

      @java.lang.Override

      public int getParametersCount() {
        return instance.getParametersMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
       */
      @java.lang.Override

      public boolean containsParameters(
          java.lang.String key) {
        java.lang.Class<?> keyClass = key.getClass();
        return instance.getParametersMap().containsKey(key);
      }

      public Builder clearParameters() {
        copyOnWrite();
        instance.getMutableParametersMap().clear();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
       */

      public Builder removeParameters(
          java.lang.String key) {
        java.lang.Class<?> keyClass = key.getClass();
        copyOnWrite();
        instance.getMutableParametersMap().remove(key);
        return this;
      }
      /**
       * Use {@link #getParametersMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelParameter> getParameters() {
        return getParametersMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
       */
      @java.lang.Override
      public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelParameter> getParametersMap() {
        return java.util.Collections.unmodifiableMap(
            instance.getParametersMap());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
       */
      @java.lang.Override

      public inference.ModelConfigOuterClass.ModelParameter getParametersOrDefault(
          java.lang.String key,
          inference.ModelConfigOuterClass.ModelParameter defaultValue) {
        java.lang.Class<?> keyClass = key.getClass();
        java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelParameter> map =
            instance.getParametersMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
       */
      @java.lang.Override

      public inference.ModelConfigOuterClass.ModelParameter getParametersOrThrow(
          java.lang.String key) {
        java.lang.Class<?> keyClass = key.getClass();
        java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelParameter> map =
            instance.getParametersMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
       */
      public Builder putParameters(
          java.lang.String key,
          inference.ModelConfigOuterClass.ModelParameter value) {
        java.lang.Class<?> keyClass = key.getClass();
        java.lang.Class<?> valueClass = value.getClass();
        copyOnWrite();
        instance.getMutableParametersMap().put(key, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
       */
      public Builder putAllParameters(
          java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelParameter> values) {
        copyOnWrite();
        instance.getMutableParametersMap().putAll(values);
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      @java.lang.Override
      public java.util.List<inference.ModelConfigOuterClass.ModelWarmup> getModelWarmupList() {
        return java.util.Collections.unmodifiableList(
            instance.getModelWarmupList());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      @java.lang.Override
      public int getModelWarmupCount() {
        return instance.getModelWarmupCount();
      }/**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelWarmup getModelWarmup(int index) {
        return instance.getModelWarmup(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public Builder setModelWarmup(
          int index, inference.ModelConfigOuterClass.ModelWarmup value) {
        copyOnWrite();
        instance.setModelWarmup(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public Builder setModelWarmup(
          int index, inference.ModelConfigOuterClass.ModelWarmup.Builder builderForValue) {
        copyOnWrite();
        instance.setModelWarmup(index,
            builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public Builder addModelWarmup(inference.ModelConfigOuterClass.ModelWarmup value) {
        copyOnWrite();
        instance.addModelWarmup(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public Builder addModelWarmup(
          int index, inference.ModelConfigOuterClass.ModelWarmup value) {
        copyOnWrite();
        instance.addModelWarmup(index, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public Builder addModelWarmup(
          inference.ModelConfigOuterClass.ModelWarmup.Builder builderForValue) {
        copyOnWrite();
        instance.addModelWarmup(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public Builder addModelWarmup(
          int index, inference.ModelConfigOuterClass.ModelWarmup.Builder builderForValue) {
        copyOnWrite();
        instance.addModelWarmup(index,
            builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public Builder addAllModelWarmup(
          java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelWarmup> values) {
        copyOnWrite();
        instance.addAllModelWarmup(values);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public Builder clearModelWarmup() {
        copyOnWrite();
        instance.clearModelWarmup();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public Builder removeModelWarmup(int index) {
        copyOnWrite();
        instance.removeModelWarmup(index);
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
       *&#64;&#64;
       *&#64;&#64;     Optional metadata of the libraries providing custom operations for
       *&#64;&#64;     this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOperations model_operations = 18;</code>
       */
      @java.lang.Override
      public boolean hasModelOperations() {
        return instance.hasModelOperations();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
       *&#64;&#64;
       *&#64;&#64;     Optional metadata of the libraries providing custom operations for
       *&#64;&#64;     this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOperations model_operations = 18;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOperations getModelOperations() {
        return instance.getModelOperations();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
       *&#64;&#64;
       *&#64;&#64;     Optional metadata of the libraries providing custom operations for
       *&#64;&#64;     this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOperations model_operations = 18;</code>
       */
      public Builder setModelOperations(inference.ModelConfigOuterClass.ModelOperations value) {
        copyOnWrite();
        instance.setModelOperations(value);
        return this;
        }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
       *&#64;&#64;
       *&#64;&#64;     Optional metadata of the libraries providing custom operations for
       *&#64;&#64;     this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOperations model_operations = 18;</code>
       */
      public Builder setModelOperations(
          inference.ModelConfigOuterClass.ModelOperations.Builder builderForValue) {
        copyOnWrite();
        instance.setModelOperations(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
       *&#64;&#64;
       *&#64;&#64;     Optional metadata of the libraries providing custom operations for
       *&#64;&#64;     this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOperations model_operations = 18;</code>
       */
      public Builder mergeModelOperations(inference.ModelConfigOuterClass.ModelOperations value) {
        copyOnWrite();
        instance.mergeModelOperations(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
       *&#64;&#64;
       *&#64;&#64;     Optional metadata of the libraries providing custom operations for
       *&#64;&#64;     this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOperations model_operations = 18;</code>
       */
      public Builder clearModelOperations() {  copyOnWrite();
        instance.clearModelOperations();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
       *&#64;&#64;
       *&#64;&#64;     Optional specification that describes the nature of transactions
       *&#64;&#64;     to be expected from the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
       */
      @java.lang.Override
      public boolean hasModelTransactionPolicy() {
        return instance.hasModelTransactionPolicy();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
       *&#64;&#64;
       *&#64;&#64;     Optional specification that describes the nature of transactions
       *&#64;&#64;     to be expected from the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelTransactionPolicy getModelTransactionPolicy() {
        return instance.getModelTransactionPolicy();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
       *&#64;&#64;
       *&#64;&#64;     Optional specification that describes the nature of transactions
       *&#64;&#64;     to be expected from the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
       */
      public Builder setModelTransactionPolicy(inference.ModelConfigOuterClass.ModelTransactionPolicy value) {
        copyOnWrite();
        instance.setModelTransactionPolicy(value);
        return this;
        }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
       *&#64;&#64;
       *&#64;&#64;     Optional specification that describes the nature of transactions
       *&#64;&#64;     to be expected from the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
       */
      public Builder setModelTransactionPolicy(
          inference.ModelConfigOuterClass.ModelTransactionPolicy.Builder builderForValue) {
        copyOnWrite();
        instance.setModelTransactionPolicy(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
       *&#64;&#64;
       *&#64;&#64;     Optional specification that describes the nature of transactions
       *&#64;&#64;     to be expected from the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
       */
      public Builder mergeModelTransactionPolicy(inference.ModelConfigOuterClass.ModelTransactionPolicy value) {
        copyOnWrite();
        instance.mergeModelTransactionPolicy(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
       *&#64;&#64;
       *&#64;&#64;     Optional specification that describes the nature of transactions
       *&#64;&#64;     to be expected from the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
       */
      public Builder clearModelTransactionPolicy() {  copyOnWrite();
        instance.clearModelTransactionPolicy();
        return this;
      }

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelRepositoryAgents model_repository_agents
       *&#64;&#64;
       *&#64;&#64;     Optional specification of the agent(s) that should be invoked
       *&#64;&#64;     with repository actions are performed for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelRepositoryAgents model_repository_agents = 23;</code>
       */
      @java.lang.Override
      public boolean hasModelRepositoryAgents() {
        return instance.hasModelRepositoryAgents();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelRepositoryAgents model_repository_agents
       *&#64;&#64;
       *&#64;&#64;     Optional specification of the agent(s) that should be invoked
       *&#64;&#64;     with repository actions are performed for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelRepositoryAgents model_repository_agents = 23;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelRepositoryAgents getModelRepositoryAgents() {
        return instance.getModelRepositoryAgents();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelRepositoryAgents model_repository_agents
       *&#64;&#64;
       *&#64;&#64;     Optional specification of the agent(s) that should be invoked
       *&#64;&#64;     with repository actions are performed for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelRepositoryAgents model_repository_agents = 23;</code>
       */
      public Builder setModelRepositoryAgents(inference.ModelConfigOuterClass.ModelRepositoryAgents value) {
        copyOnWrite();
        instance.setModelRepositoryAgents(value);
        return this;
        }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelRepositoryAgents model_repository_agents
       *&#64;&#64;
       *&#64;&#64;     Optional specification of the agent(s) that should be invoked
       *&#64;&#64;     with repository actions are performed for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelRepositoryAgents model_repository_agents = 23;</code>
       */
      public Builder setModelRepositoryAgents(
          inference.ModelConfigOuterClass.ModelRepositoryAgents.Builder builderForValue) {
        copyOnWrite();
        instance.setModelRepositoryAgents(builderForValue.build());
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelRepositoryAgents model_repository_agents
       *&#64;&#64;
       *&#64;&#64;     Optional specification of the agent(s) that should be invoked
       *&#64;&#64;     with repository actions are performed for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelRepositoryAgents model_repository_agents = 23;</code>
       */
      public Builder mergeModelRepositoryAgents(inference.ModelConfigOuterClass.ModelRepositoryAgents value) {
        copyOnWrite();
        instance.mergeModelRepositoryAgents(value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelRepositoryAgents model_repository_agents
       *&#64;&#64;
       *&#64;&#64;     Optional specification of the agent(s) that should be invoked
       *&#64;&#64;     with repository actions are performed for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelRepositoryAgents model_repository_agents = 23;</code>
       */
      public Builder clearModelRepositoryAgents() {  copyOnWrite();
        instance.clearModelRepositoryAgents();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:inference.ModelConfig)
    }
    @java.lang.Override
    @java.lang.SuppressWarnings({"unchecked", "fallthrough"})
    protected final java.lang.Object dynamicMethod(
        com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
        java.lang.Object arg0, java.lang.Object arg1) {
      switch (method) {
        case NEW_MUTABLE_INSTANCE: {
          return new inference.ModelConfigOuterClass.ModelConfig();
        }
        case NEW_BUILDER: {
          return new Builder();
        }
        case BUILD_MESSAGE_INFO: {
            java.lang.Object[] objects = new java.lang.Object[] {
              "schedulingChoice_",
              "schedulingChoiceCase_",
              "name_",
              "platform_",
              "versionPolicy_",
              "maxBatchSize_",
              "input_",
              inference.ModelConfigOuterClass.ModelInput.class,
              "output_",
              inference.ModelConfigOuterClass.ModelOutput.class,
              "instanceGroup_",
              inference.ModelConfigOuterClass.ModelInstanceGroup.class,
              "defaultModelFilename_",
              "ccModelFilenames_",
              CcModelFilenamesDefaultEntryHolder.defaultEntry,
              "metricTags_",
              MetricTagsDefaultEntryHolder.defaultEntry,
              inference.ModelConfigOuterClass.ModelDynamicBatching.class,
              "optimization_",
              inference.ModelConfigOuterClass.ModelSequenceBatching.class,
              "parameters_",
              ParametersDefaultEntryHolder.defaultEntry,
              inference.ModelConfigOuterClass.ModelEnsembling.class,
              "modelWarmup_",
              inference.ModelConfigOuterClass.ModelWarmup.class,
              "backend_",
              "modelOperations_",
              "modelTransactionPolicy_",
              "batchInput_",
              inference.ModelConfigOuterClass.BatchInput.class,
              "batchOutput_",
              inference.ModelConfigOuterClass.BatchOutput.class,
              "modelRepositoryAgents_",
            };
            java.lang.String info =
                "\u0000\u0016\u0001\u0000\u0001\u0017\u0016\u0003\u0006\u0000\u0001\u0208\u0002\u0208" +
                "\u0003\t\u0004\u0004\u0005\u001b\u0006\u001b\u0007\u001b\b\u0208\t2\n2\u000b<\u0000" +
                "\f\t\r<\u0000\u000e2\u000f<\u0000\u0010\u001b\u0011\u0208\u0012\t\u0013\t\u0014\u001b" +
                "\u0015\u001b\u0017\t";
            return newMessageInfo(DEFAULT_INSTANCE, info, objects);
        }
        // fall through
        case GET_DEFAULT_INSTANCE: {
          return DEFAULT_INSTANCE;
        }
        case GET_PARSER: {
          com.google.protobuf.Parser<inference.ModelConfigOuterClass.ModelConfig> parser = PARSER;
          if (parser == null) {
            synchronized (inference.ModelConfigOuterClass.ModelConfig.class) {
              parser = PARSER;
              if (parser == null) {
                parser =
                    new DefaultInstanceBasedParser<inference.ModelConfigOuterClass.ModelConfig>(
                        DEFAULT_INSTANCE);
                PARSER = parser;
              }
            }
          }
          return parser;
      }
      case GET_MEMOIZED_IS_INITIALIZED: {
        return (byte) 1;
      }
      case SET_MEMOIZED_IS_INITIALIZED: {
        return null;
      }
      }
      throw new UnsupportedOperationException();
    }


    // @@protoc_insertion_point(class_scope:inference.ModelConfig)
    private static final inference.ModelConfigOuterClass.ModelConfig DEFAULT_INSTANCE;
    static {
      ModelConfig defaultInstance = new ModelConfig();
      // New instances are implicitly immutable so no need to make
      // immutable.
      DEFAULT_INSTANCE = defaultInstance;
      com.google.protobuf.GeneratedMessageLite.registerDefaultInstance(
        ModelConfig.class, defaultInstance);
    }

    public static inference.ModelConfigOuterClass.ModelConfig getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static volatile com.google.protobuf.Parser<ModelConfig> PARSER;

    public static com.google.protobuf.Parser<ModelConfig> parser() {
      return DEFAULT_INSTANCE.getParserForType();
    }
  }


  static {
  }

  // @@protoc_insertion_point(outer_class_scope)
}
